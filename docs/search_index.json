[["index.html", "Reporting with Data in R About this book About my philosphy Tips on my writing style About the author License Other resources", " Reporting with Data in R Christian McDonald 2021-09-16 About this book NOTE: This book is in the middle of a rewrite. The original “beta” version used in spring 2019 is long gone in a commit history and the new version is being rewritten under a new URL. Some chapters need to be written or rewritten, as noted. This new version will become v1.0 and is being used for fall 2021. Reporting with Data in R is a series of lessons and instructions used for courses in the School of Journalism and Media, Moody College of Communication at the University of Texas at Austin. It is written in bookdown and the source is available on Github. I’m a strong proponent of what I call Scripted Journalism, a method of committing data-centric journalism in a programmatic, repeatable and transparent way. There are a myriad of programming languages that further this, including Python (pandas using Jupyter) and JavaScript (Observable), but we’ll be using R, RMarkdown and RStudio. R is a super powerful, open-source programming language for data that is deep with features and an awesome community of users who build upon it. No matter the challenge before you in your data storytelling, there is probably a package available to help you solve that challenge. Probably more than one. About my philosphy There is always more than one way to do things in R. This book is a Tidyverse-oriented, opinionated collection of lessons intended to teach students new to programming and R for the expressed act of committing journalism. As a beginner course, I strive to make it as simple as possible, which means I may not go into detail about alternative (and possibly better) ways to accomplish tasks in favor of staying in the Tidyverse and reducing options to simplify understanding. This is the second version of this book. The first “beta” version was used in Spring 2019, and it was my first time to introduce R to beginning students. While the experience went well, there were pros and cons to using R in a beginning data class and I continue to experiment with material. I hope to use my experience in that first class to improve this edition. After that Spring 2019 class I chose to use a different web-based tool — Workbench — which allowed for a similar scripted workflow but without the same level of coding. I loved Workbench, especially for beginning students, but the site is scheduled to close down in October 2021. Tips on my writing style I try to be consistent in the way I write documentation and lessons. I’m human, so sometimes break my own rules, but in general I keep the following in mind. Things to do I usually put things I want you to DO in ordered lists: Do this thing. Then do this thing. Explanations are usually in text, like this very paragraph. And sometimes I’ll explain things in lists: This is the first thing I want you to know. This is the second. You don’t have to DO these things, just know about them. Notes, some important I will use the blockquote syntax to set off irrelevant background: Markdown was developed by JOHN GRUBER, as outlined on his Daring Fireball blog. But sometimes those asides are important. I usually indicate that: IMPORTANT: You really should learn how to use Rmarkdown as you will use it the whole semester, and hopefully for the rest of your life. Copy code blocks When you see R code in the instructions, you can roll your cursor over the right-corner and click on the copy icon to copy the code to your clipboard: Copy to clipboard You can then paste the code inside your R chunk. That said, typing code yourself has many, many benefits. You learn better when you type yourself, make mistakes and have to fix them. I encourage you to always type short code snippets. Leave the copying to long ones. About the author I’m a career journalist who most recently served as Data and Projects Editor at the Austin American-Statesman before joining the University of Texas at Austin faculty full-time in Fall 2018 as an assistant professor of practice. I’ve taught data-related course at UT since 2013. My UT Github: utdata My Personal Github: critmcdonald Twitter: crit Email: christian.mcdonald@utexas.edu License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Let’s just say this is free on the internet and I don’t make any money from it and you shouldn’t either. Other resources This text stands upon the shoulders of giants and by design does not cover all aspects of using R. Here are some other useful books, tutorials and sites dedicated to R. There are other task-specific tutorials and articles sprinkled throughout the book in the Resources section of select chapters. R Journalism Examples, a companion piece of sorts to this book with example code to accomplish specific tasks. It is a work-in-progress, and quite nascent at that. R for Data Science by Hadley Wickham and Garrett Grolemund. The Tidyverse site, which has tons of documentation and help. The RStudio Cheatsheets. R Graphics Cookbook The R Graph Gallery another place to see examples. Practical R for Journalism by Sharon Machlis, an editor with PC World and related publications. Sharon is a longtime proponent of using R in journalism. Sports Data Analysis and Visualization and Data Journalism with R and the Tidyverse by Matt Waite, a professor at the University of Nebraska-Lincoln. R for Journalists site by Andrew Tran, a reporter at the Washington Post. A series of videos and tutorials on using R in a journalism setting. "],["install.html", "Chapter 1 Install Party 1.1 Mac vs PC 1.2 Installing R 1.3 Installing RStudio 1.4 Class project folder", " Chapter 1 Install Party Let’s get this party started. NOTE: R and RStudio are already installed on lab computers. 1.1 Mac vs PC I use a Mac in class and in my examples. I’m a big fan of using keyboard commands to do operations in any program, but I reference this from a Mac perspective. So if I say use Cmd+S or Command+S to save, that might be Cntl+S or Control+S on a PC. The letters may not be the same on a PC, but you can usually figure it out by look at menu items in RStudio to figure out the PC command. We will install R and RStudio. It might take some time depending on your Internet connection. If you are doing this on your own you might follow this tutorial. But below you’ll find the basic steps. 1.2 Installing R Our first task is to install the R programming language onto your computer. Go to the https://cloud.r-project.org/. Click on the link for your operating system. The following steps will differ slightly based on your operating system. For Macs, you want the “latest package” unless you have an “M1” Mac (Nov. 2020 or newer), in which case choose the arm64.pkg version. For Windows, you want the “base” package. You’ll need to decide whether you want the 32- or 64-bit version. (Unless you’ve got a pretty old system, chances are you’ll want 64-bit.) Here’s hoping it will be self explanatory after that. You’ll never “launch” R as a program in a traditional sense, but you need it on your computers. We’ll use RStudio, which is next. 1.3 Installing RStudio RStudio is an “integrated development environment” – or IDE – for programming in R. Basically, it’s the program you will use when doing work for this class. Go to https://www.rstudio.com/download. Scroll down to the versions and find RStudio Desktop and click on the Download button. It should take you down the page to the version you need for your computer. Install it. Should be like installing any other program on your computer. 1.4 Class project folder To keep things consistent and help with troubleshooting, I’d like you to save your work in the same location all the time. On both Mac and Windows, every user has a “Documents” folder. Open that folder. (If you don’t know where it is, ask me to help you find it.) Create a new folder called “rwd.” Use all lowercase letters. When we create new “Projects,” I want you to always save them in the Documents/rwd folder. This just keeps us all on the same page. "],["intro.html", "Chapter 2 Introduction to R 2.1 RStudio tour 2.2 Updating preferences 2.3 Starting a new Project 2.4 Using R Notebooks 2.5 Turning in our projects", " Chapter 2 Introduction to R 2.1 RStudio tour When you launch RStudio, you’ll get a screen that looks like this: RStudio launch screen 2.2 Updating preferences There is a preference in RStudio that I would like you to change. By default, the program wants to save a the state of your work (all the variables and such) when you close a project, but that is not good practice. We’ll change that. Go to the RStudio menu and choose Preferences Under the General tab, uncheck the first four boxes. On the option “Save Workspace to .Rdata on exit,” change that to Never. Click OK to close the box. RStudio preferences 2.3 Starting a new Project When we work in RStudio, we will create “Projects” to hold all the files related to one another. This sets the “working directory,” which is a sort of home base for the project. Click on the second button that has a green +R sign. That brings up a box to create the project with several options. You want New Directory (unless you already have a Project directory, which you don’t for this.) For Project Type, choose New Project. Next, for the Directory name, choose a new name for your project folder. For this project, use “firstname-first-project” but use YOUR firstname. For the subdirectory, you want to use the Browse button to find your new rwd folder we created earlier. I want you to be anal about naming your folders. It’s a good programming habit. Use lowercase characters. Don’t use spaces. Use dashes. For this class, start with your first name. Rstudio project name, directory When you hit Create Project, your RStudio window will refresh and you’ll see the yourfirstname-first-project.Rproj file in your Files list. 2.4 Using R Notebooks For this class, we will almost always use RNotebooks. This format allows us to write text in between our blocks of code. The text is written in a language called RMarkdown, a juiced-up version of the common documentation syntax used by programmers, Markdown. We’ll learn that in a moment. 2.4.1 Create your first notebook Click on the button at the top-left of RStudio that has just the green + sign. Choose the item R Notebook. This will open a new file with some boilerplate R Markdown code. At the top between the --- marks, is the metadata. This is written using YAML, and what is inside are commands for the R Notebook. Don’t sweat the YAML syntax too much right now, as we won’t be editing it often. Next, you’ll see a couple of paragraphs of text that describes how to use an RNotebook. It is written in RMarkdown, and has some inline links and bold commands, which you will learn, Then you will see an R code chunk that looks like the figure below. R code chunk Let’s take a closer look at this: The three back tick characters (the key found at the top left on your keyboard) followed by the {r} indicate that this is a chunk of R code. The last three back ticks say the code chunk is over. The {r} bit can have some parameters added to it. We’ll get into that later. The line plot(cars) is R programming code. We’ll see what those commands do in a bit. The green right-arrow to the far right is a play button to run the code that is inside the chunk. The green down-arrow and bar to the left of that runs all the code in the Notebook up to that point. That is useful as you make changes in your code and want to rerun what is above the chunk in question. 2.4.2 Save the .Rmd file Do Cmd+S or hit the floppy disk icon to save the file. It will ask you what you want to name this file. Call it 01-first-file.Rmd. When you do this, you may see another new file created in your Files directory. It’s the pretty version of the notebook which we’ll see in a minute. In the metadata portion of the file, give your notebook a better title. Replace “R Notebook” in the title: \"R Notebook\" code to be “Christian’s first notebook,” but use your name. 2.4.3 Run the notebook There is only one chunk to run in this notebook, so: Click on the green right-arrow to run the code. The keyboard command (from somewhere within the chunk) is Cmd+Shift+Return. You should get something like this: Cars plot What you’ve done here is create a plot chart of a piece of sample data that is already inside R. (FWIW, It is the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s.) But that wasn’t a whole lot of code to see there is a relationship with speed vs stopping distance, eh? This is a “base R” plot. We’ll be using the tidyverse ggplot methods later in the semester. 2.4.4 A note about RMarkdown We always want to annotate our code to explain what we are doing. To do that, we use a syntax called RMarkdown, which is an R-specific version of Markdown. We use this syntax because it both makes sense in text but also makes a very pretty version in HTML when we “knit” our project. You can see how it to write RMarkdown here. This entire book is written in RMarkdown. Here is an example: ## My dating age The following section details the [socially-acceptable maximum age of anyone you should date](https://www.psychologytoday.com/us/blog/meet-catch-and-keep/201405/who-is-too-young-or-too-old-you-date). The math works like this: - Take your age - subtract 7 - Double the result The ## line is a headline. Add more ### and you get a smaller headline, like subheads. There is a full blank return between each element, including paragraphs of text. In the first paragraph we have embedded a hyperlink. We put the words we want to show inside square brackets and the URL in parenthesis DIRECTLY after the closing square bracket: [words to link](https://the_url.org). The - at the beginning of a line creates a bullet list. (You can also use *). Those lines need to be one after another without blank lines. Go ahead and copy the code above and add it as text in the notebook so you can see it works later. 2.4.5 Adding new code chunks The text after the chart describes how to insert a new code chunk. Let’s do that. Add a couple of returns before the paragraph of text about code chunks. Use the keys Cmd+Option+i to add the chunk. Your cursor will be inserted into the middle of the chunk. Type in this code in the space provided: # update 54 to your age age &lt;- 54 (age - 7) * 2 ## [1] 94 Change for “54” to your real age. With your cursor somewhere in the code block, use the key command Cmd+Shift+Return, which is the key command to RUN ALL LINES of code chunk. NOTE: To run an individual line, use Cmd+Return while on that line. Congratulations! The answer given at the bottom of that code chunk is the socially-acceptable maximum age of anyone you should date. Throwing aside whether the formula is sound, let’s break down the code. # update 54 to your age is a comment. It’s a way to explain what is happening in the code without being considered part of the code. We create comments by starting with #. You can also add a comment at the end of a line. age &lt;- 54 is assigning a number (54) to an R object/variable called (age). A variable is a placeholder. It can hold numbers, text or even groups of numbers. Variables are key to programming because they allow you to change a value as you go along. The next part is simple math: (age - 7) * 2 takes the value of age and subtracts 7, then multiplies by 2. When you run it, you get the result of the math equazion, [1] 94 in my case. That means there was one observation, and the value was “94.” For the record, my wife is much younger than that. Now you can play with the number assigned to the age variable to test out different ages. Do that. 2.4.6 Practice adding code chunks Now, on your own, add a similar section that calculates the minimum age of someone you should date, but using the formula (age / 2) + 7. Add a RMarkdown headline and text describing what you are doing. Create a code chunk that that calculates the formula based on your age. Include a comment within the code block. 2.4.7 Preview the report The rest of the boilerplate text here describes how you can Preview and Knit a notebook. Let’s do that now. Press Cmd+Shift+K to open a Preview. This will open a new window and show you the “pretty” notebook that we are building. Preview is a little different than Knit, which runs all the code, then creates the new knitted HTML document. It’s Knit to HMTL that you’ll want to do before turning in your assignments. That is explained below. 2.4.8 The toolbar One last thing to point out before we turn this in: The toolbar that runs across the top of the R Notebook file window. The image below explains some of the more useful tools, but you REALLY should learn and use keyboard commands when they are available. R Notebook toolbar 2.4.9 Knit the final workbook Save your File with Cmd+S. Click on the dropdown next to the Run menu item and choose Restart R and Run All Chunks. We do this to make sure everything still works. Use the Knit button in the toolbar to choose Knit to HTML. This will open your knitted file. Isn’t it pretty? 2.5 Turning in our projects If you now look in your Files pane, you’ll see you have four files in our project. (Note the only one you actually edited was the .Rmd file.) Files list The best way to turn in all of those files into Canvas is to compress them into a single .zip file that you can upload to the assignment. In your computer’s Finder, open the Documents/rwd folder. Follow the directions for your operating system linked below to create a compressed version of your yourname-final-project folder. Compress files on a Mac. Compress flies on Windows. Upload the resulting .zip file to the assignment for this week in Canvas. Here is what the compression steps looks like on a Mac: Compress file: Mac If you find you make changes to your R files after you’ve zipped your folder, you’ll need to delete the zip file and compress it again. Because we are building “repeatable” code, I’ll be able to download your .zip files, uncompress them, and the re-run them to get the same results. Well done! You’ve completed the first level and earned the Beginner badge. "],["import.html", "Chapter 3 Import 3.1 Learning goals of this lesson 3.2 Basic steps of this lesson 3.3 Create a new project 3.4 About data sources 3.5 Our project data 3.6 Data dictionary 3.7 Import the data 3.8 Assign our import to a tibble 3.9 Cleaning column names 3.10 Fixing the date 3.11 Arrange the data 3.12 Selecting columns 3.13 Exporting data 3.14 Naming chunks 3.15 Knit your page 3.16 Review of what we’ve learned so far 3.17 What’s next", " Chapter 3 Import “If you’re doing data analysis every day, the time it takes to learn a programming language pays off pretty quickly because you can automate more and more of what you do.” –Hadley Wickham, chief scientist at RStudio 3.1 Learning goals of this lesson Practice organized project setup. Learn a little about data types available to R. Learn about R packages, how to install and import them. Learn how to download and import CSV files using the readr package. Introduce the Data Frame/Tibble. Introduce the tidyverse %&gt;%. Learn how to modify data types (date) and select() columns. We’ll be exploring the Billboard Hot 100 charts along the way. Eventually you find the answers to a bunch of questions in this data and write about it. 3.2 Basic steps of this lesson Before we get into our storytelling, we have to get our data and make sure it is in good shape for analysis. This is pretty standard for any new project. Here are the major steps we’ll cover in detail for this lesson (and many more to come): Create your project structure Find the data and get it Import the data into your project Clean up data types and columns Export cleaned data for later analysis 3.3 Create a new project We did this once Chapter 2, but here are the basic steps: Launch RStudio Make sure you don’t have an existing project open. Use File &gt; Close project if you do. Use the +R button to create a New Project in a New Directory Name the project yourfirstname-billboard and put it in your ~/Documents/rwd folder. Use the + button and use R Notebook to start a new notebook. Change the title to “Billboard Hot 100 Import.” Delete the other boilerplate text. Save the file as 01-import.Rmd. 3.3.1 Describe the goals of the notebook We’ll add our first bit of RMarkdown just after the meta data to explain what we are doing. Add this text to your notebook: ## Goals of this notebook Steps to prepare our data: - Download the data - Import into R - Clean up data types and columns - Export for next notebook We want to start each notebook with a list like this so our future selves and others know what the heck we are trying to accomplish. We will also write text like this for each new “section” or goal in the notebook. 3.3.2 The R Package environment We have to back up from the step-by-step nature of this lesson and talk a little about the R programming language. R is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful. What happens is developers will find it difficult to do a certain task, so they will write an R “Package” of code that helps them with that task. They share that code with the community, and suddenly the R garage has an “ultimate set of tools” that would make Spicoli’s dad proud. One set of these tools is Hadley Wickham’s Tidyverse, a set of packages for data science. These are the tools we will use most in this course. While not required reading, I highly recommend Wickham’s book R for data science, which is free. There are also a series of useful cheatsheets that can help you as you use the packages and functions from the tidyverse. We’ll refer to these throughout the course. 3.3.3 Installing and using packages There are two steps to using an R package: Install the package using install.packages(\"package_name\"). You only have to do this once for each computer, so I usually do it using the R Console instead of in notebook. Include the library using library(package_name). This has to be done for each Notebook or script that uses it, so it is usually one of the first things in the notebook. Note that you have to use “quotes” around the package name when you are installing, but you DON’T use quotes when you load the library. We’re going to install several packages we will use in this project. To do this, we are going to use the Console, which we haven’t talked about much yet. The Console and Terminal Use the image above to orient yourself to the R Console and Terminal. In the Console, type in: install.packages(&quot;tidyverse&quot;) As you type into the Console, you’ll see some type-assist hints on what you need. You can use the arrow keys to select one and hit the tab key to complete that command, then enter the values you need. If it asks you to install “from source,” type Yes and hit return. You’ll see a bunch of response in the Console. We need two other packages as well, so also do: install.packages(&quot;janitor&quot;) install.packages(&quot;lubridate&quot;) We’ll use janitor to clean up our data column names, among other things. A good reference to learn more is the janitor vignette. We’ll use lubridate to fix some dates, which are a special complicated thing in programming. Lubridate is part of the tidyverse universe, but we have to install and load it separately. You only have to install the packages once on your computer (though you have to load them into each new notebook, which is explained below). 3.3.4 Load the libraries Next, we’re going to tell our R Notebook to use these three libraries. After the metadata at the top of your notebook, use Cmd+option+i to insert an R code chunk. In that chunk, type in the two libraries and run the code block with Cmd+Shift+Return. This is the code you need: library(tidyverse) library(janitor) library(lubridate) Your output will look something like this: Libraries imported 3.3.5 Create a directory for your data I want you to create a folder called data-raw in your project folder. We are creating this folder because we want to keep a pristine version of our original data that we never change or overwrite. This is a basic data journalism principle: Thou shalt not change raw data. In your Files pane at the bottom-right of Rstudio, there is a New Folder icon. Click on the New Folder icon. Name your new folder data-raw. This is where we’ll put raw data. We never write data to this folder. Also create another new folder called data-processed. This is were we write data. We separate them so we don’t accidentally overwrite raw data. Once you’ve done that, they should show up in the file explorer in the Files pane. Click the refresh button if you don’t see them. (The circlish thing at top right of the screenshot below. You might have to widen the pane to see it.) Directory made Your .Rproj file name is likely different (and that s OK) and you can ignore the .gitignore I have there. 3.3.6 Let’s get our data Now that we have a folder for our data, we can download our data into it. The data was scraped and saved on data.world by Sean Miller, but you can just download my copy of the data using the download.file function in R. For the purposes of this assignment, we will “source” the data as being from Billboard Media, as that is who initially provided it. I’ve worked with data fairly extensively, and it is sound. Add a Markdown headline ## Downloading data and on a new line text that indicates you are downloading data. You would typically include a link and explain what it is, etc, often linking to the original source. Create an R chunk and include the following (hint: use the copy icon at the top right): # hot 100 download download.file(&quot;https://github.com/utdata/rwd-billboard-data/blob/main/data-process/hot-100/hot100-orig.csv?raw=true&quot;, &quot;data-raw/hot-stuff.csv&quot;) This download.file function takes at least two arguments: The URL of the file you are downloading The path and name of where you want to save it. Note those two arguments are in quotes. The path includes the folder name you are saving the file to, which we called hot-stuff.csv. When you run this, it should save the file and then give you output similar to this: trying URL &#39;https://github.com/utdata/rwd-billboard-data/blob/main/data-process/hot-100/hot100-orig.csv?raw=true&#39; Content type &#39;text/plain; charset=utf-8&#39; length 45795374 bytes (43.7 MB) ================================================== downloaded 43.7 MB That’s a pretty big file. 3.4 About data sources Depending on the data source, importing can be brilliantly easy or a major pain in the rear. It all depends on how well-formatted is the data. In this class, we will primarily use data from CSVs (Comma Separated Value), Excel files and APIs (Application Programming Interface). CSVs are a kind of lowest-common-denominator for data. Most any database or program can import or export them. It’s just text with a , between each value. Excel files are good, but are often messy because humans get involved. They often have multiple header rows, columns used in multiple ways, notes added, etc. Just know you might have to clean them up before or after importing them. APIs are systems designed to respond to programming. In the data world, we often use the APIs by writing a query to ask a system to return a selection of data. By definition, the data is well structured. You can often determine the file type of the output as part of the API call, including … JSON (or JavaScript Object Notation) is the data format preferred by JavaScript. R can read it, too. It is often the output format of APIs, and prevalent enough that you need to understand how it works. We’ll get into that later in semester. Don’t get me wrong … there are plenty of other data types and connections available through R, but those are the ones we’ll deal with most in this book. 3.5 Our project data Now that we’ve downloaded the data and talked about what data is, lets talk about our Billboard data specifically. The data includes the Billboard’s Weekly Hot 100 singles charts from its inception on 8/2/1958 through 2020. It is a modified version of data compiled by SEAN MILLER and posted on data.world. We are using a copy I have saved. When you write about this data (and you will), you should source it as the Billboard Hot 100 from Billboard Media, since that is where the data comes from via an API. 3.6 Data dictionary This data contains weekly Hot 100 singles chart from Billboard.com. Each row of data represents a song and the corresponding position on that week’s chart. Included in each row are the following elements: Billboard Chart URL: Website for the chart WeekID: Basically the date Song name Performer name SongID: Concatenation of song &amp; performer Current week on chart Instance: This is used to separate breaks on the chart for a given song. For example, an instance of 6 tells you that this is the sixth time this song has fallen off and then appeared on the chart Previous week position Peak Position: As of the current week Weeks on Chart: As of the current week Let’s import it so we can see the data. 3.7 Import the data Since we are doing a new thing, we should note that with a Markdown headline and text. Add a Markdown headline: ## Import data Add some text to explain that we are importing the Billboard Hot 100 data. After your description, add a new code chunk (Cmd+Option+i). We’ll be using the read_csv() function from the tidyverse readr package, which is different from read.csv that comes with base R. read_csv() is mo betta. Inside the function we put in the path to our data, inside quotes. If you start typing in that path and hit tab, it will complete the path. (Easier to show than explain). Add the follow code into your chunk and run it. read_csv(&quot;data-raw/hot-stuff.csv&quot;) Note the path is in quotes. You get two results printed to your screen. The first result called “R Console” shows what columns were imported and the data types. It’s important to review these to make sure things happened the way that expected. In this case it noted which columns came in as text (chr), or numbers (dbl). Note: Red colored text in this output is NOT an indication of a problem. RConsole output The second result spec_tbl_df prints out the data like a table. The data object is a tibble, which is a fancy tidyverse version of a “data frame.” I will use the term tibble and data frame interchangably. Think of tibbles and data frames like a well-structured spreadsheet. They are organized rows of data (called observations) with columns (called variables) where every column is a specific data type. Data output When we look at the data output into RStudio, there are several things to note: Below each column name is an indication of the data type. This is important. You can use the arrow icon on the right to page through the additional columns. You can use the paging numbers and controls at the bottom to page through the rows of data. The number of rows and columns is displayed. Of special note here, we have only printed this data to the screen. We have not saved it in any way, but that is next. 3.8 Assign our import to a tibble As of right now, we’ve only printed the data to our screen. We haven’t “saved” it at all. Next we need to assign it to an R object so it can be named thing in our project environment so we can reuse it. We don’t want to re-import the data every time we use the data. The syntax to create an object in R can seem weird at first, but the convention is to name the object first, then insert stuff into it. So, to create an object, the structure is this: # this is pseudo code. don&#39;t run it. new_object &lt;- stuff_going_into_object Let’s make a object called hot100 and fill it with our imported tibble. Edit your existing code chunk to look like this. You can add the &lt;- by using Option+- as in holding down the Option key and then pressing the hyphen: hot100 &lt;- read_csv(&quot;data-raw/hot-stuff.csv&quot;) Run that chunk and several things happen: We no longer see the result of the data printed to the screen. That’s because we created a tibble instead of printing it to the screen. You do get the RConsole output. In the Environment tab at the top-right of RStudio, you’ll see the hot100 object listed. Click on the blue play button next to ratings and it will expand to show you a summary of the columns. Click on the name and it will open a “View” of the data in another window, so you can look at it in spreadsheet form. You can even sort and filter it. Once you’ve looked at the data, close the data view with the little x next to the tab name. 3.8.1 Print a peek to the screen Since we can’t see the data after we assign it, let’s print the object to the screen so we can refer to it. Edit your import chunk to add the last two lines of this, including the one with the #: hot100 &lt;- read_csv(&quot;data-raw/hot100.csv&quot;) # peek at the data hot100 You can use the green play button at the right of the chunk, or preferrably have your cursor inside the chunk and do Cmd+Shift+Return to run all lines. (Cmd+Return runs only the current line.) This prints your saved tibble to the screen. The line with the # is a comment within the code chunk. Commenting what your code is important to your future self, and sometimes we do that within the code chunk instead of markdown if it will be more clear. 3.8.2 Glimpse the data There is another way to peek at the data that I prefer because it is more compact and shows you all the columns and data examples without scrolling: glimpse(). In your existing chunk, edit the last line to add the glimpse() function as noted below. I’m showing the return here as well. Afterward I’ll explain the pipe: %&gt;%. hot100 &lt;- read_csv(&quot;data-raw/hot-stuff.csv&quot;) # peek at the data hot100 %&gt;% glimpse() ## Rows: 327,895 ## Columns: 10 ## $ url &lt;chr&gt; &quot;http://www.billboard.com/charts/hot-100/1965-0… ## $ WeekID &lt;chr&gt; &quot;7/17/1965&quot;, &quot;7/24/1965&quot;, &quot;7/31/1965&quot;, &quot;8/7/196… ## $ Week.Position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9… ## $ Song &lt;chr&gt; &quot;Don&#39;t Just Stand There&quot;, &quot;Don&#39;t Just Stand The… ## $ Performer &lt;chr&gt; &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patt… ## $ SongID &lt;chr&gt; &quot;Don&#39;t Just Stand TherePatty Duke&quot;, &quot;Don&#39;t Just… ## $ Instance &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Previous.Week.Position &lt;dbl&gt; 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9… ## $ Peak.Position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,… ## $ Weeks.on.Chart &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, … Here you get the RConsole printout (hidden here for clarity), plus the glimpse shows there are 300,000+ rows and 10 columns in our data. Each column is then listed out with its data type and the first several values in that column. 3.8.3 About the pipe %&gt;% We need to break down this code a little: hot100 %&gt;% glimpse(). We are starting with the tibble hot100, but then we follow it with %&gt;%, which is called a pipe. It is a tidyverse tool that allows us to take the results of an object or function and pass them into another function. Think of it like a sentence that says “AND THEN” the next thing. It might look like there are no arguments inside glimpse(), but what we are actually doing is passing the hot100 tibble into it. You can’t start a new line with a pipe. If you are breaking into multiple lines, but the %&gt;% at the end. IMPORTANT: There is a keyboard command for the pipe %&gt;%: Cmd+Shift+m. Learn that one. 3.8.4 What is clean data The “Checking Your Data” section of this DataCamp tutorial has a good outline of what makes good data, but in general it should: Have a single header row with well-formed column names. One column name for each column. No merged cells. Short names are better than long ones. Spaces in names make them harder to work with. Use and _ or . between words. I prefer _ and lowercase text. Remove notes or comments from the files. Each column should have the same kind of data: numbers vs words, etc. Each row should be a single thing called an “observation.” The columns should describe attributes of that observation. Data rarely comes clean like that. There can be many challenges importing and cleaning data. We’ll face some of those challenges here. In our case our columns names could use help, and our field WeekID is not really a date, but text characters. We’ll tackle those issues next. 3.9 Cleaning column names So, given those notes above, we should clean up our column names. This is why we have included the janitor package, which includes a neat function called clean_names() Edit the first line of your chunk to add a pipe and the clean_names function: %&gt;% clean_names() hot100 &lt;- read_csv(&quot;data-raw/hot-stuff.csv&quot;) %&gt;% clean_names() # peek at the data hot100 %&gt;% glimpse() ## Rows: 327,895 ## Columns: 10 ## $ url &lt;chr&gt; &quot;http://www.billboard.com/charts/hot-100/1965-0… ## $ week_id &lt;chr&gt; &quot;7/17/1965&quot;, &quot;7/24/1965&quot;, &quot;7/31/1965&quot;, &quot;8/7/196… ## $ week_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9… ## $ song &lt;chr&gt; &quot;Don&#39;t Just Stand There&quot;, &quot;Don&#39;t Just Stand The… ## $ performer &lt;chr&gt; &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patt… ## $ song_id &lt;chr&gt; &quot;Don&#39;t Just Stand TherePatty Duke&quot;, &quot;Don&#39;t Just… ## $ instance &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ previous_week_position &lt;dbl&gt; 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9… ## $ peak_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,… ## $ weeks_on_chart &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, … This function has cleaned up your names, making them all lowercase and using _ instead of periods between words. Believe me when I say this is helpful later to auto-complete column names when you are writing code. 3.10 Fixing the date Dates are a tricky datatype because you can do math with them. To use them properly in R we need to convert them from the text we have here to a real date. Converting dates can be a pain, but the tidyverse universe has a package called lubridate that can help us with that. Since we are doing something new, we want to start a new section in our notebook and explain what we are doing. Add a headline: ## Fix our dates. Add some text that you are using lubridate to create a new column with a real date. Add a new code chunk. Remember Cmd+Option+i will do that. We are going to start by creating a new data frame that is the same as our current on, and then add a glimpse so we can see the results as we build upon it. Add the following inside your code chunk. # part we will build on hot100_date &lt;- hot100 # peek at the result hot100_date %&gt;% glimpse() ## Rows: 327,895 ## Columns: 10 ## $ url &lt;chr&gt; &quot;http://www.billboard.com/charts/hot-100/1965-0… ## $ week_id &lt;chr&gt; &quot;7/17/1965&quot;, &quot;7/24/1965&quot;, &quot;7/31/1965&quot;, &quot;8/7/196… ## $ week_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9… ## $ song &lt;chr&gt; &quot;Don&#39;t Just Stand There&quot;, &quot;Don&#39;t Just Stand The… ## $ performer &lt;chr&gt; &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patt… ## $ song_id &lt;chr&gt; &quot;Don&#39;t Just Stand TherePatty Duke&quot;, &quot;Don&#39;t Just… ## $ instance &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ previous_week_position &lt;dbl&gt; 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9… ## $ peak_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,… ## $ weeks_on_chart &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, … A refresher to break this down: I have a comment starting with # to explain the first part of the code We are taking the hot100 object and pushing it into a new object called hot100_date. I have a blank line for clarity Another comment We glimpse the new hot100_date object so we can see changes as we work on it. To be clear, we haven’t changed any data yet. We just created a new tibble like the old tibble. 3.10.1 Working with mutate() We are going to replace our current date field week_id with a converted date. We use a dplyr function mutate() to do this, with some help from lubridate. dplyr is the tidyverse package of functions to manipulate data. We’ll use it a lot. It is loaded with the library(tidyverse). Let’s explain how mutate works first. Mutate changes every value in a column. We can either create a new column or overwrite an existing one. Within the mutate function, we name the new thing first (confusing I know) and the set the value of that new thing. # this is psuedo code. Don&#39;t run it. data %&gt;% mutate( newcol = oldcol ) That new value could be arrived at through math or any combination of other functions. In our case, we want to convert our old text-based date to a real date, and then assign it back to the “new” column, but really we are overwriting the existing one. Some notes about the above: It might seem weird to list the new thing first when we are changing it, but that is how R works in this case. You’ll see that pattern elsewhere, like we have already with assigning data into tibbles. We need to be careful when we overwrite data. In this case I feel comfortable doing so because we are creating a new tibble at the same time, so I still have my original data in my project. I strategically used returns to make the code more readable. This code would work the same if it were all on the same line, but writing it this way helps me understand it. RStudio will help you indent properly this as you type. (Easier to show than explain.) Edit your chunk to add the changes below and run it. I implore you to type the changes so you see how RStudio helps you write it. Use tab completion, etc. # part we will build on hot100_date &lt;- hot100 %&gt;% mutate( week_id = mdy(week_id) ) # peek at the result hot100_date %&gt;% glimpse() ## Rows: 327,895 ## Columns: 10 ## $ url &lt;chr&gt; &quot;http://www.billboard.com/charts/hot-100/1965-0… ## $ week_id &lt;date&gt; 1965-07-17, 1965-07-24, 1965-07-31, 1965-08-07… ## $ week_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9… ## $ song &lt;chr&gt; &quot;Don&#39;t Just Stand There&quot;, &quot;Don&#39;t Just Stand The… ## $ performer &lt;chr&gt; &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patty Duke&quot;, &quot;Patt… ## $ song_id &lt;chr&gt; &quot;Don&#39;t Just Stand TherePatty Duke&quot;, &quot;Don&#39;t Just… ## $ instance &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ previous_week_position &lt;dbl&gt; 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9… ## $ peak_position &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,… ## $ weeks_on_chart &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, … What we did there with your mutate() function was name our new column but we used the name of an existing one week_id so it will really replace the data. We replaced it with a lubridate function which I’ll explain next. Lubridate allows us to parse text and then turn it into a date if we supply the order of the date values in the original data. Our original date was something like “07/17/1965.” That is month, followed by day, followed by year. The lubridate function mdy() converts that text into a real date, which properly shows as YYYY-MM-DD, or year then month then day. Lubridate is smart enough to figure out if you have / or - between your values in the original date. If your original text is in a different date order, then you look up what function you need. I typically use the cheatsheet that you’ll find on the lubridate page. You’ll find them in the PARSE DATE-TIMES section. 3.11 Arrange the data If you inspect our newish week_id in your glimpse return, you’ll notice the first record starts in “1965-07-17” but our data goes back to 1958. We want to sort our data by the oldest records first using arrange(). We will use the %&gt;% and then the arrange function, feeding it our data (implied with the pipe) and the columns we wish to sort by. Edit your chunk to the following to add the arrange() function: # part we will build on hot100_date &lt;- hot100 %&gt;% mutate( week_id = mdy(week_id) ) %&gt;% arrange(week_id, week_position) # peek at the result hot100_date %&gt;% glimpse() ## Rows: 327,895 ## Columns: 10 ## $ url &lt;chr&gt; &quot;http://www.billboard.com/charts/hot-100/1958-0… ## $ week_id &lt;date&gt; 1958-08-02, 1958-08-02, 1958-08-02, 1958-08-02… ## $ week_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ song &lt;chr&gt; &quot;Poor Little Fool&quot;, &quot;Patricia&quot;, &quot;Splish Splash&quot;… ## $ performer &lt;chr&gt; &quot;Ricky Nelson&quot;, &quot;Perez Prado And His Orchestra&quot;… ## $ song_id &lt;chr&gt; &quot;Poor Little FoolRicky Nelson&quot;, &quot;PatriciaPerez … ## $ instance &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ previous_week_position &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ peak_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ weeks_on_chart &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… Now when you look at the glimpse, the first record is from “1958-08-02” and the first week_position is “1,” which is the top of the chart. Just to see this all clearly in table form, we’ll print the top of the table to our screen so we can see it. Add a line of text in your notebook explaining your are looking at the table. Add a new code chunk and add the following. The result will look different in your notebook vs this book. hot100_date %&gt;% head(10) ## # A tibble: 10 × 10 ## url week_id week_position song performer song_id instance ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 http://ww… 1958-08-02 1 Poor L… Ricky Nelson Poor Littl… 1 ## 2 http://ww… 1958-08-02 2 Patric… Perez Prado… PatriciaPe… 1 ## 3 http://ww… 1958-08-02 3 Splish… Bobby Darin Splish Spl… 1 ## 4 http://ww… 1958-08-02 4 Hard H… Elvis Presl… Hard Heade… 1 ## 5 http://ww… 1958-08-02 5 When Kalin Twins WhenKalin … 1 ## 6 http://ww… 1958-08-02 6 Rebel-… Duane Eddy … Rebel-&#39;rou… 1 ## 7 http://ww… 1958-08-02 7 Yakety… The Coasters Yakety Yak… 1 ## 8 http://ww… 1958-08-02 8 My Tru… Jack Scott My True Lo… 1 ## 9 http://ww… 1958-08-02 9 Willie… The Johnny … Willie And… 1 ## 10 http://ww… 1958-08-02 10 Fever Peggy Lee FeverPeggy… 1 ## # … with 3 more variables: previous_week_position &lt;dbl&gt;, peak_position &lt;dbl&gt;, ## # weeks_on_chart &lt;dbl&gt; This just prints the first 10 lines of the data. Use the arrows to look at the other columns of the data (which you can’t see in the book). 3.12 Selecting columns We don’t need all of these columns for our analysis, so we are going to select only the ones we need. This will make our exported data file smaller. To understand the concept, you can review the Select video in the Basic Data Journalism Functions series. It boils down to this: We are selecting only the columns we need. In doing so, we will drop url, song_id and instance. Add a Markdown headline: ## Selecting columns. Explain in text we are tightening the date to only the columns we need. Add the code below and then I’ll explain it. hot100_tight &lt;- hot100_date %&gt;% select( -url, -song_id, -instance ) hot100_tight %&gt;% glimpse() ## Rows: 327,895 ## Columns: 7 ## $ week_id &lt;date&gt; 1958-08-02, 1958-08-02, 1958-08-02, 1958-08-02… ## $ week_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ song &lt;chr&gt; &quot;Poor Little Fool&quot;, &quot;Patricia&quot;, &quot;Splish Splash&quot;… ## $ performer &lt;chr&gt; &quot;Ricky Nelson&quot;, &quot;Perez Prado And His Orchestra&quot;… ## $ previous_week_position &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ peak_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ weeks_on_chart &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… In our code we: Name our new tibble Assign to a result of the hot100_date tibble In that tibble, we use the select statement to remove (using -) certain columns. Alternately, we could just name the columns we want to keep without the - sign. But there were fewer to remove than keep. To be clear, there are other ways to use select() this that use less code, but I want to be as straigtforward as possible at this point. It’s pretty powerful. 3.13 Exporting data 3.13.1 Single-notebook philosophy I have a pretty strong opinion that you should be able to open any RNotebook in your project and run it from top to bottom without it breaking. In short, one notebook should not be dependent on the previous running of another open notebook. This is why I had you name this notebook 01-import.Rmd with a number 1 at the beginning. We’ll number our notebooks in the order they should be run. It’s an indication that before we can use the notebook 02-analysis (next lesson!) that the 01-import.Rmd notebook has to be run first. I use 01- instead of just 1- in case there are more than nine notebooks. I want them to appear in order in my files directory. I’m anal retentive. So we will create an exported file from our first notebook that can be used in the second one. Once we create that file, the second notebook can be opened and run at any time. Why make this so complicated? The answer is consistency. When you follow the same structure with each project, you quickly know how to dive into that project at a later date. If everyone on your team uses the same structure, you can dive into your teammates code because you already know how it is organized. If we separate our importing/cleaning into it’s own file to be used by many other notebooks, we can fix future cleaning problems in ONE place instead of many places. One last example to belabor the point: It can save time. I’ve had import/cleaning notebooks that took 20 minutes to process. Imagine if I had to run that every time I wanted to rebuild my analysis notebook. Instead, the import notebook spits out clean file that can be imported in a fraction of that time. This was all a long-winded way of saying we are going to export our data now. 3.13.2 Exporting as rds We are able to pass cleaned data between notebooks because of a native R data format called .rds. When we export in this format it saves not only rows and columns, but also the data types. (If we exported as CSV, we would potentially have to re-fix the date or other data types when we imported again.) We will use another readr function called write_rds() to create our file to pass along to the next notebook, saving the data into the data-processed folder we created earlier. We are separating it from our data-raw folder because “Thou shalt not change raw data” even by accident. By always writing data to this different folder, we help avoid accidentally overwriting our original data. Create a Markdown headline ## Exports and write a description that you are exporting files to .rds. Add a new code chunk and add the following code: hot100_tight %&gt;% write_rds(&quot;data-processed/01-hot100.rds&quot;) So, we are starting with the hot100_tight tibble that we saved earlier. We then pipe %&gt;% the result of that into a new function write_rds(). In addition to the data, the function needs to where to save the file, so in quotes we give the path to where and what we want to call the file: \"data-processed/hot100.rds\". Remember, we are saving in data-processed because we never export into data-raw. We are naming the file starting with 01- to indicate to our future selves that this output came from our first notebook. We then name it, and use the .rds extension. 3.14 Naming chunks I didn’t want to break our flow of work to explain this earlier, but I want you to name all your chunks so you can use a nice feature in RStudio to jump up and down your notebook. Let me show you and example of why first. Look at the bottom of your window above the console and you’ll see a dropdown window. Click on that. Here is mine, but yours wlll be different: RStudio bookmarks You’ll notice that my chunks have names, but yours probably don’t. It’s pretty helpful have these names so you know what the chunk does. You can use this menu to skip up and down the notebook. How to name a chunk? Well, I can’t show you in code because it is not rendered in the book, but here is a picture: Named chunks See where I have {r download}? I named it that because that is what the chunk does. Chunk names can’t have spaces. Use a single word or - or _ between words. There are other configurations we can do here, but that is for later. Go back through your notebook and name all your chunks. Under the Run menu, choose Restart R and run all chunks. Make sure that your Notebook ran all the way from top to bottom. The order of stuff in the notebook matters and you can make errors as you edit up and down the notebook. You always want to do this before you finish a notebook. 3.15 Knit your page Lastly, we want to Knit your notebook so you can see the pretty HTML verison. Next to the Preview menu in the notebook tool bar, click the little dropdown to see the knitting options. Choose Knit to HTML. Knit to HTML After you do this, the menu will probably change to just Knit and you can just click on it to knit again. This will open a nice reader-friendly version of your notebook. You could send that file (called 01-import.html) to your editor and they could open it in a web browser. I use these knit files to publish my work on Github, but it is a bit more involved to do all that so we’ll skip it at least for now.) 3.16 Review of what we’ve learned so far Most of this lesson has been about importing and combining data, with some data mutating thrown in for fun. (OK, I have an odd sense of what fun is.) Importing data into R (or any data science program) can sometimes be quite challenging, depending on the circumstances. Here we were working with well-formed data, but we still used quite a few tools from the tidyverse universe like readr (read_csv, write_rds) and dplyr (select, mutate). Here are the functions we used and what they do. Most are linked to documentation sites: install.packages() downloads an R package to your computer. Typically executed from within the Console and only once per computer. We installed the tidyverse, janitor and lubridate packages. library() loads a package. You need it for each package in each notebook, like library(tidyverse). read_csv() imports a csv file. You want that one, not read.csv. clean_names() is a function in the janitor package that standardizes column names. glimpse() is a view of your data where you can see all of the column names, their data type and a few examples of the data. head() prints the first 6 rows of your data unless you specify a different integer within the function. mutate() changes data. You can create new columns or overwrite existing ones. mdy() is a lubridate function to convert text into a date. There are other functions for different date orders. select() selects columns in your tibble. You can list all the columns to keep, or use - to remove columns. There are many variations. write_rds() writes data out to a file in a format that preserves data types. 3.17 What’s next Importing data is just the first step of exploring a data set. We’ll work through the next chapter before we turn in any work on this. Please reach out to me if you have questions on what you’ve done so far. These are important skills you’ll use on future projects. "],["wrangle.html", "Chapter 4 Wrangle 4.1 Goals of this lesson 4.2 About the story 4.3 Setting up an analysis notebook 4.4 Introducing dplyr 4.5 Most appearances 4.6 Most songs to reach No. 1 4.7 No. 1 hits in last five years 4.8 Top 10 hits overall 4.9 Most appearances at any position 4.10 Most weeks at No. 1 4.11 Review of what we’ve learned 4.12 Turn in your project 4.13 Soundtrack for this assignment", " Chapter 4 Wrangle This chapter continues the Billboard Hot 100 project. In the previous chapter we downloaded, imported and cleaned the data. We’ll be working in the same project. 4.1 Goals of this lesson Dive deeper into dplyr functions to filter and summarize data. 4.2 About the story Now that we have the Billboard Hot 100 charts data in our project it’s time to find the answers to the following questions: Who are the 10 performers with the most appearances on the Hot 100 chart at any position? Which performer had the most songs reach No. 1? Which performer had the most songs reach No. 1 in the most recent five years? Which performer had the most Top 10 hits overall? Which performer/song combination has been on the charts the most number of weeks at any position? Which performer/song combination was No. 1 for the most number of weeks? What are your guesses for the questions above? No peeking! Before we can get into the analysis, we need to set up a new notebook. 4.3 Setting up an analysis notebook At the end of the last notebook we exported our clean data as an .rds file. We’ll now create a new notebook and import that data. It will be much easier this time. If you don’t already have it open, go ahead and open your Billboard project. If your import notebook is still open, go ahead and close it. Use the + menu to start a new **RNotebook*. Update the title as “Billboard analysis” and then remove all the boilerplate text below the YAML metadata. Save the file as 02-analysis.Rmd in your project folder. Check your Environment tab (top right) and make sure the Data pane is empty. We don’t want to have any leftover data. If there is, then go under the Run menu and choose Restart R and Clear Output. Since we are starting a new notebook, we need to set up a few things. First up we want to list our goals. Add a headline and text describing the goals of this notebook. You are exploring the Billboard Hot 100 charts data. Go ahead and copy all the questions outlined above into your notebook. Start each line with a - or * followed by a space. Now add a headline (two hashes) called Setup. Add a chunk, also name it “setup” and add the tidyverse library. Run the chunk to load the library. library(tidyverse) 4.3.1 Import the data on your own In this next part I want you to think about how you’ve did the import in the last notebook and I want you to: Write a section to import the data using read_rds() and put it into a tibble called hot100. Yes, it is true that we haven’t talked about read_rds() yet but it works exactly the same way as read_csv(), so you should try to figure it out. Here are some hints and guides: Start a new section with a headline and text to say what you are doing Don’t forget to name your code chunk (this should all be getting familiar). read_rds() works the same was as read_csv(). The path should be data-processed/01-hot100.rds if you did the previous notebook properly. Remember the tibble needs to be named first and read data pushed into it. Add a glimpse to the chunk so you can refer to the data. Try real hard first before clicking here for the answer hot100 &lt;- read_rds(&quot;data-processed/01-hot100.rds&quot;) # peek at the data hot100 %&gt;% glimpse() ## Rows: 327,895 ## Columns: 7 ## $ week_id &lt;date&gt; 1958-08-02, 1958-08-02, 1958-08-02, 1958-08-02… ## $ week_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ song &lt;chr&gt; &quot;Poor Little Fool&quot;, &quot;Patricia&quot;, &quot;Splish Splash&quot;… ## $ performer &lt;chr&gt; &quot;Ricky Nelson&quot;, &quot;Perez Prado And His Orchestra&quot;… ## $ previous_week_position &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ peak_position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ weeks_on_chart &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… 4.4 Introducing dplyr One of the packages within the tidyverse is dplyr. Dplyr allows us to transform our data frames in ways that let us explore the data and prepare it for visualizing. It’s the R equivalent of common Excel functions like sort, filter and pivoting. There is a cheatsheet on the dplyr that you might find useful. dplyr. Images courtesy Hadley and Charlotte Wickham We’ve used select(), mutate() and arrange() already, but we’ll introduce more dplyr functions in this chapter. 4.5 Most appearances Our first question: Who are the 10 performers with the most appearances on the Hot 100 chart at any position? 4.5.1 Group &amp; Aggregate Before we dive into the code, let’s review this video about “Group and Aggregate” to get a handle on the concept. Let’s work through the logic of what we need to do to get our answer before I explain exactly how. Each row in the data is one song on the chart. Each of those rows has the performer which is the person(s) who performed it. To figure out how many times a performer is in the data, we need to count the rows with the same performer. We’ll use the tidyverse’s version of Group and Aggregate to get this answer. It is actually two different functions within dplyr that often work together: summarize() and it’s companion group_by(). 4.5.2 Summarize summarize() and summarise() are the same function, as R supports both the American and UK spelling of summarize. I don’t care which you use. We’ll start with summarize() first because it can stand alone. The summarize() function computes tables about your data. Our logic above has us wanting a “summary” of how many times certain performers appear in data, hence we use this function. Here is an example in a different context: Learn about your data with summarize() Much like the mutate() function we used earlier, we list the name of the new column first, then assign to it the function we want to accomplish using =. The example above is giving us two summaries: It is applying a function mean() (or average) on all the values in the lifeExp column, and then again with min(), the lowest life expectancy in the data. Let me show you a similar example with our data answer this question: Let’s find the average “peak_position” of all songs on the charts through history: hot100 %&gt;% summarize(mean_position = mean(peak_position)) ## # A tibble: 1 × 1 ## mean_position ## &lt;dbl&gt; ## 1 41.4 Meaning the average song on the charts tops out at No. 41. This is an admittedly simplistic view of the average peak_position since the same song will be listed multiple times with possibly new peak_positions, but hopefully you get the idea. But in our case we want to count the number of rows, and there is a function for that: n(). (Think “number of observations or rows.”) Let’s write the code and run it on our code, then I’ll explain: Set up a new section with a headline, text and empty code chunk. Inside the code chunk, add the following: hot100 %&gt;% summarize(appearances = n()) ## # A tibble: 1 × 1 ## appearances ## &lt;int&gt; ## 1 327895 We start with the tibble first and then pipe into summarize(). Within the function, we define our summary: We name the new column “appearances” because that is a descriptive column name for our result. We set that new column to count the number of rows. Basically we are summarizing the total number of rows in the data. AN ASIDE: I often break up the inside a summarize() into new lines so they are easier to read. # you don&#39;t have to do this here, but know # it is helpful when you have more than one summary hot100 %&gt;% summarize( appearances = n() ) But your are asking: Professor, we want to count the performers, right? This is where summarize()’s close friend group_by() comes in. 4.5.3 Group by Here is a weird thing about group_by(): It is always followed by another function. It really just pre-sorts data into groups so that whatever function is applied after happens within each individual group. If add a group_by() on our performers before our summarize function, it will put all of the “Aerosmith” rows together, then all the “Bad Company” rows together, etc. and then we count the rows within those groups. Modify your code block to add the group_by: hot100 %&gt;% group_by(performer) %&gt;% summarize(appearances = n()) ## # A tibble: 10,061 × 2 ## performer appearances ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;? (Question Mark) &amp; The Mysterians&quot; 33 ## 2 &quot;&#39;N Sync&quot; 172 ## 3 &quot;&#39;N Sync &amp; Gloria Estefan&quot; 20 ## 4 &quot;&#39;N Sync Featuring Nelly&quot; 20 ## 5 &quot;&#39;Til Tuesday&quot; 53 ## 6 &quot;\\&quot;Groove\\&quot; Holmes&quot; 14 ## 7 &quot;\\&quot;Little\\&quot; Jimmy Dickens&quot; 10 ## 8 &quot;\\&quot;Pookie\\&quot; Hudson&quot; 1 ## 9 &quot;\\&quot;Weird Al\\&quot; Yankovic&quot; 91 ## 10 &quot;(+44)&quot; 1 ## # … with 10,051 more rows What we get in return is a summarized table that shows all 10,000+ different performers that have been on the charts, and number of rows in which they appear in the data. That’s great, but who had the most? 4.5.4 Arrange the results Remember in our last notebook when we had to sort the songs by date. We’ll use the same arrange() function here, but we’ll change the result to descending order, because journalists almost always want to know the most of something. Add the pipe and arrange function below and run it, then I’ll explain. hot100 %&gt;% group_by(performer) %&gt;% summarize(appearances = n()) %&gt;% arrange(appearances %&gt;% desc()) ## # A tibble: 10,061 × 2 ## performer appearances ## &lt;chr&gt; &lt;int&gt; ## 1 Taylor Swift 1022 ## 2 Elton John 889 ## 3 Madonna 857 ## 4 Kenny Chesney 758 ## 5 Drake 746 ## 6 Tim McGraw 731 ## 7 Keith Urban 673 ## 8 Stevie Wonder 659 ## 9 Rod Stewart 657 ## 10 Mariah Carey 621 ## # … with 10,051 more rows We added the arrange() function and fed it the column of “appearances.” If we left it with just that, then it would list the smallest values first. Within the arrange function we piped the “appearances” part into another function: desc() to change the order. So if you read that line in English it would be “arrange by (appearances AND THEN descending order).” 4.5.5 Get the top of the list We’ve printed 10,000 rows of data into our notebook when we really only wanted the Top 10 or so. You might think it doesn’t matter, but your knitted HTML file will store all that data and can make it a big file (like in megabytes), so I try to avoid that when I can. We can use the head() command again to get our Top 10. Pipe the result into head() function set to 10 rows. hot100 %&gt;% group_by(performer) %&gt;% summarize(appearances = n()) %&gt;% arrange(appearances %&gt;% desc()) %&gt;% head(10) ## # A tibble: 10 × 2 ## performer appearances ## &lt;chr&gt; &lt;int&gt; ## 1 Taylor Swift 1022 ## 2 Elton John 889 ## 3 Madonna 857 ## 4 Kenny Chesney 758 ## 5 Drake 746 ## 6 Tim McGraw 731 ## 7 Keith Urban 673 ## 8 Stevie Wonder 659 ## 9 Rod Stewart 657 ## 10 Mariah Carey 621 If I was to explain the code above in English, I would descibe it as this: We start with the hot100 data AND THEN we group the data by performer AND THEN we summarize it by counting the number of rows in each group, calling the count “appearances” AND THEN we arrange the result by appearances in descending order AND THEN we kept just the first 10 rows Since we have our answer here and we’re not using the result later, we don’t need to create a new tibble or anything. 4.5.6 A shortcut: count() You are going to think I’m a horrible person, but there is an easier way to do this … We count stuff in data science (and journalism) all the time. Because of this tidyverse has a shortcut to group and count rows of data. I needed to show you the long way because a) we will use group_by() and summarize() with other math that isn’t just counting rows, and b) you need to understand what is happening when you use count(), which is really just using group_by/summarize underneath. The count() function takes the columns you want to group and then does the summarize on n() for you: hot100 %&gt;% count(performer) ## # A tibble: 10,061 × 2 ## performer n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;? (Question Mark) &amp; The Mysterians&quot; 33 ## 2 &quot;&#39;N Sync&quot; 172 ## 3 &quot;&#39;N Sync &amp; Gloria Estefan&quot; 20 ## 4 &quot;&#39;N Sync Featuring Nelly&quot; 20 ## 5 &quot;&#39;Til Tuesday&quot; 53 ## 6 &quot;\\&quot;Groove\\&quot; Holmes&quot; 14 ## 7 &quot;\\&quot;Little\\&quot; Jimmy Dickens&quot; 10 ## 8 &quot;\\&quot;Pookie\\&quot; Hudson&quot; 1 ## 9 &quot;\\&quot;Weird Al\\&quot; Yankovic&quot; 91 ## 10 &quot;(+44)&quot; 1 ## # … with 10,051 more rows To get the same pretty table you still have to rename the new column and reverse the sort, you just do it differently as arguments within the count() function. You must still pipe into head() to limit the output. You can view the count() options here. hot100 %&gt;% count(performer, name = &quot;appearances&quot;, sort = TRUE) %&gt;% head(10) ## # A tibble: 10 × 2 ## performer appearances ## &lt;chr&gt; &lt;int&gt; ## 1 Taylor Swift 1022 ## 2 Elton John 889 ## 3 Madonna 857 ## 4 Kenny Chesney 758 ## 5 Drake 746 ## 6 Tim McGraw 731 ## 7 Keith Urban 673 ## 8 Stevie Wonder 659 ## 9 Rod Stewart 657 ## 10 Mariah Carey 621 So you have to do the same things get the nice Top 10 table, but when you just need a quick count to get an answer, then count() is brilliant. AN IMPORTANT NOTE: The list we’ve created here is based on unique performer names, and as such considers collaborations separately. For instance, Drake is near the top of the list but those are only songs he performed alone and not the many, many collaborations he has had with other performers. So, songs by “Drake” are counted separately than “Drake featuring Future” and “Future featuring Drake.” You’ll need to make this clear when you write your data drop in a later assignment. So, Taylor Swift … is that who you guessed? A little history here, Swift past Elton John in the summer of 2019. Elton John has been around a long time, but Swift’s popularity at a young age, plus changes in how Billboard counts plays in the modern era (like streaming) has rocketed her to the top. (Sorry, Rocket Man). 4.6 Most songs to reach No. 1 Our second quest is to find “Which performer has the most No. 1 hits?” The answer might be easier to guess, but perhaps not. Again, let’s think through the logic of what we have to do to get our answer: We need to consider only No. 1 songs. Because a song could be No. 1 for more than one week, we need to consider the same song/performer combination only once. Once we have all the unique No. 1 songs in a list, then we can count how many times a performer is on the list. To solve our first criteria, we need to learn a new function: filter(). 4.6.1 Filter Filtering is one of those Basic Data Journalism Functions: The filter() function of dplyr reduces the number of rows in our data based on one or more criteria. The syntax works like this: # this is psuedo code. don&#39;t run it data %&gt;% filter(variable comparison value) # example hot100 %&gt;% filter(performer == &quot;Judas Priest&quot;) The filter() function works in this order: What is the variable (or column) you are searching in. What is the comparison you want to do. Equal to? Greater than? What is the observation (or value in the data) you are looking for? Note the two equals signs == there. It’s important to use two of them when you are looking for “equals,” as a single = will not work, as that means something else in R. 4.6.2 Comparisons: Logical tests There are a number of these logical test for the comparison: Operator Definition x &lt; y Less than x &gt; y Greater than x == y Equal to x &lt;= y Less than or equal to x &gt;= y Greater than or equal to x !- y Not equal to x %in% c(y,z) In a group is.na(x) Is NA !is.na(x) Is not NA 4.6.3 Filter for week_position Now that we’ve seen how filter works, let’s see it in action: Start a new section with a headline and text of what you are doing. Add a code chunk and add the following: hot100 %&gt;% filter(week_position == 1) ## # A tibble: 3,279 × 7 ## week_id week_position song performer previous_week_p… peak_position ## &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1958-08-02 1 Poor Lit… Ricky Nels… NA 1 ## 2 1958-08-09 1 Poor Lit… Ricky Nels… 1 1 ## 3 1958-08-16 1 Nel Blu … Domenico M… 2 1 ## 4 1958-08-23 1 Little S… The Elegan… 2 1 ## 5 1958-08-30 1 Nel Blu … Domenico M… 2 1 ## 6 1958-09-06 1 Nel Blu … Domenico M… 1 1 ## 7 1958-09-13 1 Nel Blu … Domenico M… 1 1 ## 8 1958-09-20 1 Nel Blu … Domenico M… 1 1 ## 9 1958-09-27 1 It&#39;s All… Tommy Edwa… 3 1 ## 10 1958-10-04 1 It&#39;s All… Tommy Edwa… 1 1 ## # … with 3,269 more rows, and 1 more variable: weeks_on_chart &lt;dbl&gt; Since we are filtering on a number 1 and not the text character “1” we don’t put the value in quotes. If you are looking for text, you need to put it in quotes: performer == \"Robert Plant\". 4.6.4 Complex filters Don’t do these, but you’ll need them for reference later: If you want single expression with multiple criteria, you write two equations and combine with &amp;. Only rows with both sides being true are returned. # gives you only Poor Little Fool rows where song is No. 1, but not any other position filter(song == &quot;Poor Little Fool&quot; &amp; week_position == 1) If you want an “or” filter, then you write two equations with a | between them. | is the Shift of the \\ key above Return on your keyboard. That | character is also sometimes called a “pipe,” which gets confusing in R with %&gt;%.) # gives you Taylor or Drake songs filter(performer == &quot;Taylor Swift&quot; | performer == &quot;Drake&quot;) If you have multiple criteria, you separate them with a comma ,. Note I’ve also added returns to make it more readable. # gives us rows with either Taylor Swift or Drake, but only those at No. 1 filter( performer == &quot;Taylor Swift&quot; | performer == &quot;Drake&quot;, week_position == 1 ) Getting back to our quest to find the artist with the most No. 1 hits … If you look at our original filter result, you now only have No. 1 songs. But also note that Ricky Nelson’s Poor Little Fool is listed more than once (as are other songs.) That’s because it was No. 1 on more than one week. 4.6.5 Using distinct() Our next challenge in our logic is to show only unique song and performer combinations. We do this with distinct(). We feed the distinct() function with the variables we want to consider together, in our case the song and performer. All other columns are dropped since including them would mess up their distinctness. Add the distinct() function to your code chunk. hot100 %&gt;% filter(week_position == 1) %&gt;% distinct(song, performer) ## # A tibble: 1,124 × 2 ## song performer ## &lt;chr&gt; &lt;chr&gt; ## 1 Poor Little Fool Ricky Nelson ## 2 Nel Blu Dipinto Di Blu (Volaré) Domenico Modugno ## 3 Little Star The Elegants ## 4 It&#39;s All In The Game Tommy Edwards ## 5 It&#39;s Only Make Believe Conway Twitty ## 6 Tom Dooley The Kingston Trio ## 7 To Know Him, Is To Love Him The Teddy Bears ## 8 The Chipmunk Song The Chipmunks With David Seville ## 9 Smoke Gets In Your Eyes The Platters ## 10 Stagger Lee Lloyd Price ## # … with 1,114 more rows Now we have a list of just No. 1 songs! 4.6.6 Summarize the performers Now that we have our list of No. 1 songs, we can “count” the number of times a performer is in the list to know how many No. 1 songs they have. We’ll use the group_by/summarize combination for this. We group_by() the performer, and then we find the number n() of rows in each group. Lastly, we need to arrange() to get the most on the top. Add the steps for the group_by, summarize and arrange functions: hot100 %&gt;% filter(week_position == 1) %&gt;% distinct(song, performer) %&gt;% group_by(performer) %&gt;% summarize(no1_hits = n()) %&gt;% arrange(no1_hits %&gt;% desc()) ## # A tibble: 744 × 2 ## performer no1_hits ## &lt;chr&gt; &lt;int&gt; ## 1 The Beatles 19 ## 2 Mariah Carey 16 ## 3 Madonna 12 ## 4 Michael Jackson 11 ## 5 Whitney Houston 11 ## 6 The Supremes 10 ## 7 Bee Gees 9 ## 8 The Rolling Stones 8 ## 9 Janet Jackson 7 ## 10 Stevie Wonder 7 ## # … with 734 more rows 4.6.7 Filter after summary OK, we’re going to do one last thing here … we don’t need all 744 rows of our summary We could use head() to include just the top 10 rows, but what if our “break” is in the middle of tied records? (i.e., if the 11th record “Daryl Hall John Oates” also had 7 chart toppers. They didn’t, but we’ll have to deal with other ties like this later.) Instead of taking the Top 10 rows, we are instead going to filter the rows to those that have 7 hits or more. We are doing this to show you can not only filter your data before your group, but you can also filter on the result after your summary. Modify your code to add the last filter. I’ve commented all the lines to explain what they do. You should do the same in your own words. hot100 %&gt;% filter(week_position == 1) %&gt;% # filters to no 1 songs distinct(song, performer) %&gt;% # gets a unique list group_by(performer) %&gt;% # groups by the performer summarize(no1_hits = n()) %&gt;% # gets number or rows in groups, names new column arrange(no1_hits %&gt;% desc()) %&gt;% # sorts by highest no_hits filter(no1_hits &gt;= 7) # filters the result to 7+ ## # A tibble: 10 × 2 ## performer no1_hits ## &lt;chr&gt; &lt;int&gt; ## 1 The Beatles 19 ## 2 Mariah Carey 16 ## 3 Madonna 12 ## 4 Michael Jackson 11 ## 5 Whitney Houston 11 ## 6 The Supremes 10 ## 7 Bee Gees 9 ## 8 The Rolling Stones 8 ## 9 Janet Jackson 7 ## 10 Stevie Wonder 7 4.6.8 Try count on your own OK, in the first challenge I also showed you how you can get this same result using count(). I want you to try and do this on your own. Create a new code block and find the same answer using count() instead of group_by, summarize and arrange. Try real hard first before clicking here for the answer hot100 %&gt;% filter(week_position == 1) %&gt;% distinct(song, performer) %&gt;% count(performer, sort = T, name = &quot;no1_hits&quot;) %&gt;% filter(no1_hits &gt;= 7) ## # A tibble: 10 × 2 ## performer no1_hits ## &lt;chr&gt; &lt;int&gt; ## 1 The Beatles 19 ## 2 Mariah Carey 16 ## 3 Madonna 12 ## 4 Michael Jackson 11 ## 5 Whitney Houston 11 ## 6 The Supremes 10 ## 7 Bee Gees 9 ## 8 The Rolling Stones 8 ## 9 Janet Jackson 7 ## 10 Stevie Wonder 7 4.7 No. 1 hits in last five years Which performer had the most songs reach No. 1 in the most recent five years? This time I’ll have you try the count() method on your own with some hints, but you’ll need to do the group/summarize method all on your own. Let’s talk through the logic. This is very similar to the No. 1 hits above but with two differences: In addition to filtering for No. 1 songs, we also want to filter for songs in 2016-2020. We might need to adjust our last filter for a better “break point.” We haven’t talked about filtering dates, so let me tell you this: You can use filter operations on dates just like you do any other text. This will give you rows after 2015. filter(week_id &gt; &quot;2015-12-31&quot;) Go back and review the “Complex filters” section where I showed how to combine more than one filter and apply that logic here. There is more than one correct answer. Here’s the task: Build a summary table that gets the most No. 1 hits in the last five years. Exclude results with a single No. 1 hit. Use the count() method. No, really. Try it on your own first. hot100 %&gt;% filter( week_position == 1, week_id &gt; &quot;2015-12-31&quot; ) %&gt;% distinct(song, performer) %&gt;% count(performer, sort = T, name = &quot;top_hits&quot;) %&gt;% filter(top_hits &gt; 1) ## # A tibble: 10 × 2 ## performer top_hits ## &lt;chr&gt; &lt;int&gt; ## 1 Drake 5 ## 2 Ariana Grande 3 ## 3 Taylor Swift 3 ## 4 BTS 2 ## 5 Cardi B 2 ## 6 Ed Sheeran 2 ## 7 Justin Bieber 2 ## 8 Olivia Rodrigo 2 ## 9 The Weeknd 2 ## 10 Travis Scott 2 4.7.1 Group on your own Find the same results as above, but using the group_by() and summarize() method instead of count. No hints this time ;-). 4.8 Top 10 hits overall Which performer had the most Top 10 hits overall? This time we’ll talk through the logic, but you have to figure out the answer on your own using the group_by/summarize method. The logic is very similar to the “most No. 1 hits” quest you did above, but you need to adjust your filter to find songs within position 1 through 10. Don’t overthink it, but do recognize that the “top” of the charts are smaller numbers. Make a new section Describe what you are doing Do it using the group_by/summarize method and have a logical number or rows. (i.e., don’t stop at a tie) 4.9 Most appearances at any position Which performer/song combination has been on the charts the most number of weeks at any position? OK, this time we’ll talk through the logic, but you have to give your own answer using the count() method. The logic is actually straightforward: We wan to count combinations over two columsn: song, performer. When you give a count (or group_by) more then one column, it will group rows where the values are the same in all columns. i.e. all rows with both “Rush” as a performer and Tom Sawyer as a song. Once you have a summary table, sort it with the most appearances on the top and then filter it to a logical stopping place. So, here’s the quest: Start a new section with a proper headline and description Use the count() method to find the song/performer combination with the most rows, sorting it properly within count. Filter the summary table to a logical end. 4.10 Most weeks at No. 1 This last quest is all on your own. You need to figure out the logic and then execute it. You can use any method you want (group_by/summarize or count), but make sure the table is sorted correctly and ends logically. Start a section and describe it Make a list of songs counting the number of weeks at No. 1. Sort by the most at top and end the table logically. 4.11 Review of what we’ve learned We introduced a number of new functions in this lesson, most of them from the dplyr package. Mostly we filtered and summarized our data. Here are the functions we introduced in this chapter, many with links to documentation: filter() returns only rows that meet logical criteria you specify. summarize() builds a summary table about your data. You can count rows n() or do math on numerical values, like mean(). group_by() is often used with summarize() to put data into groups before building a summary table based on the groups. count() is a shorthand for the group_by/summarize operation to count rows based on groups. You can name your summary columns and sort the data within the same function. distinct() returns rows based on unique values in columns you specify. i.e., it deduplicates data. 4.12 Turn in your project Make sure everything runs properly (Restart R and Run All Chunks) and then Knit to HTML. Zip the folder. Upload to the Canvas assignment. 4.13 Soundtrack for this assignment This lesson was constructed with the vibes of The Bright Light Social Hour. They’ve never had a song on the Hot 100 (at least not through 2020). "],["wrangle-mastery.html", "Chapter 5 Wrangle mastery 5.1 Military surplus transfers 5.2 The LESO program 5.3 About the data 5.4 Some helpful hints 5.5 The questions to answer and write about 5.6 How to tackle the summaries 5.7 Technical help 5.8 Write a data drop", " Chapter 5 Wrangle mastery Now that you’ve gone through the data wrangling process — importing data, cleaning it and querying it for answers — it’s time to do this on your own with a new data set. You will also write a data drop about your answers. 5.1 Military surplus transfers In June 2020, Buzzfeed published the story Police Departments Have Received Hundreds Of Millions Of Dollars In Military Equipment Since Ferguson about the amount of military equipment transferred to local law enforcement agencies since Michael Brown was killed in Ferguson, Missouri. After Brown’s death there was a public outcry after “what appeared to be a massively disproportionate show of force during protests brought scrutiny to a federal program that transfers unused military equipment to local law enforcement.” Reporter John Templon used data from the Law Enforcement Support Office for the update on the program and published his data analysis, which he did in Python. You will analyze the same data focusing on some local police agencies and write a short data drop about transfers to those agencies. 5.2 The LESO program The Defense Logistics Agency transfers surplus military equipment to local law enforcement through its Law Enforcement Support Office. You can find more information about the program here. The agency updates the data quarterly and the data I’ve collected contains transfers through June 30, 2021. The original file is linked from the headline “ALASKA - WYOMING AND US TERRITORIES.” The data there comes in an Excel spreadsheet that has a new sheet for each state. I used R to pull the data from each sheet and combine it into a single data set and I’ll cover the process I used in class, but you won’t have to do that part. I will supply a link to the combined data below. 5.3 About the data There is no data dictionary or record layout included with the data but I have corresponded with the Defense Logistics Agency to get a decent understanding of what is included. sheet: Which sheet the data came from. This is an artifact from the data merging script. state: A two-letter designation for the state of the agency. agency_name: This is the agency that got the equipment. nsn: A special number that identifies the item. It is not germane to this specific assignment. item_name: The item transferred. Googling the names can sometimes yield more info on specific items. quantity: The number of the “units” the agency received. ui: Unit of measurement (item, kit, etc.) acquisition_value: a cost per unit for the item. demil_code: Another special code not germane to this assignment. demil_ic: Another special code not germane to this assignment. ship_date: The date the item(s) were sent to the agency. station_type: What kind of law enforcement agency made the request. Each row of data is a transfer of a particular type of item from the U.S. Department of Defense to a local law enforcement agency. The row includes the type of item, the number of them, and the cost of each unit. This means to get the total value of the items in the shipment you have to multiply the quantity times the acquisition_value. The agencies really only pay the shipping costs, so you can’t say they paid for the items, so the total value you calculate is the “value” of the items, not their cost to the agency. 5.4 Some helpful hints Note there are some “Technical hints” below that you will need to tackle the challenges noted below. Be sure to read those before proceeding. 5.5 The questions to answer and write about Read everything here before you start. All answers should be based on data from Jan. 1, 2010 to present. In addition, only consider Texas agencies as you answer the following. For each agency in Texas, find the total_quantity and total_value of the equipment they received. Are there any that stand out? Why? Like above, find the total_quantity and total_value , but filter the summary to the following local agencies: AUSTIN POLICE DEPT SAN MARCOS POLICE DEPT TRAVIS COUNTY SHERIFFS OFFICE UNIV OF TEXAS SYSTEM POLICE HI_ED WILLIAMSON COUNTY SHERIFF’S OFFICE What is the summed total quantity and summed total value of each item shipped to each local agency listed above. i.e., each agency should have its own table (own code chunk) that lists the unique items, their summed quantity and summed total value. Research some of the more interesting items in the items list (i.e. Google them). You might check explore when they were shipped to the agency. 5.5.1 Requirements of the assignment You are following the same project structure employed with the Billboard project. Create a new project with the proper folder structure. Use separate RNotebooks for downloading/cleaning of the data and the analysis. Name your files logically and save files in proper folders. Each notebook should list the goals at the top. Each section within a notebook should have a headline and description for the task. Each notebook should run independently if you Restart R and Run All Chunks. 5.5.2 Specifics for import/cleaning These are not in a specific order, but make sure you take care of them at some point. Use the download.file() function to download the file into your project. This is similar to what you did in the Billboard project to download the data. The url for the data is https://github.com/utdata/rwd-r-leso/blob/main/data-processed/leso.csv?raw=true. You will also need to supply the path and file name to save the data into, which is the second argument in that function. It should go into a data-raw folder, but you need to add a file name. (nont hot100, but something like leso-data.csv) Once imported and saved into an R object, remove the columns we aren’t using: sheet, nsn, demil_code and demil_ic. this is similar to how we removed columns in the Billboard assignment. Make sure you focus only on Texas records. filter() is your friend here. Make sure you use only records with the dates noted in the questions list. Again, filter(). Make sure you create a new column (mutate()!) that gives the total_value of the equipment for each row. See the Technical note below “Create columns using math from other columns.” Export the cleaned data as an .rds file. 5.5.3 Specifics for analysis The analysis should use the cleaned data from the import notebook. You’ll import this with read_rds(). The summary tables you create require math beyond counting rows, so group_by/summarize are your friend (instead of count()). See the Technical note “Math within summarize.” You should end up with a table for each of the questions in the list. One of the questions requires multiple code chunks (one for each agency listed). 5.6 How to tackle the summaries When I am querying my data, I start by envisioning what the result should look like. Let’s take the first question: For each agency in Texas, find the total_quantity and total_value of the equipment they received. Let’s break this down: “For each agency in Texas.” For all the questions, we only want Texas agencies. In the import notebook, you should filter for only the TX agencies. So, if done right, the TX agencies are already filtered. But the “For each agency” part tells me I need to group_by the agency_name so I can summarize totals within each agency. “find the total_quantity and total_value”: Because I’m looking for a total (or sum() of columns) I need summarize(). So I envision my result looking like this: agency_name item_count equip_value A POLICE DEPT 6419 10825707.5 B SHERIFF’S OFFICE 381 3776291.52 C SHERIFF’S OFFICE 270 3464741.36 D POLICE DEPT 1082 3100420.57 With group_by I can organize all the rows by their agency_name so that my “summarize” can do the math within those groups. data %&gt;% group_by(agency_name) Running that code by itself doesn’t show me anything of worth. It is the summarize that adds the totals for quantity Here is what it looks like once I’ve added that, but see the Technincal help below “Math within summarize” for the details of how it works. data %&gt;% group_by(agency_name) %&gt;% summarize( sum_quantity = sum(quantity), sum_total_value = sum(total_value) ) In that summarize() I have two lines. Within each line I name the new column first and then make it equal the sum of the respective columns I’m adding. 5.7 Technical help There are some operations needed in this assignment that we touched on only tangentially in the Billboard assignment, so I want to provide some specifics here. 5.7.1 Create columns using math from other columns This relates to creating a total_value column in the import notebook to tell you the total value for the items the agency received. When we used mutate() in the Billboard assignment, we were reassigning values in each row if a column back into the same column. This is when we converted the date. In the “import” part of this assignment you’ll need to use mutate() to create a new column with new values based on a calculation – quantity multiplied by the acquisition_value – for each row. So here is a quick discussion on that. If you started with data like this: item item_count item_value Bread 2 1.5 Milk 1 2.75 Beer 3 9 And wanted to create a total value of each item, you would use mutate(): data %&gt;% mutate(total_value = item_count * item_value) And you would get a return like this: item item_count item_value total_value Bread 2 1.5 3 Milk 1 2.75 2.75 Beer 3 9 27 Other math operators work as well: +, -, * and /. 5.7.2 Math within summarize This relates to finding the sum of quantity and total_value. While there was one example of summarizing number values in our Billboard assignment using mean(), you’ll be doing more math summaries in this lesson, so … If you want to summarize to add together all the values within a column you use the sum() function: If you started with the groceries table above and wanted to know the total number of all items and the total cost of all your groceries: data %&gt;% summarize( sum_items_count = sum(item_count), sum_total_value = sum(total_value) ) and you would end up with this: sum_item_count sum_total_value 6 32.75 If you use a group_by before your summary, it will do the math within each group. 5.7.3 Filter from a collection One last thing that might help in this assignment. This relates to the quest for finding the local agencies. When you filter data, you usually choose the column and then compare for some value. To find the rows for “Bread” in our data above, we would use: groceries %&gt;% filter(item == &quot;Bread&quot;) You can also find look for rows that contain any values %in% a collection of terms. If we wanted to find all rows with “Bread” or “Beer,” we could do this: groceries %&gt;% filter(item %in% c(&quot;Bread&quot;, &quot;Beer&quot;)) The c() function is for “combine” to create a collection of values. You can even save that collection in an object and use that in a function: items_important = c(&quot;Bread&quot;, &quot;Beer&quot;) groceries %&gt;% filter(item %in% items_important) 5.8 Write a data drop Once you’ve found answers to all the questions listed, you’ll weave those into a writing assignment. Include this as a Microsoft Word document saved into your project folder along with your notebooks. (If you are a Google Docs fan, you can write there and then export as a Word doc.) You will not be writing a full story … we are just practicing writing a lede and “data sentences” about what you’ve found. You do need to source the data and briefly describe the program but this is not a fully-fleshed story. Just concentrate on how you would write the facts and attribution. Use Microsoft Word and include it with your project when you upload it to canvas. Write a data drop from the data of between four and six paragraphs. Be sure to include attribution about where the data came from. You can pick the lede angle from any of the questions outlined above. Each additional paragraph should describe what you found from the data. Here is a partial example to give you an idea of what I’m looking for. (These numbers may be old and you can’t use this angle ;-)). The Jefferson County Sheriff’s Office is flying high thanks to gifts of over $3.5 million worth of surplus U.S. Department of Defense equipment. Among the items transferred over the past decade to the department was a $923,000 helicopter in October 2016 and related parts the following year, according to data from the Defense Logistics Agency data — the agency that handles the transfers. The sheriff’s office has received the fourth highest value of equipment among any law enforcement agency in Texas since August 2014 despite being a county of only 250,000 people. "],["plots.html", "Chapter 6 Plots 6.1 UPDATES NEEDED 6.2 Goals for this section 6.3 Introduction ggplot 6.4 The basic ggplot template 6.5 Let’s plot! 6.6 Plotting our wells data 6.7 Wells per county over time 6.8 Your turn: Build a line chart 6.9 Review of ggplot 6.10 Plotly for more interactive graphics 6.11 Resources", " Chapter 6 Plots 6.1 UPDATES NEEDED use skittles for testing? (Not j-data) use billboard or military surplus for mastery Fix lists PREVIOUS DRAFT 6.2 Goals for this section An introduction to the Grammar of Graphics We’ll make charts! 6.3 Introduction ggplot ggplot2 is the data visualization library within Hadley Wickham’s tidyverse. It uses a concept called the Grammar of Graphics, the idea that you can build every graph from the same components: a data set, a coordinate system, and geoms – the visual marks that represent data points. With a hat tip to Matt Waite, the main concepts are: data: which data frame you are pulling from aesthetics: the specific data from the data frame which we are going to plot geometries: the shape the data is going to take scales: any transformations we might make on the data layers: how we might lay multiple geometries over top of each other to reveal new information. facets: which means how we might graph many elements of the same data set in the same space The challenge to understand here is for every graphic, we start with the data, and then describe how to layer plots or pieces on top of that data. 6.4 The basic ggplot template The template for a basic plot is this. (The &lt;&gt; denote that we are inserting values there.) ggplot(data = &lt;DATA&gt;, aes(mapping = &lt;MAPPINGS&gt;)) + &lt;GEOM&gt;(&lt;ADDITONAL_MAPPINGS&gt;) ggplot() is our function. We feed into it the data we wish to plot. aes() stands for “aesthetics,” and it describes the column of data we want to plot, and how, like which column is on the x axis and which is on the y axis. These are called mappings, which we show in our template with &lt;MAPPINGS&gt;. They typically look like this: aes(x = col_name_x, y = col_name_y). Now matter what type of chart we are building (bar chart, scatterplot, etc) we have to tell it which columns to show on the chart. The + is the equivalent of %&gt;% in our tidyverse data. It means we are adding a layer, and it should always be at the end of the line, not at the beginning of the next. &lt;GEOM_FUNCTION&gt; is the type of chart or addition we are adding. They all start with the term geom_ like geom_bar, which is what we will build with this example. It will take the mappings we supplied and plot them on the type of geom_ we choose. &lt;ADDITIONAL MAPPINGS&gt; if a geom_ requires it, we can specify additional columns/axis mapping combinations to that geom_. We don’t always have or need them. There are some ways to simplify this, and some ways to complicate it. Let’s simplify first: It is implied that the first thing fed to ggplot is the data, so you don’t have to write out data = unless there is ambiguity. The aes() values are also implied as mappings, so you don’t have to write out mapping = unless there is ambiguity. ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;) + &lt;GEOM&gt; ) One of the ways we make things complicated, is we layer different geometries. We might start with a scatterplot, and then add a reference line on top of it, which is a new geometry. Each goem_ can specify their own mappings. ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;(aes(&lt;SPECIFIC_MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;(aes(&lt;SPECIFIC_MAPPINGS&gt;)) 6.5 Let’s plot! 6.5.1 Set up our Notebook Create a new RNotebook. Title it “Wells visualizations” and name the file 04-charts.Rmd. Load the following libraries: tidyverse, lubridate. library(tidyverse) library(lubridate) 6.5.2 Scatterplot One of the better ways to see this in action for the first time is build a scatterplot showing the relationship between two numbers. Unfortunately, our wells data does not have two such values, so we’ll explore this using a data set that is already built into ggplot2, mpg. Take a look at the mpg by calling it like a data frame. mpg It looks something like this, which shows the first and last couple of rows: manufacturer model displ year cyl trans drv cty hwy fl class audi a4 1.8 1999 4 auto(l5) f 18 29 p compact audi a4 1.8 1999 4 manual(m5) f 21 29 p compact audi a4 2 2008 4 manual(m6) f 20 31 p compact audi a4 2 2008 4 auto(av) f 21 30 p compact … … … … . … . .. .. . … volkswagen passat 2 2008 4 manual(m6) f 21 29 p midsize volkswagen passat 2.8 1999 6 auto(l5) f 16 26 p midsize volkswagen passat 2.8 1999 6 manual(m5) f 18 26 p midsize volkswagen passat 3.6 2008 6 auto(s6) f 17 26 p midsize The data is a subset of fuel economy data from 1999 and 2008 for 38 popular cars. Don’t get too hung up on the data, it is just for examples. The size of an engine is shows in the column displ. The Audi A4 has a 1.8 liter engine. The column hwy is the fuel rating for highways. Well also us the class column, which categorizes the type of vehicle. What kind of relationship might you expect between the size of the engine and highway mileage? Let’s use our plot to show this. If our basic template is like this: ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;) + &lt;GEOM&gt; Now, let’s put our data in here. Our goal here is to show how the hwy number (y axis) changes as the displ number gets bigger (x axis.) ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() Which gets us a our first chart: MPG: hwy vs displ We can see there is a relationships of sort here, but ggplot has some additional geometries to help us see this, including geom_smooth(). Since we have already the mappings in the main ggplot() call, all we have to do is add the new geom. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth() # new plot. don&#39;t forget the + on previous line MPG: displ vs hwy with smooth line Let’s add one more visual cue (or aesthetic) to this graphic by coloring the dots based on the class of the vehicle. Since we want this aesthetic to apply only to the geom_point(), we have to add the aes() value there. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + # added color aesthetic geom_smooth() MPG: disply vs mpg with class Looking at that graphic, what values might you want to learn more about? 6.6 Plotting our wells data For bar charts and line charts, we can return to our wells data, so let’s import what we had from our last notebook. wells &lt;- readRDS(&quot;data-out/wells_03.rds&quot;) 6.6.1 Total wells per county 6.6.1.1 Shape our data If we are plotting wells per county, we need to first build a data frame that counts the number of wells for each county. We can use the same count() function that we used when we cleaned our data. wells_by_county &lt;- wells %&gt;% count(county) %&gt;% rename(wells_count = n) wells_by_county Let’s break this down: The first line creates the new data frame wells_by_county, starting with our wells data frame. We apply the count() function on the “county” column. This makes our basic pivot table. On the third line, we rename the “n” column that was created by count(), so it is more descriptive, calling it wells_count. So now we have a data frame with two columns: county and wells_count. We print it on the fourth line so we can inspect it. 6.6.1.2 Plot our wells by county Here is the verbose plot for our counties. ggplot(data = wells_by_county) + geom_bar(mapping = aes(x = county, y = wells_count), stat = &quot;identity&quot;) On the first line we tell ggplot() that we are using the we wells_by_county data. On the next, we apply the geom_bar() function to make a bar chart. It needs two things: The mapping, which are the aesthetics. We well it to plot county on the x (horizontal) axis, and wells_count on the y (vertical) axis. Because county is a category instead of a number, we have to use the stat = \"identity\" value to describe that we are using values within county to separate the bars. This is a special thing for bar charts. One of those things that drive you nuts. Basic county plot Our less verbose way to do this looks like this: ggplot(wells_by_county, aes(x=county, y=wells_count)) + geom_bar(stat = &quot;identity&quot;) 6.6.2 Add a layer of text labels For each new thing that we add to our graphic, we add it with +. In this case, we want to add number labels to show the wells_count for that county. ggplot(data = wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.25) # adds the numbers on bars Basic county plot In this case, we are just adding another layer, the geom_text(). It requires some additional aesthetics, like what label= we want to use. The vjust= moves the numbers up a little. Change the number and see what happens. The last layer we want to add here is a Title layer. The function for labels is called labs() and it takes an argument of title = \"\" You can also change your x and y axis names, etc. ggplot(data = wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.25) labs(title = &quot;Number of wells drilled by county&quot;) # adds the title Wells by county with title Congratulations! You made your first ggplot() chart. Not particularly revealing, but it does show that Travis County has WAY more wells than the other counties. Let’s see how those trends play out over time. 6.7 Wells per county over time Our next chart will be a line chart to show how the number of wells drilled has changed over time within each county. Again, it will help us to think about what we are after and then build our data frame to match. In this case, we want to plot the “number of wells” for each county, by year. That means we need a data frame that has columns for county, year and the number of wells. To get that, we have to use group and summarize. Sometimes it helps to write out the steps of everything before you to do it. Start with the wells data frame. Filter to 2003 or later, because that’s when the system came online. Group by the county and year_drilled fields. Summarize to create a count of the number of wells_drilled. Set all of the above to a new data frame, wells_county_year. Start a plot with the new data. Set x (horizontal) to be year_drilled and y (vertical) to be wells_drilled, and color to be the county. 6.7.1 Work up the data frame wells %&gt;% filter(year_drilled &gt;= 2003) %&gt;% group_by(county, year_drilled) %&gt;% summarise( wells_drilled = n() ) This gives you a table similar to this: county year_drilled wells_drilled Bastrop 2003 110 Bastrop 2004 99 Bastrop 2005 97 … … … Caldwell 2003 40 Caldwell 2004 32 Caldwell 2005 40 We call this long data, because each row contains a single observation, instead of wide data, which would have a column for each observation. Once you are have the data formatted, set it to fill a new data frame called wells_county_year. 6.7.2 Draw the plot Remember the formula for a basic plot: ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt; and if all our mappings are the same, they can go into the ggplot function. ggplot(wells_county_year, aes(x=year_drilled, y=wells_drilled)) + geom_line(aes(color=county)) + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) Wells drilled by county by year How easy would it be to add points for every year to make each data point stand out? 6.7.3 Your turn: Add layers Add a new layer geom_point() and see what happens Add a labels layer to add a title, like we did in the bar chart above. 6.7.4 Dates as numbers and the problems they cause There was one point during my work on this graphic when my x axis did not fall evenly on years, and I figured it was because the year_drilled field was a number and not a date. It’s possible to fix that by including the library(lubridate) and then mutating the year_drilled column like this: mutate( year_drilled = ymd(year_drilled, truncated=2L) ) %&gt;% 6.8 Your turn: Build a line chart Now, I’d like you to build a line chart that shows how the different kinds of wells drilled has changed over time. Here’s a major hint: It’s very much like the line chart you just built, but with different columns. You’ll need so start at creating a data frame with the correct data. 6.9 Review of ggplot Exploring with graphics are one of the more powerful features of working with R. It takes a bit to get used to the Grammar of Graphics and ggplot2 and it will be frustrating at first. But be assured it can do about anything once you learn how, and being able to fold in these charts with your thoughts and analysis in a repeatable way will make you a better data journalist. By design, every chart in ggplot starts with the same three things: data, a geometric coordinate system, and a mapping of the aesthetics, including the x and y values. ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt; If your graphic is simple, there may be less verbose ways to write it as ggplot will assume your are passing it data first, and that aes() functions are for mapping. 6.10 Plotly for more interactive graphics At the risk of adding yet a little more complexity I want to introduce you to Plotly. I see two ways you might find Plotly interesting: 6.10.1 ggplotly ggplotly allows you to port your ggplot graphic into Plotly so they have interactive tooltips. The tutorial examples are also not bad for a general ggplot reference. After installing and loading the Plotly library, giving your chart hover tips is as easy as assigning your plot to an object (p in the example below), and then calling that with the ggplotly() function: p &lt;- ggplot(mpg, aes(x=displ, y=hwy)) + geom_point() + geom_smooth() ggplotly(p) ggplotly example The black label above appears when you hover on the graphic. 6.10.2 More with plot_ly() function You can gain a little more control over your Plotly graphic if you build them using the plot_ly() function instead of ggplot(), but you have to learn a new syntax. It’s still based on the Grammar of Graphics, so it’s not hard … just different. For example, for our “Wells by County and Year” graphic we did earlier looks like this: wells_county_year %&gt;% plot_ly(x = ~year_drilled, y = ~wells_drilled, name = ~county, type = &quot;scatter&quot;, mode = &quot;lines+markers&quot;) And it ends up looking like this: Plotly example 6.10.3 Plotly’s freemium model It appears that you can use these open source libraries without charge from Plotly. They do also have a hosting service to allow you to embed charts in other websites, which can get into a pay tier of their service. 6.11 Resources The ggplot2 documentation and ggplot2 cheatsheets. R for Data Science, Chap 3. Hadley Wickam dives right into plots in his book. R Graphics Cookbook has lots of example plots. Good to harvest code and see how to do things. The R Graph Gallery another place to see examples. "],["census.html", "Chapter 7 Census 7.1 Goals of this section 7.2 Census Bureau programs and platforms 7.3 Using the new data portal 7.4 Using the Census API in R 7.5 Interactive maps with leaflet 7.6 Resources", " Chapter 7 Census NEEDS REWRITE fix lists The U.S. Census Bureau has a wealth of data that can help journalists tell stories. This chapter is not a comprehensive guide on how to use it, but instead an introduction on some ways you can. But we can’t talk about specifics of how do to anything before learning about the different Census programs, so here is a mini overview of some of the more popular programs. 7.1 Goals of this section Introduce three of the more popular Census data sets and discuss how they differ: Decennial Census American Community Survey Population Estimates Program Discuss differences between American FactFinder and data.census.gov. Introduce and demonstrate packages that use the Census Bureau’s API to pull data into R. Introduce packages that allow static and interactive mapping within R, which often comes into play when using Census data. None of these topics are comprehensive. I have lectured on the census in the past and have multiple lessons using different software if you want more general instruction, or you could just dive in and gain your own experience. 7.2 Census Bureau programs and platforms The Census Bureau has a series of different data “programs” and data sets, many with their own distribution platforms. They are currently in a development effort to combine distribution of all of them into a new, combined platform: data.census.gov. It is a work in progress. That said, the plan is to stop publishing new data to the much-bemoaned American FactFinder, their “old” site, this summer of 2019. As such, I’ll concentrate on data.census.gov. That’s to say that while I’m outlining three different programs here, the data will eventually be found all int he same place, but it’s in a state of flux currently. Your default should be to use data.census.gov first, then go to American FactFinder if you can’t find something. We’ll also discuss some R packages that use the Census API, and the advantages of using them. 7.2.1 Decennial Census Every ten years, the government tries to count everyone in every household in America. The results are used to redraw Congressional Districts, allocate tax dollars and a million other reasons. The number of questions asked each decade has been condensed to center around how many people live in a household, their age, race and ethnicity, their relationship to each other, and if they own or rent the home. The decennial census data is the most accurate snapshot you can get for a single point in time. It’s just limited in the scope of data. Most decennial data has been migrated to the data.census.gov data portal. April 1, 2020 is Census Day for the next count. 7.2.2 American Community Survey The American Community Survey, or ACS, is the method the government uses to collect more detailed data than who lives where. It is quite extensive, with information relevant to almost any beat in Journalism. While the decennial count comes once every ten years, the ACS has been continuously collecting data since 2005. Each year about 1.7% of Americans answer the ACS survey, and those results are distributed at regular intervals each year. This is fine for large geographic areas with 65,000 people or more. But, when you want results for smaller geographic areas or very specific individuals, the bureau combines five years of survey results together so they have enough data to make estimates. This combination of 1-year and 5-year data sets is a trade off: New data each year for large areas, but fuzzier 5-year windows for small areas. Each estimate also comes with a “margin of error,” which represents the upper and lower bounds of the estimate with a 90% confidence rate. You can read more about this in the ACS media guide, but here is a brief example: If an estimate is 2,000 with an MOE of +/- 100, that means that you could take 10 new random samples in that area and the average of those samples would be between 1,900 and 2,100 for nine out of the 10 samples. As journalists, we don’t typically report the margin of error each time we use an estimate, but we do make sure our readers understand the data are based on survey estimates. Most importantly we make sure the MOE is not more than 10% of our estimate if we are basing our story on that number. The ACS allows us to get the most detailed characteristics about our population, like how we travel to work or how much of our income we spend on rent. However, it comes with a degree of uncertainty, especially for smaller geographic areas. It is still very valuable. Most of these tables are available on data.census.gov, with the rest scheduled to be ported this year (2019). New releases will only be published to the data portal. 7.2.3 Population Estimates Program PEP uses current data on births, deaths, and migration to calculate population change since the most recent decennial census. Each year the Census Bureau publishes tables with estimates of population, demographics, and housing units for cities, counties and metro areas. Populations estimates are a great way to see population and demographic changes for large areas. It does not have a margin of error like the ACS because it is based on actual data and not random surveys of a part of the population. These tables are currently only available in FactFinder, but should be ported to the new data.census.gov data portal this year (2019). 7.3 Using the new data portal Again, the data.census.gov data portal is under development and has some known challenges, but it’s the one you should learn first since FactFinder will be retired this year. It is slow, cumbersome and error prone but getting better with each new release. Instead of me writing out directions, I recommend you watch this webinar that demonstrates how to use the portal and outlines the current challenges and development plans for the future. I will provide brief demos of the data portal and FactFinder in class, just so you can see them. 7.3.1 Tips about using downloaded portal data in R When you download a table from the portal, you get a stuffed archive with three files. Here is an example from a 5-year ACS data set for table B19013, which includes median income data: ACSDT5Y2017.B19013_data_with_overlays_2019-04-20T000019.csv is the data. It contains two header rows (arg!) with the first row being coded values for each column. The second row has long descriptions of what is in each column. ACSDT5Y2017.B19013_metadata_2019-04-20T000019.csv is a reference file that gives the code and description for each header in the data. ACSDT5Y2017.B19013_table_title_2019-04-20T000019.txt is a reference file with information about the table. If data is masked or missing, this file will explain the symbols used in the data to describe how and why. The first part of the file names include the program, year and table the data comes from. At the end of the file name is the date and time the data was downloaded from the portal. 7.3.2 Importing downloaded data When I import this data into R, I typically use the read_csv() function and skip the first, less-descriptive row. The second row becomes the headers, which are really long but explain the columns. I then rename them to something shorter. If you are only using selected columns, then you might use select() to get only those you need. Here is an example: tx_income &lt;- read_csv(&quot;data-raw/ACSDT5Y2017.B19013_2019-04-20T000022/ACSDT5Y2017.B19013_data_with_overlays_2019-04-20T000019.csv&quot;, skip = 1) %&gt;% rename( median_income = `Estimate!!Median household income in the past 12 months (in 2017 inflation-adjusted dollars)`, median_income_moe = `Margin of Error!!Median household income in the past 12 months (in 2017 inflation-adjusted dollars)` ) %&gt;% clean_names() Which yields this: id geographic_area_name median_income median_income_moe 0500000US48199 Hardin County, Texas 56131 3351 0500000US48207 Haskell County, Texas 43529 6157 0500000US48227 Howard County, Texas 50855 2162 7.3.3 Fields made to join with other data Pay attention to fields named id or geoid or similar names as these are often fields meant to be joined to other tables. Many use parts of FIPS codes that define specific geographic areas, and allow you to match similar fields in multiple data sets. This is especially important when it comes to mapping data, as these codes are how you join data to “shape files,” which are a data representation of geographic shapes for mapping. While we won’t go into a lot of detail about maps in this lesson, I’ve linked some examples below. You may find you want to join data based on geography names, in which case you might need to use dplyr tools to split and normalize those terms so they match your other data set, like changing “Travis County, Texas” to just “Travis.” 7.4 Using the Census API in R You can also import data into R directly from the Census Bureau using their API. There are a number of packages that do this, and they all work a little differently to solve different challenges. Again, I won’t go into great detail about how to use these, but I’ll show examples and provide links to further self-study. Manually downloading census data is usually a multiple-step and multiple-decision process. An advantage to using the API is you can script that decision-making process for consistency. Different packages also provide the data in different formats, which might be beneficial depending on your goal. 7.4.1 Setting up an API key The Census requires a free API key to use their service. It’s like your personal license, and should not be shared with others. You can sign up for an API key here and then store it on your machine in an .Rnviron file so you don’t have to display it in your code. You only have to set up your API key once on each machine. Once installed, it gets automatically loaded when you restart R. 7.4.2 The censusapi package Hannah Recht of Bloomberg News developed the censusapi to pull data directly from the Census Bureau into R. See that site for more examples and documentation on use. You can view the full notebook of the examples below here. This example code chunk pulls the same data set we manually downloaded from the portal. We are asking for the median income estimate (B19013_001E) and margin of error (B19013_001M), but I’m also including the total population for the county with B01003_001E, which was not in the “B19013” table. This is another advantage to using the API, as we are pulling from multiple tables at once. To do this manually, we would have to search for and download two separate data sets and merge them. tx_income &lt;- getCensus(name = &quot;acs/acs5&quot;, vintage = 2017, vars = c(&quot;NAME&quot;,&quot;B01003_001E&quot;, &quot;B19013_001E&quot;, &quot;B19013_001M&quot;), region = &quot;county:*&quot;, regionin = &quot;state:48&quot;) Which ends up looking like this: state county NAME B01003_001E B19013_001E B19013_001M 48 199 Hardin County, Texas 55993 56131 3351 48 207 Haskell County, Texas 5806 43529 6157 48 227 Howard County, Texas 36491 50855 2162 See how this is similar to the data we imported from file we downloaded from the data portal? In this case we have a state and county field instead of the id, but the shape of the data is the same. Also note that our extra variable for population is added as a new column, which will differ with our next example. Using an API like this to fetch data can take some effort to learn, but the exact “steps” to get the data are recorded in your code. 7.4.3 The tidycensus package Kyle Walker is a professor at TCU who developed the tidycensus package to return census data in tidyverse-ready data frames. He also includes an option to pull the spatial geometry to make maps. You can view this example in full, with the map. With tidycensus we don’t have to specify to get the MOE with his get_acs() function, it just comes. We only supply the two variables we want, those for population and the median income. tx_income &lt;- get_acs( geography = &quot;county&quot;, variables = c(&quot;B01003_001&quot;,&quot;B19013_001&quot;), state = &quot;TX&quot; ) And the result is this: GEOID NAME variable estimate moe 48001 Anderson County, Texas B01003_001 57747 NA 48001 Anderson County, Texas B19013_001 42313 2337 48003 Andrews County, Texas B01003_001 17577 NA 48003 Andrews County, Texas B19013_001 70753 6115 48005 Angelina County, Texas B01003_001 87700 NA 48005 Angelina County, Texas B19013_001 46472 1452 Note how the resulting data is a different shape here. Instead of the table getting wider for each variable added, it gets longer. This is a more “tidy” shape that can potentially be easier to plot or map. 7.4.4 Adding geometry to tidycensus The tidycensus package also allows you to download the geometry or shapes of your data at the same time by adding geometry = TRUE to your tidycensus call. This allows you to quickly make static maps of your data. I’m removing the population variable because we don’t need it for the map. tx_income_map &lt;- get_acs( geography = &quot;county&quot;, variables = c(&quot;B19013_001&quot;), state = &quot;TX&quot;, geometry = TRUE # gets shapes ) Now we can use geom_sf() to plot the estimate value to each county in shapefile in ggplot. ggplot(tx_income_map) + geom_sf(aes(fill=estimate), color=&quot;white&quot;) + theme_void() + theme(panel.grid.major = element_line(colour = &#39;transparent&#39;)) + scale_fill_distiller(palette=&quot;Oranges&quot;, direction=1, name=&quot;Median income&quot;) + labs(title=&quot;2017 median income in Texas counties&quot;, caption=&quot;Source: Census Bureau/ACS5 2017&quot;) Which yields this: Map from tidy census That’s basically two lines of code to pull data and build a map. 7.4.5 Map shapes only with tigris If you already have the Census data, or perhaps data that is not from the census but has a county name or one of the other geographic code values, then you can use Walker’s tigris package to get just the shapefiles. The censusapi example I have finishes out by pulling tigiris data to make the same median income map as above. 7.5 Interactive maps with leaflet Here is a tutorial that walks through creating an interactive map of median income by census tract using the leaflet, mapview, tigris and acs packages. It’s a pretty basic map best used for exploration, but it’s pretty neat and not too hard to make. 7.6 Resources Some other resources not already mentioned: R Census guide Sharon Machlis guide Mapping Census Bureau Data in R with Choroplethr by package creator Ari Lamstein Using the R Package RankingProject to Make Simple Visualizations for Comparing Populations by former Census Bureau statistician Jerzy Wieczorek Thematic maps tutorial. Baltimore Sun example story and code. Christine Zhang says \"Sometimes I prefer the output of one over the other (censusapi vs tidycensus) which is why I alternate. Spatial Data Science with R Tutorial. Not a tutorial but but this post by Timo Grossenbacher is an explanation and inspiration on how far you can take R in mapping. Here is a version that uses U.S. Census Bureau data. "],["tidy.html", "Chapter 8 Tidy data 8.1 UPDATES NEEDED 8.2 Goals for this section 8.3 What is tidy data 8.4 Tidyr package 8.5 The tidyr verbs 8.6 Set up the mixbev project 8.7 Create an explore notebook 8.8 What might we learn about this dataset 8.9 Add years and months values 8.10 Campus bars 8.11 West Campus student hot spots 8.12 Campus-area bar sales over time 8.13 Types of sales within student bars 8.14 How gather() works 8.15 Applying gather() to beer, wine and liquor 8.16 The spread() function 8.17 Practice assignment: Exploring the top seller 8.18 Bonus: Top 5 sellers in Travis County over three years 8.19 Bonus: Total sales by county", " Chapter 8 Tidy data 8.1 UPDATES NEEDED Introduce acc data. Leave mixbev for an assignment. fix lists PREVIOUS DRAFT Data “shape” can be important when you are trying to work with and visualize data. In this chapter we’ll discuss “tidy” data and how this style of organization helps us. Slides by Hadley Wickham are used with permission from the author. 8.2 Goals for this section Explore what it means to have “tidy” data. Learn gather(), spread() and other tidyr verbs. Use Mixed Beverage Gross Receipts to explore shaping data. We’ll introduce the RSocrata package to get the data. Explore and chart the alcohol data to practice our skills. 8.3 What is tidy data “Tidy” data is well formatted so each variable is in a column, each observation is in a row and each value is a cell. Our first step in working with any data is to make sure we are “tidy.” Tidy data definition It’s easiest to see the difference through examples. The data frame below is of tuberculosis reports from the World Health Organization. Each row is a set of observations (or case) from a single country for a single year. Each column describes a unique variable. The year, the number of cases and the population of the country at that time. A tidy table Table2 below isn’t tidy. The count column contains two different type of values. A tidy table When our data is tidy, it is easy to manipulate. We can use functions like mutate() to calculate new values for each case. Manipulate a tidy table 8.4 Tidyr package When our data is tidy, we can use the tidyr package to reshape the layout of our data to suit our needs. In the figure below, the table on the left is “wide.” There are are multiple year columns describing the same variable. It might be useful if we want to calculate the difference of the values for two different years. It’s less useful if we want plot on a graphic because we don’t have columns to map as X and Y values. The table on the right is “long,” in that each column describes a single variable. It is this shape we need when we want to plot values on a chart. We can then set our “Year” column as an X axis, our “n” column on our Y axis, and group by the “Country.” Wide vs long 8.5 The tidyr verbs We’ll use functions within the tidyr package to manipulate data to our liking, depending on our need. Tidy verbs 8.6 Set up the mixbev project We’re going to work in a new project with new data for this assignment. I will try to get you up and running as quickly as possible. In RStudio, choose File &gt; New Project Walk through the steps to create a New Project in a New Directory called yourname-mixbev. Once you have your project, create a new RNotebook. Save the file and name it 01-mixbev-import.Rmd. Go to this link and copy the text and replace everything in your RNotebook. There are a couple of things we need to do before you run this notebook: In the R Console, run install.packages(\"RSocrata\") Use the Files pane to create a new folder called data-raw so we have a place to save our data. Now use Cmd-option-R (or go to Run &gt; Run All) to run the notebook. Running that notebook will download three years of data from Travis County establishments and save it into your data-raw folder in your project. How that is done is all documented in that notebook, but we may spend some time in class explaining going over it. 8.7 Create an explore notebook Now that we have data we don’t have to download it again. Let’s create a new RNotebook to import and explore it (and learn Tidyr while we are at it). Create a new RNotebook. Save the file as 02-mixbev-explore.Rmd. Update the title in the metadata. Remove the boilerplate below the data. Add the code below, then Restart R and Run All Chunks. library(tidyverse) library(lubridate) library(scales) # import the data receipts &lt;- readRDS(&quot;data-raw/receipts_api.rds&quot;) 8.7.1 Peruse the data In the environment window, click on the receipts data frame so it opens and you can look at the data. Some key things to know about the data: Columns with location_ are about a specific restaurant or bar selling alcohol. Columns with taxpayer_ are about the owners of that establishment. The monetary amounts for _receipts are total sales numbers for that establishment in that month. The obligation_end_date is the last day of the month for those sales. The liquor type sales like beer and wine should all add up to the total_receipts, but sometimes type sales are blank. I don’t trust cover_charge_receipts at all. There are several other columns we won’t deal with in this lesson. So these are NOT the number of drinks sold. It’s the amount of money brought in for the total sale of each type of liquor within that month. The total_receipts is used to calculate tax paid to the state on those sales based on a formula. See the record layout on Socrata for more information. 8.8 What might we learn about this dataset If we look at the data set, there are a series of questions we might ask it. How have total sales changed over the past three years? Who has sold the most alcohol over the past three years? Which campus-area bars have sold the most, and what are their sales trends? Do these campus-area bars sell more beer, wine or liquor? Have the number of establishments that sell alcohol increased? If so, where? There are others for sure, but for this lesson we’ll concentrate on campus-area bars you might be familiar with. 8.9 Add years and months values We will end up doing a lot of summaries based on year and month of the data. It will be easier to do that if we create some new columns that have those values. receipts &lt;- receipts %&gt;% mutate( # sales_year = year(obligation_end_date_yyyymmdd), sales_year = year(obligation_end_date_yyyymmdd) %&gt;% as.character(), sales_month = month(obligation_end_date_yyyymmdd, label = TRUE) ) I specifically coerced sales_year into a string because the year as a continuous number was causing problems with plots later when I got an axis mark for “2016.5.” Probably not the best solution, but it worked. 8.10 Campus bars Let’s take a look at sales around campus. Make a data frame of just the receipts from the 78705 area. uni_area &lt;- receipts %&gt;% filter( location_zip == &#39;78705&#39; ) # peek at the result uni_area %&gt;% head() 8.10.1 Total 78705 sales leaders in 2018 Let’s make a quick table to add total_receipts for 2018 so we can find the top selling bars from last year. We can do that by filtering for 2018, grouping by location and address, and then summing together the total receipts. uni_area %&gt;% filter(sales_year == 2018) %&gt;% group_by(location_name, location_address) %&gt;% summarize( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) %&gt;% head(10) And we get this: location_name location_address total_sales EXECUTIVE EDUCATION AND CONFERENCE CENTER 1900 UNIVERSITY AVE 1684571 SPIDER HOUSE 2908 FRUTH ST 1575812 CAIN &amp; ABEL’S 2313 RIO GRANDE ST 1051230 TRUDY’S TEXAS STAR CAFE 409 W 30TH ST 1012269 THE HOLE IN THE WALL 2538 GUADALUPE ST 827799 HOTEL ELLA/GOODAL’S KITCHEN &amp; BAR 1900 RIO GRANDE ST 707431 DOC’S MOTORWORKS 38TH 1106 W 38TH ST 561019 THE LOCAL PUB AND PATIO 2610 GUADALUPE ST 544261 VIA 313 PIZZA RESTAURANT II 3016 GUADALUPE ST 531878 THE BACK LOT 606 MAIDEN LN 494638 8.11 West Campus student hot spots Looking at the list above, let’s filter our original data to some top student hangouts that we know are still open. student_bars &lt;- receipts %&gt;% filter( location_name %in% c( &quot;SPIDER HOUSE&quot;, &quot;CAIN &amp; ABEL&#39;S&quot;, &quot;TRUDY&#39;S TEXAS STAR CAFE&quot;, &quot;THE HOLE IN THE WALL&quot;, &quot;THE LOCAL PUB AND PATIO&quot; ) ) # check the results student_bars %&gt;% count(location_name, location_address) Note the filter() function above. In order to feed in a list of location names into the filter, I used the %in% operator (instead of ==) and I put list of locations into a c() function. The C stands for concatenate, FWIW. After creating the student_bars data frame above, I used count() to make sure we caught all the bars and made sure each had the same number of reports. Which looks like this: location_name location_address n CAIN &amp; ABEL’S 2313 RIO GRANDE ST 36 SPIDER HOUSE 2908 FRUTH ST 36 THE HOLE IN THE WALL 2538 GUADALUPE ST 36 THE LOCAL PUB AND PATIO 2610 GUADALUPE ST 36 TRUDY’S TEXAS STAR CAFE 409 W 30TH ST 36 Note I didn’t save the count() function back to the data frame. I just viewed that to the notebook so we could check our work. 8.12 Campus-area bar sales over time 8.12.1 Visualizing your visualization Now, if we are interested in charting it helps to think about what we need and how to shape our data to get it. Let’s start with charting how sales at each bar have changed over the past three years. Since we are looking at value over time for multiple things, a line chart will probably work best. If our basic line chart works like this: ggplot(&lt;DATA&gt;, aes(x=&lt;COL_VALUE&gt;, y=&lt;COL_VALUE&gt;, group=&lt;COL_VALUE&gt;)) + geom_line() We need to figure out how to configure our data to fit the chart. For the “X” value (horizontal) we have the sales_year field to track over time. For the “Y” value (vertical) we want the total sales of each bar for that year. We’ll need to do a summary to get that. We want a line for each campus bar, which means we need a column for location_name for our “group,” too. We create this data frame by using our group_by() on location_name and sales_year and then summarize() to get our total sales by our grouping: student_bars_grouped &lt;- student_bars %&gt;% group_by(location_name, sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) # peek at the results student_bars_grouped %&gt;% head() Which ends up looking this this: location_name sales_year total_sales CAIN &amp; ABEL’S 2016 896114 CAIN &amp; ABEL’S 2017 990631 CAIN &amp; ABEL’S 2018 1051230 SPIDER HOUSE 2016 1797800 SPIDER HOUSE 2017 1721580 SPIDER HOUSE 2018 1575812 Now we can plug in our columns to get our chart: ggplot(student_bars_grouped, aes(x=sales_year, y=total_sales, group=location_name)) + geom_line(aes(color=location_name)) + geom_point() + scale_y_continuous(labels=comma) Campus bar sales over time We’ve added a couple of do-dads to our line chart to make it prettier, like adding a color aesthetic the geom_line() function to color the lines and give us a legend, and a scale with labels to make the Y numbers pretty. 8.12.2 What did we find? Looking at the chart, we can see that Cain &amp; Abel’s is the only top campus-area bar with an increase in alcohol sales over each of the past three years. The Hole in the Wall did have a good 2018, reversing a downward trend. Everywhere else dropped sales each year. 8.13 Types of sales within student bars If we want to see how beer, liquor and wine sales differ at an establishment, we could use a very similar chart, but we would want each line to be the type of alcohol: beer, wine or liquor. But this is how our columns look right now. Beer, wine and liquor are separate columns. Sales columns This is where we introduce gather() from tidyr so we can shape our data to meet our needs. 8.14 How gather() works The gather() function is what we use to change wide data into long data. We are “gathering” all the extra columns into two: one for the value and one to describe it. In the example below, a key column is created called “Year” and a value column called “n” is created to hold the values from each of the yearly columns. For each value, a new row is created, and the column name is used for the “key” and the data is use for the “value.” Show how gather works We can see how this works by tracking a single value from one shape to the other: Show gather with single value Now, how do we define this in code? Gather as a function We are, of course, starting with the data frame and then piping it into the gather() function. The first value we have to give the function is to name our key column. Name the key column And the next value we give it is name of the value column. Name the value column Then we have to give it the range of columns that we want to gather. You can define those in different ways. The first method is a number range of the columns, starting in order of the columns in the data frame. So, for our example above, we want the second, third and fourth column, so we use the range 2:4. Name the range Or, we could supply those columns in other ways, like by their column names: Range as column names Or by specifying which columns we don’t want to gather: Range through deselect 8.15 Applying gather() to beer, wine and liquor Let’s start by making a new data frame with just the columns we need to work with. We are starting with the student_bars data frame we created that has the three years of receipts from just our five campus-area bars: student_sales &lt;- student_bars %&gt;% select(location_name, sales_year, beer_receipts, wine_receipts, liquor_receipts) %&gt;% rename( beer = beer_receipts, wine = wine_receipts, liquor = liquor_receipts ) student_sales %&gt;% head() # to show our results We get a result like this: location_name sales_year beer wine liquor THE LOCAL PUB AND PATIO 2018 24832 163 14202 THE LOCAL PUB AND PATIO 2017 27702 470 16086 THE HOLE IN THE WALL 2018 30374 370 32533 THE HOLE IN THE WALL 2017 34232 174 32789 THE HOLE IN THE WALL 2018 49320 252 42080 CAIN &amp; ABEL’S 2016 34239 456 42543 Now, if we can “gather” the beer, wine and liquor columns into two new columns – alcohol type and sales amount – then we could group and sum by the year and type. The code below builds the new data frame we’ll use for our chart (student_sales_grouped) but you skip the beginning data frame assignment and walk through it line by line to see how it gets built, starting with the student_sales data frame. Once you have it, then assign it back to student_sales_group. student_sales_grouped &lt;- student_sales %&gt;% gather(alcohol_type, sales_by_type, 3:5) %&gt;% group_by(location_name, sales_year, alcohol_type) %&gt;% summarise( sales_sum_year = sum(sales_by_type) ) # peek at the result student_sales_grouped %&gt;% head() We start with the student_sales data frame, where each row has the sales for each month of the year in multiple columns. gather() names our fields “alcohol_type” and “sales_by_type” and gathers our beer, wine and liquor columns. At this point, each row of data has the individual sale of a type of alcohol for each month of each year. group_by() collects our data by the name, year and type. This prepares us to: summarize() adds together all the beer sales for each month are summed up into one row for each year. 8.15.1 Plot sales by type for a single campus-area bar Let’s figure out the line chart with one establishment first: cain &lt;- student_sales_grouped %&gt;% filter(location_name == &quot;CAIN &amp; ABEL&#39;S&quot;) ggplot(cain, aes(x=sales_year, y=sales_sum_year, group=alcohol_type, color=alcohol_type)) + geom_line() We get our chart like this: Cain &amp; Abel’s alcohol sales 8.15.2 Plot sales by type for multiple campus-area bars For this chart, we are going to duplicate what we did for the graphic above, and then instead of applying to the cain data frame, we’ll substitute in our our student_sales_grouped data frame so have all the West Campus bars. And now we can introduce the facet_wrap(), which allows you to duplicate a graphic based on one of the categories in the data. We feed the facet_wrap() function with the variable (the column name) we want to duplicate. Weirdly, it has to start with a tilde, like this: facet_wrap(~location_name). ggplot(student_sales_grouped, aes(x=sales_year, y=sales_sum_year, group=alcohol_type, color=alcohol_type)) + geom_line() + facet_wrap(~location_name) + scale_y_continuous(label=comma) Student bars as a facet wrap 8.15.3 What did we learn? Liquor sales are what is driving Cain &amp; Abel’s increase in revenue. Wine sales are pretty low everywhere and Trudy’s really sells a lot of liquor. It’s those Mexican Martinis and Margaritas, I imagine. 8.16 The spread() function There is another tidyr function called spread() that does the opposite of gather(). We won’t do an example with this lesson, but this is how it generally works. The spread() function A spread() example 8.17 Practice assignment: Exploring the top seller This assignment looks a little further into some specific establishments from the Mixed Beverage Data. Here are the directions, with lots of hints thrown in. Create a new notebook called “03-practice.Rmd.” Use the same libraries and data as the in-class assignment. Find the establishment with highest total receipts in last three years. See the first Bonus section below for the the code. For the top establishment found, create a bar chart with sales by year. Here are some hints: Start with receipts, then filter by the location_name and the location_address. (You have to include location_address because there is more than one location in Ausitn for this chain.) Assign that to a data frame that you can use going forward. Group by sales_year, then summarize to get the sum(total_receipts). Review Summarize if needed. Save the result into a new data frame. Use ggplot() with geom_bar(stat=\"identity\"). Review the less verbose wells by county for an example of how to write the plot. For that top establishment, create a bar chart with total_receipts each month. Some hints: Don’t over think this. You already have a filtered data set, and you can plot the x axis on the obligation_end_date_yyyymmdd. Because the chart shows trends but it’s hard to see the values for the top months, create a table showing the top monthly total sales by month. Some hints: Select the oblibgation_end_date_yyyymmdd and arrange the total_receipts in descending order. Show just the top by piping into head() For that establishment, create a line chart with sales by alcohol type by year. This one is more challenging, but you have everything you need in the chapter when we did this for Cain &amp; Abel’s. Here are some hints: You can start from the the filtered data that you did with the previous chart. Create a new data frame with just the columns you need: select name, sales_year, beer, wine, liquor Use gather() to collect the beer, wine, liquor columns as alcohol_type and sales_by_type. This is the same as we did in class. Group by sales_year, alcohol_type. Summarize to get the sales_sum_year. Plot as a geom_line chart with x=sales_year, y=sales_sum_year, group and color as alcohol_type. Lastly, to show that once you have code you can re-purpose it, create a line chart of sales by alcohol type by year for the Circuit of the Americas. Your hint: Create a new data frame with just the Circuit of the Americas data. Copy the steps from the last chart, and change out the data frame name. 8.18 Bonus: Top 5 sellers in Travis County over three years Because some chains might have that same location_name but more than one location, we need to group by both the location_name and the location_address fields. Then we can summarize their total sales. receipts %&gt;% group_by(location_name, location_address) %&gt;% summarise( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) %&gt;% head(5) Which gives you something like this: location_name location_address total_sales WLS BEVERAGE CO 110 E 2ND ST 35878211 RYAN SANDERS SPORTS 9201 CIRCUIT OF THE AMERICAS BLVD 20714630 W HOTEL AUSTIN 200 LAVACA ST 15435458 ROSE ROOM/ 77 DEGREE 11500 ROCK ROSE AVE 14726420 THE DOGWOOD DOMAIN 11420 ROCK ROSE AVE STE 700 14231072 What exactly is that top location? (Google it). It looks like that location (hint: a hotel) has sold more that twice the amount as the next hotel. What is the second location? That could be pretty interesting to look further into as well. 8.19 Bonus: Total sales by county So, are alcohol sales going up in Travis County? We can use our new sales_year column to group the data and summarize by the sum of total_sales. receipts %&gt;% group_by(sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) If you want to see the result, we can plot as a bar chart. I’m repeating our code above, but then shoving it into a new data frame, which we use to plot. receipts_group_all &lt;- receipts %&gt;% group_by(sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) ggplot(receipts_group_all, aes(x=sales_year, y=total_sales)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(labels=comma) The last line in the plot comes from the scales package and it gives us pretty numbers on the Y axis. If you are writing a story about alcohol sales in Travis County over the last three years, you can say that sales have risen over each year. You might go further and compare the sales to the population over the same time period, but we’ll skip that for now. "],["review-plots.html", "Chapter 9 Review: Plots 9.1 UPDATES NEEDED 9.2 Project setup 9.3 The story 9.4 Explore the data 9.5 About joins 9.6 Build our scatterplot 9.7 How does Texas compare? 9.8 Correlaton test for Texas 9.9 Practice 1: Compare Penn State 9.10 Practice 2: Penalties vs scoring 9.11 Using bind_rows() to merge data sets", " Chapter 9 Review: Plots 9.1 UPDATES NEEDED This football data, perhaps? This is end exclude/joins fix lists This lesson is heavily cribbed from a lesson in Matt Waite’s Sports Data Journalism course at the University of Nebraska. 9.2 Project setup Create your project. Call it yourname-football. Create a data-raw folder so you have a place to download the files. Start a new notebook. 9.2.1 Download the data You can download the files we are using based on their URLs in a Github repository. Once you’ve run this block of code, comment out the three download.file() lines by putting a # at the beginning. You really only need to download the files once. # Downloads the files. Convert to comments once you&#39;ve done this: download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/master/football-compare/penalties.csv?raw=true&quot;, &quot;data-raw/penalties.csv&quot;) download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/master/football-compare/scoring_offense.csv?raw=true&quot;, &quot;data-raw/scoring_offense.csv&quot;) download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/master/football-compare/third_down_conversion.csv?raw=true&quot;, &quot;data-raw/third_down_conversion.csv&quot;) Import the three files: # import the files scoring &lt;- read_csv(&quot;data-raw/scoring_offense.csv&quot;) %&gt;% clean_names() thirddown &lt;- read_csv(&quot;data-raw/third_down_conversion.csv&quot;) %&gt;% clean_names() 9.3 The story The data we are using comes from cfbstats, a website for college football statistics. We will be comparing how third-down conversions might correlate to a football team’s scoring offense. 9.4 Explore the data We have two data sets here. 9.4.1 Third-down conversions year: Year. Goes from 2009-2018 name: Team name. There are 131 different teams. g: Number of games played attempts: Third-down attempts conversions: Third-down attempts that were successful conversion_percent: conversions/attempts * 100 9.4.2 Scoring year: Year name: Team name g: Number of games played td: Touchdowns fg: Field goals x1xp: 1pt PAT made x2xp: 2pt PAT made safety: Safeties points: Total points scored points_g: Points per game Now our goal here is to compare how the conversion_percent might relate to points_g for all teams, and how specific teams might buck the national trend. 9.5 About joins To make our plot, we need to join the two data sets on common fields. We want to start with the scoring data frame, and then add all the columns from the thirddown data frame. We want to do this based on the same year and team. There are several types of joins. We describe these as left vs right based on which table we reference first (which is the left one). In the figure below, we can see which matching records are retained based on the type of join we use. Types of joins In our case we only want records that match on both year and name, so we’ll use an inner_join(). The syntax works like this: new_dataframe &lt;- *_join(first_df, second_df, by = field_name_to_join_on) If you want to use more than one field in the by part like we do, then you define them in a concatenated list: by = \"field1\", \"field2\"). If the fields you are joining on are not named the same, then you can define the relationships: by = c(\"a\" = \"b\"). For our project we want to use an inner_join(). Add the code below to your notebook along with notes describing that you are joining the two data sets: offense &lt;- inner_join(scoring, thirddown, by=c(&quot;year&quot;, &quot;name&quot;)) # peak at the new data frame offense %&gt;% head() So, to break this down: Our new combined dataframe will be called offense. We’ll be doing an inner_join(), which is just keep matching records. (They all match, FWIW.) Our “left” table is scoring and our “right” table is thirddown. We are joining on both the year and name columns. Anytime you do a join (or a bind as described below), check the resulting number of rows and columns to make sure they pass the logic test. 9.6 Build our scatterplot We’re trying to show the relationship between conversion_percent and points_g, so we can use those as our x and y values in a geom_point() graphic. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point() First scatterplot 9.6.1 Add a fit line We can see by the shape of the dots that indeed, as conversion percentage goes up, points go up. In statistics, there is something called a fit line – the line that predicts what happens given the data. There’s lot of fit lines we can use but the easiest to understand is a straight line. It’s like linear algebra – for each increase or decrease in x, we get an increase or decrease in x. To get the fit line, we add geom_smooth() with a method. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point() + geom_smooth(method=lm, se=FALSE) # adds fit line First scatterplot The lm means linear method. The se=FALSE removes the confidence interval (based on the standard error) of the prediction. See the geom_smooth() for more information. 9.6.2 Run a correlation test So we can see how important third down conversions are to scoring. But still, how important? For this, we’re going to dip our toes into statistics. We want to find out the correlation coefficient (specifically the Pearson Correlation Coefficient. That will tell us how related our two numbers are. We do that using cor.test(), which is part of R core. cor.test(offense$conversion_percent, offense$points_g) The result is: Pearson&#39;s product-moment correlation data: offense$conversion_percent and offense$points_g t = 30.82, df = 1251, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.6242939 0.6873331 sample estimates: cor 0.6569602 That bottom number is the key. If we square it, we then know exactly how much of scoring can be predicted by third down conversions. (0.6569602 * 0.6569602) * 100 Which gets us 43.15967. So what that says is that 43 percent of a team’s score is predicted by their third down percentage. That’s nearly half. In social science, anything above 10 percent is shockingly predictive. So this is huge if this were human behavior research. In football, it’s not that surprising, but we now know how much is predicted. 9.7 How does Texas compare? Let’s compare how Texas does vs the field by plotting their results on top of the national stats. Create a data frame of the Texas data. tx &lt;- offense %&gt;% filter(name == &quot;Texas&quot;) And now we’ll add some layers to our ggplot graphic. We’re also editing our original geom_point() and geom_smooth() layers to make them light grey so that the Texas plots stand out more. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point(color = &quot;light grey&quot;) + # adds light grey color geom_smooth(method=lm, se=FALSE, color = &quot;light grey&quot;) + # adds light grey color geom_point(data = tx, aes(x = conversion_percent, y = points_g), color = &quot;#bf5700&quot;) + # adds Texas points colored burnt orange, of course geom_text(data = tx, aes(x = conversion_percent, y = points_g, label = year)) # adds year labels to see Adding Texas to plot This is good, but the labels for the year are sitting on top of the values. There is an R package called ggrepel that will move those labels off the numbers, which we’ll use with our next set of changes. You might have to run install.packages('ggrepel') to make this work. For this update, we are doing a number of things, adding or modifying layers along the way: Add a goem_smooth() fit line specific to Texas, in burnt orange. We’ll put it before the text so it shows underneath the labels. Modify the geom_text to geom_text_repel to move the labels off the points. Add labs() for a title and such to finish out our graphic. Add theme_minimal() just to improve the looks. ggplot(aes(x = conversion_percent, y = points_g)) + geom_point(color = &quot;light grey&quot;) + geom_smooth(method=lm, se=FALSE, color = &quot;light grey&quot;) + geom_point(data = tx, aes(x = conversion_percent, y = points_g), color = &quot;#bf5700&quot;) + geom_smooth(data = tx, aes(x = conversion_percent, y = points_g), method=lm, se=F, color = &quot;#bf5700&quot;) + geom_text_repel(data = tx, aes(x = conversion_percent, y = points_g, label = year)) + labs(x=&quot;Third-down conversion rate&quot;, y=&quot;Points per game&quot;, title=&quot;Texas&#39; third down success predicts scoreboard&quot;, subtitle=&quot;In 2018 the Longhorns were 18th in the FBS for third down conversions.&quot;, caption=&quot;Source: NCAA&quot;) + theme_minimal() Finished Texas graphic 9.8 Correlaton test for Texas It looks like Texas tracks pretty much along the national average. Let’s do the correlation test for Texas just to compare. cor.test(tx$conversion_percent, tx$points_g) Which yields a correlation of 0.685967. Let’s see how much third-down conversions predict Texas’ scoring per game. (0.685967 * 0.685967) * 100 Which gets us 47.1%, not too far from the national average of 43.2%. 9.9 Practice 1: Compare Penn State Not every team tracks the national average like Texas. Tell me (and show me) how Penn State performs in this same third down conversion vs scoring metric by creating a similar graphic and correlation test from the Penn State data on top of the national data. 9.10 Practice 2: Penalties vs scoring How predictive are penalty yards per game on points per game? Do more disciplined teams score more points than undisciplined ones? How does Texas compare to the rest of the league? Create a new RNotebook to answer these questions. You will need to join penalty data (you’ve already downloaded the file data-raw/penalties.csv file) to the same points-per-games statistics from scoring offense. Make your own scatterplot with a fit line to show the relationships between penalty yards per game vs points per game. Run a correlation test for both the national average and for Texas. What does it say? Write a sentence that explains this to a reader. 9.11 Using bind_rows() to merge data sets We won’t go through an example or do practice sessions, but you should be aware that you can also merge data sets on top of each other when your columns are the same. Let’s say you have a multiple data sets where each year is broken into a different file or data frame. You can “stack” data frames on top of each other with a tidyverse function called bind_rows(). When row-binding, columns are matched by name, and any missing columns will be filled with NA. An example might look like this … Let’s say you have three years of data in three different data frames: fy2016, fy2017, fy2018. And all three data frames have the same column names: donor_type, date, amount. And each data frame has 1000 rows of data. If you want then all in the same file you would do this: combined &lt;- bind_rows(fy2016, fy2017, fy2018) The new data frame combined would have all the same columns, but would have 3000 rows of data. 9.11.1 Combine data with a group If you needed to know which data frame each row came from, you can name a “group” for each data frame, and then merge them. We will name our groups for each year they come from. combined &lt;- bind_rows( &quot;2016&quot; = fy2016, &quot;2017&quot; = fy2017, &quot;2018&quot; = fy2018, .id = &quot;year&quot; ) With this you would end up with a new data frame called combined, but it would have four columns: year, donor_type, date, amount. It would have all 3000 rows. All the rows that were pulled from fy2016 would have a year of “2016,” and so on. It’s a good way to note which file the data came from, and especially helpful if they are from different years, like the example. It is admittedly weird that you name the groups before you specify the data frame the come from, but specify .id before the name of your new column. ¯\\_(ツ)_/¯ "],["reordering-factors.html", "Chapter 10 Reordering factors 10.1 Goals for this section 10.2 Create our survey project 10.3 Figuring out our data shape 10.4 Charting the popularity of princess 10.5 Create our princess plot 10.6 Factors 10.7 Reorder princess 10.8 Factors recap 10.9 Practice: Make an ice cream chart 10.10 Resources", " Chapter 10 Reordering factors NEEDS REWRITE fix lists There is a complexity within R data frames that we need to cover becomes it comes into play when we want to order categorical data within graphics. We are going to handle this through a new project using our Survey data from class. 10.1 Goals for this section Create a new project with our class survey data. Create a chart that uses categorical data. Reorder the values in the chart using fct_reorder(). This is the chart we want to build: Popular princesses 10.2 Create our survey project 10.2.1 Setup In RStudio, choose File &gt; New Project Walk through the steps to create a New Project in a New Directory called yourname-survey. In your project, create a new directory called data-raw. Go to this link in a browser. Do File &gt; Save page as and save the file inside your data-out folder as survey-results.csv. Create a new RNotebook. Save the file and name it 01-survey.Rmd. For this simple example, we’ll only be using one notebook. 10.2.2 Libraries We need two libraries. I think the forcats library is already installed, but if not you’ll have to run the following in your RConsole: install.packages(\"forcats\"). library(tidyverse) library(forcats) 10.2.3 Import the data Import the data from the csv survey &lt;- read.csv(&quot;data-raw/survey-results.csv&quot;) %&gt;% clean_names() # peek at the data survey %&gt;% head() Peeking at the data, we see it is something like this: class graduating ice_cream princess computer Senior Yes Cookies &amp; Cream Mulan Macintosh Masters candidate Yes Rocky Road Mulan Windows Senior Yes Chocolate Ariel (Little Mermaid) Windows Senior No Mint chocolate chip Mulan Macintosh Junior No Mint chocolate chip Jasmine (Aladdin) Windows Senior No Mint chocolate chip Rapunzel (Tangled) Macintosh 10.3 Figuring out our data shape As we’ve talked about before, it is helpful to think of what we want the graphic to be, even to draw it out, so we can figure out what columns we need for the X and Y axis. The example I showed above is a little weird in that we are really building a bar/column chart, but we’ve turned sideways so we can read the labels. To build the chart, we are really looking at this: Princess before flip What do we need for the x value? Well, we need the total votes for each princess. What do we need for the y value? We need to list each princess that got votes. 10.4 Charting the popularity of princess So, we need a princess column and a votes column. These easiest way to do this is a simple count() summary. Build the count before you assign it back to princess_count so you see what is happening, but this is similar to what we’ve done in the past. princess_count &lt;- survey %&gt;% count(princess) %&gt;% rename(votes = n) %&gt;% arrange(desc(votes)) # peak princess %&gt;% head In order, we are: Assigning the result (which you do at the end), starting from survey. Count the rows of each princess. Rename the n column to votes. Arrange so the most votes are on top. We get this: princess votes Mulan 14 Rapunzel (Tangled) 7 Jasmine (Aladdin) 6 Ariel (Little Mermaid) 5 Tiana (Princess and the Frog) 2 Aurora (Sleeping Beauty) 1 Belle (Beauty and the Beast) 1 Merida (Brave) 1 Snow White 1 10.5 Create our princess plot We are going to use a new chart, geom_col, which is like geom_bar but it already understands the stat=\"identity\". princess_count %&gt;% ggplot(aes(x = princess, y = votes)) + geom_col() + coord_flip() + labs(title=&quot;Favorite Disney Princesses in class&quot;, x = &quot;Princess&quot;, y = &quot;Votes&quot;) + geom_text(aes(label = votes), hjust=-.25) And we get this: Princess wrong order Well, that is frustrating. The bars are not in the same order that we arranged them in the data frame. As explained in the R-Graph-Gallery post: This is due to the fact that ggplot2 takes into account the order of the factor levels, not the order you observe in your data frame. You can sort your input data frame with sort() or arrange(), it will never have any impact on your ggplot2 graphic. 10.5.1 Labels and titles Before I get into factors, let me explain the other lines in the graphic above: coord_flip() transposes the bars so they go horizontal instead of vertical. This allows us to read the princess values. labs() allows us to add the title and cleaner axis labels. geom_text() adds the numbers at the end of the bar, with some adjustments to get them off the top of the bars. 10.6 Factors Factors allow you to apply an order (called “levels”) to values beyond being alphabetical. It is super useful when you are dealing with things like the names of months which have a certain order (“Jan,” “Feb,” “March”) which would not be ordered correctly alphabetically. But is is kind of frustrating here. We can improve it by reordering the levels of princess using a function from the forcats package. (forcats is an anagram for “factors”). There are four functions you can use reorder a factor: fct_reorder(): Reordering a factor by another variable. fct_infreq(): Reordering a factor by the frequency of values. fct_relevel(): Changing the order of a factor by hand. fct_lump(): Collapsing the least/most frequent values of a factor into “other.” We will use fct_reorder() to reorder the princess values based on the votes values. fct_reorder() takes two main arguments: a) the factor (or column) whose levels you want to modify, and b) a numeric vector (or column of values) that you want to use to reorder the levels. fct_reorder(what_you_are_reordering, the_col_to_base_it_on) 10.7 Reorder princess While we could do this in the ggplot code, I find it’s easiest to do in the data frame as we shape our data. So, go back up to where we made the princes_count data frame and add a new pipe like this: princess_count &lt;- survey %&gt;% count(princess) %&gt;% rename(votes = n) %&gt;% arrange(desc(votes)) %&gt;% mutate(princess = fct_reorder(princess, votes)) # this line reorders the factors The data frame won’t look any different but if you re-run the ggplot code chunk, you graphic will be reordered. Popular princess 10.8 Factors recap Factors in R allow us to apply “levels” to sort categorical data into a logical order beyond alphabetical. If you are building a graphic that uses a categorical column as one of your aesthetics, then you might need to reorder the factor using fct_reorder() or one of the other functions. It’s easiest to do that using dplyr’s mutate() function on your data frame before you plot. 10.9 Practice: Make an ice cream chart Make the same chart as above, but using the ice_cream counts. Order the column chart by the most popular ice cream. 10.9.1 Turn in this project Save, knit and zip the project and turn it into the Canvas assignment for “Factors.” 10.10 Resources These resources can help you understand the concepts. This post on Reordering a variable in ggplot helped me understand how to reorder factors for graphics. Hadley Wickam’s R for Data Science has a Chapter on factors. For those who really want to learn more about them later. "],["plotly.html", "Chapter 11 Plotly", " Chapter 11 Plotly Introduce here. "],["review---data-wrangling.html", "Chapter 12 Review - Data wrangling 12.1 Start a new project 12.2 Downloading our data 12.3 Importing from an Excel spreadsheet 12.4 A quick look at our data", " Chapter 12 Review - Data wrangling NEEDS REWRITE Fix lists For practice, we are going to import some marijuana arrests data from the Austin Police Department. The data was obtained through an open records request as an Excel spreadsheet, which will be new for us. I’ve removed the names of the suspects. The story here is we want to see how many “B Misdemeanor” arrests there have been over time in Austin. This are arrest for small amounts of marijuana, less than 2 ounces. Our goal here is to practice setting up a project and importing data. Because this is a real-world example, it will not go quite as planned. We’ll continue to learn new things as we progress. If a step in the process is something we’ve done before, I’ll give brief directions. If something is new, I’ll provide more detail and explanation. 12.1 Start a new project A new project means we start from scratch. Close out any previous project work you have open. File &gt; Close Project is a good way. It is possible to have two RStudio projects open at the same time, but you’ll need open the second one as a “new session.” It is also possible to open notebooks from different projects in the same session, but I would not recommend that. Use the +R icon or File &gt; New Project. Go through the steps to create your project folder in a place where you can find it later. For the purposes of this class, name it yourname-arrests. Use the Files panel to create two new folders called data-raw and data-processed. (I name my folders like this so all the data folders are together when I view them.) Create a new RNotebook called 01-import.Rmd, update the metadata at the top and remove the boilerplate code. Create a chunk called “setup” and add the following libraries. library(tidyverse) library(janitor) library(readxl) library(lubridate) When you run the setup chunk, you’ll probably get some errors that some packages are not installed. Look at the list and use your Console to install them: install.packages(\"readxl\") for example. Readxl allows us to import Excel data into R. The other new package, lubridate, helps us with dates and times in data. 12.2 Downloading our data Like with our special education data, we’re going to download our data using the download.file() function. Add some text that explains you are downloading the data. Create an R chunk called “download” and add the following code: # arrest data download download.file(&quot;https://github.com/utdata/rwdir/blob/main/data-raw/APD_marijuana_arrests_2016-2020_data.xlsx?raw=true&quot;, &quot;data-raw/APD_marijuana_arrests_2016-2020_data.xlsx&quot;) You should get a response like this: trying URL &#39;https://github.com/utdata/rwdir/blob/main/data-raw/APD_marijuana_arrests_2016-2020_data.xlsx?raw=true&#39; Content type &#39;application/octet-stream&#39; length 567494 bytes (554 KB) ================================================== downloaded 554 KB This downloads the spreadsheet into your data-raw folder. Once you’ve done this, you can add a # to the front of the download.file line to comment it out since you won’t need to download it again. If you open up that spreadsheet in Excel, you’ll notice there are two sheets. The first sheet has summary notes included by the public information officer: PIR notes And the second is the actual data, on a sheet called “Results.” Again, note I’ve removed the “Name of arrestee” as we are just interested in counts for this exercise. Results 12.3 Importing from an Excel spreadsheet Add a new Markdown header: ## Import. Add a note that you are adding data from the specific sheet called “Results.” Start a new chunk and name it “import.” Add the following code: data &lt;- read_excel( &quot;data-raw/APD_marijuana_arrests_2016-2020_data.xlsx&quot;, sheet = &quot;Results&quot; ) %&gt;% clean_names() data %&gt;% glimpse() ## Rows: 7,828 ## Columns: 9 ## $ arrest_primary_key &lt;dbl&gt; 201600012, 201600018, 201600061, 201600074, 201600… ## $ arrest_date &lt;dttm&gt; 2016-01-01, 2016-01-01, 2016-01-01, 2016-01-01, 2… ## $ arrest_charge &lt;chr&gt; &quot;POSS MARIJUANA &lt;2OZ&quot;, &quot;POSS MARIJ &lt; 2OZ&quot;, &quot;POSS M… ## $ apd_race &lt;chr&gt; &quot;BLACK&quot;, &quot;BLACK&quot;, &quot;HISPANIC OR LATINO&quot;, &quot;HISPANIC … ## $ sex &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, … ## $ arrest_location &lt;chr&gt; &quot;2336 DOUGLAS ST&quot;, &quot;212 E 6TH ST&quot;, &quot;100 TILLERY ST… ## $ zip &lt;dbl&gt; 78741, 78701, 78702, 78758, 78758, 78752, 78758, 7… ## $ arrest_x_coordinate &lt;dbl&gt; 3117763, 3114977, 3125173, 3124635, 3128359, 31257… ## $ arrest_y_coordinate &lt;dbl&gt; 10056344, 10070652, 10065077, 10124175, 10107000, … I suppressed the warnings in my output, but you will some warnings: Expecting numeric in A4572 / R4572C1: got ‘201717-10450’Expecting numeric in A5460 / R5460C1: got ’201818-01877’Expecting numeric in A5748 / R5748C1: got ’201818-03343’Expecting numeric in A5784 / R5784C1: got ’201818-03522’Expecting numeric in A5889 / R5889C1: got ’201818-04226’Expecting numeric in A6023 / R6023C1: got ’201818-05278’Expecting numeric in A6116 / R6116C1: got ’201818-06048’Expecting numeric in A6156 / R6156C1: got ’201818-06369’Expecting numeric in A6221 / R6221C1: got ’201818-07021’Expecting numeric in A6344 / R6344C1: got ’201818-08182’Expecting numeric in A6761 / R6761C1: got ’201818-11409’Expecting numeric in A6814 / R6814C1: got ’201818-11848’ We can’t ignore this, and it is one of the challenges of importing data into R or any other strict database. I wasn’t sure what was going on here so I opened the spreadsheet and searched for ‘201818-03522’ as noted in the error. Data issues And if I look at the glimpse() returns I see that the arrest_primary_key is considered a &lt;dbl&gt; data type, which is a number. So what does that row look like in my data?. Let’s look at all the rows from Sept. 17, 2017. Add a new chunk called “date-test” and add the following code: The ymd() part of the filter below is using lubridate to note the format of my date so R can understand it. It stands for year-month-day. We’ll get more into dates later. data %&gt;% filter(arrest_date == ymd(&quot;2017-09-28&quot;)) %&gt;% head() ## # A tibble: 6 × 9 ## arrest_primary_key arrest_date arrest_charge apd_race sex ## &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… WHITE M ## 2 201735768 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… WHITE F ## 3 201735775 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… BLACK M ## 4 201735797 2017-09-28 00:00:00 POSS MARIJ &gt;2OZ HISPANIC OR L… M ## 5 201735808 2017-09-28 00:00:00 POSS MARIJ HISPANIC OR L… M ## 6 201735836 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… BLACK M ## # … with 4 more variables: arrest_location &lt;chr&gt;, zip &lt;dbl&gt;, ## # arrest_x_coordinate &lt;dbl&gt;, arrest_y_coordinate &lt;dbl&gt; Note that first record doesn’t have an primary key! This is a problem. R is considering that column to be a set of numbers - - and ‘201717-10450’ is not a real number because it has a dash in it, so it was imported as NA. This is not good. We want all the arrest_primary_keys to be text, not numbers. While fixing that we’ll also make sure the ZIP code is text. Within the read_excel() function we can either set all the columns to the same data type, like “text,” or we have to set each individual column, which is what we have to do. The description of all the readxl data types are here, but note we are using “guess” for those we aren’t being specific about. We list the data types in the order of the columns in the spreadsheet. Edit your code chunk to add the col_types part below. I added notes to myself to keep track of the columns. data &lt;- read_excel( &quot;data-raw/APD_marijuana_arrests_2016-2020_data.xlsx&quot;, sheet = &quot;Results&quot;, col_types = c( &quot;text&quot;, # key &quot;guess&quot;, # date &quot;guess&quot;, # charge &quot;guess&quot;, # race &quot;guess&quot;, # sex &quot;guess&quot;, # location &quot;text&quot;, # zip &quot;guess&quot;, # x coordinate &quot;guess&quot; # y coordinate ) ) %&gt;% clean_names() data %&gt;% glimpse() ## Rows: 7,828 ## Columns: 9 ## $ arrest_primary_key &lt;chr&gt; &quot;201600012&quot;, &quot;201600018&quot;, &quot;201600061&quot;, &quot;201600074&quot;… ## $ arrest_date &lt;dttm&gt; 2016-01-01, 2016-01-01, 2016-01-01, 2016-01-01, 2… ## $ arrest_charge &lt;chr&gt; &quot;POSS MARIJUANA &lt;2OZ&quot;, &quot;POSS MARIJ &lt; 2OZ&quot;, &quot;POSS M… ## $ apd_race &lt;chr&gt; &quot;BLACK&quot;, &quot;BLACK&quot;, &quot;HISPANIC OR LATINO&quot;, &quot;HISPANIC … ## $ sex &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, … ## $ arrest_location &lt;chr&gt; &quot;2336 DOUGLAS ST&quot;, &quot;212 E 6TH ST&quot;, &quot;100 TILLERY ST… ## $ zip &lt;chr&gt; &quot;78741&quot;, &quot;78701&quot;, &quot;78702&quot;, &quot;78758&quot;, &quot;78758&quot;, &quot;7875… ## $ arrest_x_coordinate &lt;dbl&gt; 3117763, 3114977, 3125173, 3124635, 3128359, 31257… ## $ arrest_y_coordinate &lt;dbl&gt; 10056344, 10070652, 10065077, 10124175, 10107000, … No more warnings about bad data! As we look through the glimpse() output we see arrest_primary_key and zip are now “” and just as important our arrest_date is still a date and coordinates are still numbers. Now run your date test just to make sure we no longer have the NA: data %&gt;% filter(arrest_date == ymd(&quot;2017-09-28&quot;)) %&gt;% head() ## # A tibble: 6 × 9 ## arrest_primary_key arrest_date arrest_charge apd_race sex ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201717-10450 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… WHITE M ## 2 201735768 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… WHITE F ## 3 201735775 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… BLACK M ## 4 201735797 2017-09-28 00:00:00 POSS MARIJ &gt;2OZ HISPANIC OR L… M ## 5 201735808 2017-09-28 00:00:00 POSS MARIJ HISPANIC OR L… M ## 6 201735836 2017-09-28 00:00:00 POSS MARIJ &lt;2OZ (… BLACK M ## # … with 4 more variables: arrest_location &lt;chr&gt;, zip &lt;chr&gt;, ## # arrest_x_coordinate &lt;dbl&gt;, arrest_y_coordinate &lt;dbl&gt; 12.4 A quick look at our data Now that we have our data imported with the correct data types, we can start exploring it. We want to learn about the values in each column, look for problems and generally become familiar with our data. We’ve used glimpse() already and we’ll be looking at that for column names and such. We can run summary() to get some basic stats on numeric fields, then run some count()s to look at categorical values. Run summary() on your data. Don’t forget to name the chunk. data %&gt;% summary() ## arrest_primary_key arrest_date arrest_charge ## Length:7828 Min. :2016-01-01 00:00:00 Length:7828 ## Class :character 1st Qu.:2016-10-10 00:00:00 Class :character ## Mode :character Median :2017-06-20 00:00:00 Mode :character ## Mean :2017-08-08 23:39:45 ## 3rd Qu.:2018-04-27 00:00:00 ## Max. :2020-03-15 00:00:00 ## ## apd_race sex arrest_location zip ## Length:7828 Length:7828 Length:7828 Length:7828 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## arrest_x_coordinate arrest_y_coordinate ## Min. : 0 Min. : 0 ## 1st Qu.:3111496 1st Qu.:10057200 ## Median :3120470 Median :10073148 ## Mean :2972667 Mean : 9602880 ## 3rd Qu.:3128359 3rd Qu.:10099438 ## Max. :3172781 Max. :10156802 ## NA&#39;s :22 NA&#39;s :22 The summary() doesn’t do much for us with this data set. The arrest_date column is good because we can see the oldest and newest dates. The coordinates are the only true numbers in our data but stats on them don’t help us given it is really geographical data. In a later lesson we’ll use those to map these records. Use count() to look at the categorical fields of arrest_charge, apd_race, and sex and zip. You might also find arrange() useful. We won’t deal with the location fields until later. data %&gt;% count(arrest_charge) %&gt;% arrange(n %&gt;% desc()) ## # A tibble: 155 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POSS MARIJ &lt; 2OZ 2214 ## 2 POSS MARIJ &lt;2OZ 1963 ## 3 POSS MARIJUANA &lt;2OZ 1355 ## 4 POSS MARIJ &lt;2OZ (MB) 848 ## 5 POSSESSION OF MARIJUANA &lt;2OZ 346 ## 6 POSS MARIJ 199 ## 7 POSS MARIJUANA 140 ## 8 POSS MARIJ &gt;2OZ&lt;=4OZ 85 ## 9 POSS MARIJ &gt;4OZ&lt;=5LBS 82 ## 10 POSS MARIJ &lt;= 5LBS &gt; 4OZ 57 ## # … with 145 more rows data %&gt;% count(apd_race) ## # A tibble: 8 × 2 ## apd_race n ## &lt;chr&gt; &lt;int&gt; ## 1 AMERICAN INDIAN/ALASKAN NATIVE 3 ## 2 ASIAN 48 ## 3 BLACK 2733 ## 4 HAWAIIAN/PACIFIC ISLANDER 1 ## 5 HISPANIC OR LATINO 3202 ## 6 MIDDLE EASTERN 21 ## 7 UNKNOWN 7 ## 8 WHITE 1813 data %&gt;% count(sex) ## # A tibble: 2 × 2 ## sex n ## &lt;chr&gt; &lt;int&gt; ## 1 F 1272 ## 2 M 6556 data %&gt;% count(zip) %&gt;% arrange(n %&gt;% desc()) ## # A tibble: 52 × 2 ## zip n ## &lt;chr&gt; &lt;int&gt; ## 1 78753 794 ## 2 78758 684 ## 3 78741 680 ## 4 78701 671 ## 5 78702 479 ## 6 78723 471 ## 7 78744 457 ## 8 78745 437 ## 9 &lt;NA&gt; 391 ## 10 78704 359 ## # … with 42 more rows The arrest_charge is a mess and also the field we are most interested in, so our next lesson will involve cleaning up that field. The other fields are in good shape. We’ll continue with this same project in the next chapter. "],["cleaning.html", "Chapter 13 Cleaning 13.1 NOT FINISHED, NEEDS REWRITE 13.2 Goals for this section 13.3 Explain thyself 13.4 Syntax etiquette 13.5 Recode the apd_race column 13.6 Filter out data we don’t need 13.7 Finding just B misdemeanors 13.8 Export your updated data frame", " Chapter 13 Cleaning 13.1 NOT FINISHED, NEEDS REWRITE include section on clean code and advanced tips. naming chunks. good indents. good markdown. tips for the navigation dropdown. keyboard command review. Restart and run. recode case_when Fix lists In this chapter we are continuing with our “arrests” project in the 01-import.Rmd notebook. We’ll use some cleaning techniques to prepare our data for analysis later in a different notebook. 13.2 Goals for this section Recap good coding practices and build on them. Create a race_simplified column using recode(). Create a cleaned arrest_charge column. Export data for next notebook. 13.2.1 Resources Update these resources or kill it since I link within the text. Strings chapter from Hadley Wickham’s book, specifically about str_replace(). RDocumentation on str_replace(). stringr cheatsheet. 13.3 Explain thyself Throughout this notebook, we want to explain our thoughts and goals in Markdown. You want to write these concepts out for collaborators or others who might read your work, but your future self is your first, best audience. Organize sections of your notebook with headlines. Write about the goal of each section. Each code block in the section should have a human readable explanation of the goal or task. Name your chunks. Use # comments in your code if you need more clarity. Go back and update your notebook with these things if you don’t have them already. 13.4 Syntax etiquette You might have noticed when I write code in R, I add returns and indent code to keep it readable. I usually pipe %&gt;% to a new line unless I have just one pipe. If I have a list of items within a function, I might put each item on a new line so they are easier to read. One thing you want to do is make sure no one line is too long. A good place to start adding returns to shorten a line (or nesting it) is within a function. The nested code should be indented one tab stop. RStudio helps you set tabs when you hit return, but you might need to clean up your code as you go along. This code chunk we’ve already written includes a lot of these concepts, including some comments to help me understand my code later. Comments like this can be on a line of their own or at the end of a line. data &lt;- read_excel( # breaking up read_excel to make shorting line &quot;data-raw/APD_marijuana_arrests_2016-2020_data.xlsx&quot;, sheet = &quot;Results&quot;, col_types = c( # putting values on own lines for readability &quot;text&quot;, # key &quot;guess&quot;, # date &quot;guess&quot;, # charge &quot;guess&quot;, # race &quot;guess&quot;, # sex &quot;guess&quot;, # location &quot;text&quot;, # zip &quot;guess&quot;, # x coordinate &quot;guess&quot; # y coordinate ) ) %&gt;% clean_names() Go back through your code chunks and make sure the syntax is clean. 13.5 Recode the apd_race column We’re going to create a simplified race column by “recoding” the values from the apd_race column. We’ll create a smaller set of races, combing some into “other.” First, let’s review the apd_race values: data %&gt;% count(apd_race) ## # A tibble: 8 × 2 ## apd_race n ## &lt;chr&gt; &lt;int&gt; ## 1 AMERICAN INDIAN/ALASKAN NATIVE 3 ## 2 ASIAN 48 ## 3 BLACK 2733 ## 4 HAWAIIAN/PACIFIC ISLANDER 1 ## 5 HISPANIC OR LATINO 3202 ## 6 MIDDLE EASTERN 21 ## 7 UNKNOWN 7 ## 8 WHITE 1813 Let’s rename everything that isn’t Asian, Black, Hispanic or White as Other. We need to make a note (in Markdown!) that we are including the Unknowns in Other and include a note with any graphic or analysis we do later. We are creating a new column (and avoid writing over an existing column) so we can keep the old one if we perhaps want to look at the more specific race/ethnicities. We’ll also port the results into a new data frame data_race in case we need to get to the original data for any reason. We are using two functions here to accomplish this: We use mutate() to create the column and we name the new columns race_simple. The values for that are filled with the recode() function that lists the existing value first, then the new values (which is kinda opposite from mutate, where we name the new item first). mutate() works like this: df %&gt;% mutate( new_col_name = data_to_draw_from ) And recode() works like this (but we will use this were data_to_draw_from is in our mutate). recode( &quot;Old Value&quot; = &quot;New Value&quot; ) This is like a “search and replace” for a value within a column. As we recode our new column we’ll leave out “ASIAN,” “BLACK,” and “WHITE” because we aren’t changing them. I’m recoding “HISPANIC OR LATINO” to just “HISPANIC” for simplicity and graph labels used later. Read over the code below to understand what is going on, then try to write it yourself without looking back. Run the code and if there is an error, read that first and try to figure it out. The completed code is below, but know I wrote it in stages. Sometimes I’ll write part of the code first and make sure it works. I might write the mutate part and just fill race_simple from apd_race to make sure it works, then edit that to add the recode() step. I might not assign the data to a new data frame until I work out all the functions. It’s a process. Below I just have the finished product. I used a count using both columns to check my work. data_race &lt;- data %&gt;% mutate( race_simple = recode(apd_race, &quot;AMERICAN INDIAN/ALASKAN NATIVE&quot; = &quot;OTHER&quot;, &quot;HAWAIIAN/PACIFIC ISLANDER&quot; = &quot;OTHER&quot;, &quot;HISPANIC OR LATINO&quot; = &quot;HISPANIC&quot;, &quot;MIDDLE EASTERN&quot; = &quot;OTHER&quot;, &quot;UNKNOWN&quot; = &quot;OTHER&quot;, ) ) # peek at the results data_race %&gt;% count( apd_race, race_simple ) ## # A tibble: 8 × 3 ## apd_race race_simple n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 AMERICAN INDIAN/ALASKAN NATIVE OTHER 3 ## 2 ASIAN ASIAN 48 ## 3 BLACK BLACK 2733 ## 4 HAWAIIAN/PACIFIC ISLANDER OTHER 1 ## 5 HISPANIC OR LATINO HISPANIC 3202 ## 6 MIDDLE EASTERN OTHER 21 ## 7 UNKNOWN OTHER 7 ## 8 WHITE WHITE 1813 And now I can look just at the new column to get an idea of the simplified race breakdown. Add this chunk to your notebook. data_race %&gt;% count(race_simple) ## # A tibble: 5 × 2 ## race_simple n ## &lt;chr&gt; &lt;int&gt; ## 1 ASIAN 48 ## 2 BLACK 2733 ## 3 HISPANIC 3202 ## 4 OTHER 32 ## 5 WHITE 1813 The race_simple column was added to the end of the data frame. If you look at your environment, data_race has 10 columns vs 9 for data. 13.6 Filter out data we don’t need Cleaning up the arrest_charge column is more complicated and we need to talk about our goals first. Let’s look again at the different arrest_charge values. Add this count to your notebook data_race %&gt;% count(arrest_charge) %&gt;% arrange(n %&gt;% desc()) ## # A tibble: 155 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POSS MARIJ &lt; 2OZ 2214 ## 2 POSS MARIJ &lt;2OZ 1963 ## 3 POSS MARIJUANA &lt;2OZ 1355 ## 4 POSS MARIJ &lt;2OZ (MB) 848 ## 5 POSSESSION OF MARIJUANA &lt;2OZ 346 ## 6 POSS MARIJ 199 ## 7 POSS MARIJUANA 140 ## 8 POSS MARIJ &gt;2OZ&lt;=4OZ 85 ## 9 POSS MARIJ &gt;4OZ&lt;=5LBS 82 ## 10 POSS MARIJ &lt;= 5LBS &gt; 4OZ 57 ## # … with 145 more rows In your notebook you’ll be able to page through the results and you can see they are a mess. (This published page doesn’t paginate the results.) 13.6.1 About marijuana charges Understanding what these charges are is important to the story. We only care about Class B Possession charges, which are under 2 ounces. (Class A is between 2 and 4 ounces.) We also need to be aware of Drug Free Zone (or DFZ) charges as those are “enhanced” to a higher charge, so we want to exclude those. We also want to exclude Delivery charges and anything not marijuana. Lastly, we also want to remove charges that include “request to apprehend” as in that case the police were looking for the suspect on a different charge. So, in short, we only want charges under 2OZ without Drug Free Zone. This will be a process! We’ll build filters and sometimes use count() to check the results before creating new data frames with the filtered data. 13.6.2 Filter out delivery charges Before we can remove data, we need to make sure we can find it properly. As you page through the charges results above, you’ll see some rows have “DEL” in the charge description. We will use filter() (which we’ve used in the Transform chapter) and str_detect() to find these records, review them, and then “negate” the filter so they are NOT included. Create a new chunk called “filter-del” and include the following code. I explain it afterward. data_race %&gt;% filter( str_detect(arrest_charge, &quot;DEL&quot;) ) ## # A tibble: 27 × 10 ## arrest_primary_k… arrest_date arrest_charge apd_race sex ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201600659 2016-01-05 00:00:00 DEL MARIJUANA &lt;= 5LB… WHITE M ## 2 201603658 2016-01-27 00:00:00 DEL MARIJ &lt;= 5LBS &gt; … BLACK M ## 3 201604087 2016-01-30 00:00:00 DEL MARIJ &lt;= 5LBS &gt; … HISPANIC O… M ## 4 201606440 2016-02-16 00:00:00 DEL MARIJUANA &lt;=5LBS… HISPANIC O… M ## 5 201610015 2016-03-14 00:00:00 POSS MARIJ &lt; 2OZ W/I… HISPANIC O… M ## 6 201611111 2016-03-22 00:00:00 POSS MARIJ &lt;= 5LBS &gt;… HISPANIC O… M ## 7 201641367 2016-11-14 00:00:00 DEL MARIJ MARIJUANA-… HISPANIC O… M ## 8 201641369 2016-11-14 00:00:00 DEL MARIJUANA- 2LBS HISPANIC O… F ## 9 201704935 2017-02-07 00:00:00 MANF/DEL-POSS MARIJ … BLACK M ## 10 201800337 2018-01-03 00:00:00 DEL MARIJUANA &gt;1/4 O… BLACK M ## # … with 17 more rows, and 5 more variables: arrest_location &lt;chr&gt;, zip &lt;chr&gt;, ## # arrest_x_coordinate &lt;dbl&gt;, arrest_y_coordinate &lt;dbl&gt;, race_simple &lt;chr&gt; The filter() selects data based on a set of rules we give it. In the past, we fed the function a column name and a value to look for. This time we are feeding is a function str_detect() which takes two values: The column to look in, and then the text we are looking for. This method will find the text no matter where it appears within the field. There are 27 results. As you page through these results you’ll see all the rows that have DEL in it, including some that have POS like “MANF/DEL-POSS MARIJ &lt;= 4OZ &gt; 2OZ.” That’s OK If it is a delivery, we don’t want it. Now we add the “negate” option of ! before the str_detect so we get all the rows except those with “DEL.” Update your “filter-del” chunk to add ! right before the str_detect and run it. data_race %&gt;% filter( !str_detect(arrest_charge, &quot;DEL&quot;) ) ## # A tibble: 7,801 × 10 ## arrest_primary_key arrest_date arrest_charge apd_race sex ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201600012 2016-01-01 00:00:00 POSS MARIJUANA &lt;… BLACK M ## 2 201600018 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ BLACK M ## 3 201600061 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ HISPANIC OR L… M ## 4 201600074 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ HISPANIC OR L… M ## 5 201600131 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ BLACK M ## 6 201600145 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ HISPANIC OR L… M ## 7 201600153 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ WHITE F ## 8 201600154 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ HISPANIC OR L… M ## 9 201600172 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ HISPANIC OR L… M ## 10 201600176 2016-01-01 00:00:00 POSS MARIJ &lt; 2OZ BLACK M ## # … with 7,791 more rows, and 5 more variables: arrest_location &lt;chr&gt;, ## # zip &lt;chr&gt;, arrest_x_coordinate &lt;dbl&gt;, arrest_y_coordinate &lt;dbl&gt;, ## # race_simple &lt;chr&gt; Now we’ll assign that filter to a new data frame. Some folks might consider this overkill and just reassign the cleaned data to the existing data_race tibble. This can get confusing if you make changes up and down your notebook, so I like to create new tibbles with my changed data. Update your “filter-del” chunk to assign the results to a new tibble, data_nodel. data_nodel &lt;- data_race %&gt;% filter( !str_detect(arrest_charge, &quot;DEL&quot;) ) If you look at your environment you’ll see the data_nodel data has 27 fewer observations than the data tibble, the same number or records you got in your original filter. This is good! 13.6.3 Remove controlled substance If you look at the charges again, you’ll see some other charges that are NOT related to marijuana. These are “controlled substance” charges. We don’t want them. data_nodel %&gt;% count(arrest_charge) ## # A tibble: 139 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POM 1 ## 2 POSS CS PG 1 &lt;1G 1 ## 3 POSS CS PG 3&lt; 28G 3 ## 4 POSS CS PG 3&lt; 28G - TYLENOL W/ CODEINE 1 ## 5 POSS CS PG 3&lt; 28G -PILLS 1 ## 6 POSS CS PG 3&lt; 28G -XANEX -ALPRAZOLAM 1 ## 7 POSS CS PG 3&lt; 28G (PILLS) 1 ## 8 POSS CS PG 3&lt; 28G / XANAX (ALPRAZOLAM) 1 ## 9 POSS CS PG 3&lt; 28G- KLONOPIN 1 ## 10 POSS CS PG 3&lt; 28G- XANAX 1 ## # … with 129 more rows We want to remove all the cases with “CS” in the arrest_charge field. Use the same technique that we used for DEL to remove the CS records and assign the results to a new dataframe data_nocs. Try it on your own before looking at the answer below. Try it first! Click to reveal answer. data_nocs &lt;- data_nodel %&gt;% filter( !str_detect(arrest_charge, &quot;CS&quot;) ) 13.6.4 Remove drug free zones Our next move is to remove all the records that have a drug free zone enhancement. Let’s look at the charges data once again using count(). Create a new chunk called “dfz-count” and use our data_nocs tibble to count arrest_charge values. data_nocs %&gt;% count(arrest_charge) ## # A tibble: 130 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POM 1 ## 2 POSS MARIJ 199 ## 3 POSS MARIJ - LT 2 OZ 1 ## 4 POSS MARIJ -1 LB 1 ## 5 POSS MARIJ 4OZ - 5LBS 1 ## 6 POSS MARIJ 4OZ&lt;5LBS 1 ## 7 POSS MARIJ - DRUG FREE ZONE 1 ## 8 POSS MARIJ (FS) 1 ## 9 POSS MARIJ (MB) 1 ## 10 POSS MARIJ &lt; 2OZ 2214 ## # … with 120 more rows As you look through the results, you’ll see the drug free zone designation is handled many different ways: DFZ, DRUG FREE ZONE, SCHOOL ZONE, etc. This might take some trial and error. We will use str_detect() again, but we need to search for multiple terms and we want to make sure the filter works as expected. So we’ll create a variable dfz_terms which will include a list of terms we want to search, using some regular expressions. We also use our count() function to check our results. Regular expressions are a powerful pattern matching concept that can get a little complicated. We’ll take them slow and only use what we need to get our results. Start a new chunk called “dfz-filter” and set it up like the code below and run it. I’ll explain what is going on below. dfz_terms &lt;- c(&quot;DFZ|SCHOOL|DRUG&quot;) data_nocs %&gt;% filter(str_detect(arrest_charge, dfz_terms)) %&gt;% count(arrest_charge) data_nocs %&gt;% filter(!str_detect(arrest_charge, dfz_terms)) %&gt;% count(arrest_charge) This code gives you two sets of results. I’m giving you screen shots because this published page doesn’t behave the same way. The first has the results that include the terms we are looking for: DFZ Then second set shows the terms that DON’T have our terms because we negated them: No DFZ Now let me explain the code. The first line creates a variable of terms to use in our string detect: “DFZ|SCHOOL|DRUG.” The pipe | is a “regular expression” that means “or.” So when we use this list in our str_detect() function we are looking for DFZ or SCHOOL or DRUG. I set this up like this so we can use the same list of terms to look at the filter that contains the terms and look at a filter that negates the terms. It’s the same concept we used for deliveries and controlled substances, but we create the search term in a variable so we can double-check to make sure we are getting the results we want. Now that we know our terms are good and we are getting the results we want, we can build a new data table using a negated str_detect using those same terms. Create a new chunk that assigns the result of a filter negating the dfz_terms. Call the new tibble data_nodfz). Use the hint below only if you can’t figure it out. Remember, you don’t need the count() anymore, just the filter. You should end up with 7672 observations. Click here for the answer data_nodfz &lt;- data_nocs %&gt;% filter(!str_detect(arrest_charge, dfz_terms)) 13.6.5 Remove request to apprehend Use the same method as above to remove any records that have “REQUEST” or “RTA.” Remember to start from your new data_nodfz tibble and name your new one data_norta. You should end up with 7665 observations. Try it first! rta_terms &lt;- c(&quot;REQUEST|RTA&quot;) # dfz results data_nodfz %&gt;% filter( str_detect(arrest_charge, rta_terms) ) # no dfz results data_nodfz %&gt;% filter( !str_detect(arrest_charge, rta_terms) ) %&gt;% count(arrest_charge) # make new data frame data_norta &lt;- data_nodfz %&gt;% filter( !str_detect(arrest_charge, rta_terms) ) 13.6.6 Removing more unwated records This charge code really is a mess. There are some other things we can remove before getting to our main challenge of finding only charges of &lt;2OZ. There are some codes used in the charges that we know we don’t want, like (MA) for an A Misdemeanor, and (FS) for a state felony, and (F3) for 3rd degree felony. This is tricky because we can’t remove all “MA” text because that would catch “MARIJUANA,” too. We have to include the parenthesis (MA), and that can be tricky in regular expressions (aka regex). The parenthesis is a special character in regex so we have to “escape” it to use it as a character. Even more confusing, when using R we have to double-escape it. This is much too confusing to get into in detail here, so this filter is a “gimmie.” We’ll also remove any row that has a charge with “LB” because that definitely is more than 2 OZ. Add this chunk to your project and run it. # list escapes ( to get (MA), etc fs_list &lt;- c(&quot;\\\\(MA\\\\)|\\\\(FS\\\\)|\\\\(F3\\\\)|LB&quot;) # checks the catch list data_norta %&gt;% filter( str_detect(arrest_charge, fs_list) ) %&gt;% count(arrest_charge) ## # A tibble: 28 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POSS MARIJ -1 LB 1 ## 2 POSS MARIJ 4OZ - 5LBS 1 ## 3 POSS MARIJ 4OZ&lt;5LBS 1 ## 4 POSS MARIJ (FS) 1 ## 5 POSS MARIJ &lt;= 50 LBS &gt;5LBS 8 ## 6 POSS MARIJ &lt;= 5LBS &gt; 4 OZ 1 ## 7 POSS MARIJ &lt;= 5LBS &gt; 4OZ 57 ## 8 POSS MARIJ &lt;= 5LBS &gt; 4OZ 4 LBS 1 ## 9 POSS MARIJ &lt;=5LBS&gt;50LBS 2 ## 10 POSS MARIJ &lt;=5LBS&gt;50LBS(F3) 6 ## # … with 18 more rows # checks the negate data_norta %&gt;% filter( !str_detect(arrest_charge, fs_list) ) %&gt;% count(arrest_charge) ## # A tibble: 58 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POM 1 ## 2 POSS MARIJ 199 ## 3 POSS MARIJ - LT 2 OZ 1 ## 4 POSS MARIJ (MB) 1 ## 5 POSS MARIJ &lt; 2OZ 2214 ## 6 POSS MARIJ &lt; 2OZ -&lt; 4OZ 1 ## 7 POSS MARIJ &lt; 2OZ -&lt;4OZ 1 ## 8 POSS MARIJ &lt; 2OZ - POM 1 ## 9 POSS MARIJ &lt; 2OZ - POSS MARIJUANA 1 ## 10 POSS MARIJ &lt; 2OZ -BY WEIGHT 1 ## # … with 48 more rows # does the deed data_nofs &lt;- data_norta %&gt;% filter( !str_detect(arrest_charge, fs_list) ) You now have data_nofs tibble, which we’ll continue with. 13.7 Finding just B misdemeanors THIS IS WHERE I STOPPED LAST. We’ve removed a lot of crap, but now we have the hard part. How do we get just B misdemeanors while dealing with some obvious data coding errors? attempts to get to just b mis mb_list &lt;- c(&quot;&lt; *2&quot;) data_nofs %&gt;% filter( str_detect(arrest_charge, mb_list) ) %&gt;% count(arrest_charge) ## # A tibble: 30 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POSS MARIJ &lt; 2OZ 2214 ## 2 POSS MARIJ &lt; 2OZ -&lt; 4OZ 1 ## 3 POSS MARIJ &lt; 2OZ -&lt;4OZ 1 ## 4 POSS MARIJ &lt; 2OZ - POM 1 ## 5 POSS MARIJ &lt; 2OZ - POSS MARIJUANA 1 ## 6 POSS MARIJ &lt; 2OZ -BY WEIGHT 1 ## 7 POSS MARIJ &lt; 2OZ UNDER 4 OUNCES 1 ## 8 POSS MARIJ &lt; 2OZ&gt;4 1 ## 9 POSS MARIJ &lt;2OZ 1963 ## 10 POSS MARIJ &lt;2OZ .3OZ 1 ## # … with 20 more rows data_nofs %&gt;% filter( !str_detect(arrest_charge, mb_list) ) %&gt;% count(arrest_charge) ## # A tibble: 28 × 2 ## arrest_charge n ## &lt;chr&gt; &lt;int&gt; ## 1 POM 1 ## 2 POSS MARIJ 199 ## 3 POSS MARIJ - LT 2 OZ 1 ## 4 POSS MARIJ (MB) 1 ## 5 POSS MARIJ &lt; 4OZ 2 ## 6 POSS MARIJ &lt;= 4OZ &gt; 2OZ 52 ## 7 POSS MARIJ &lt;1OZ 1 ## 8 POSS MARIJ &lt;4OZ 21 ## 9 POSS MARIJ &gt; 2OZ 1 ## 10 POSS MARIJ &gt;2OZ 1 ## # … with 18 more rows 13.8 Export your updated data frame Export into data-processed "],["sped-import.html", "Chapter 14 SPED Joins 14.1 Goals for this section 14.2 Our project data 14.3 About data sources 14.4 Create a new project 14.5 Downloading raw data 14.6 Assign our import to a data frame 14.7 Prepare data for joining 14.8 Joining the data together 14.9 Exporting data 14.10 Review of what we’ve learned so far 14.11 Resources", " Chapter 14 SPED Joins DRAFT FOR v1.0 This three-part lesson was originally conceived as a first project instead of Billboard so it has some pretty specific details and explanation. fix lists 14.1 Goals for this section Learn a little about data types available to R. Practice organized project setup. Learn about R packages, how to install and import them. Learn how to import CSV files. Introduce the Data Frame/Tibble. Introduce the tidyverse %&gt;%. Using readr package Introduce joins 14.2 Our project data We achieve these goals while working with same Texas Education Agency data from their [TAPR] tool. This is a replication of an assignment for Workbench. For now, see this assignment for details about the data and concepts. Here are the questions we seek from the data: Which campus gained the most (count difference) special education students from 2015 to 2020? Which campus has the highest share (percent) of special education students in 2020? Which campus had the greatest change in share of in special education students between 2015 and 2020. How many AISD schools would be above the special education “audit threshold” of 8.5% in 2020 if it were still in effect? How does those numbers compare to 2015? We want to only consider “Regular Instructional” schools and exclude alternative, disciplinary or schools part of the justice system. I NEED TO ADD MORE DETAIL HERE, BUT USE THE ASSIGNMENT LINK FOR NOW FOR BACKGROUND 14.3 About data sources After installing and launching RStudio, the next trick is to import data. Depending on the data source, this can be brilliantly easy or a pain in the rear. It all depends on how well-formatted is the data. In this class, we will primarily be using data from Excel files, CSVs (Comma Separated Value) and APIs (Application Programming Interface). CSVs are a kind of lowest-common-denominator for data. Most any database or program can import or export them. Excel files are good, but are often messy because humans get involved. There often have multiple header rows, columns used in multiple ways, notes added, etc. Just know you might have to clean them up before using them. APIs are systems designed to respond to programming. In the data world, we often use the APIs by writing a query to ask a system to return a selection of data. By definition, the data is well structured. You can often determine the file type of the output as part of the API call, including … JSON (or JavaScript Object Notation) is the data format preferred by JavaScript. R can read it, too. It is often the output format of APIs, and prevalent enough that you need to understand how it works. We’ll get into that later in semester. Don’t get me wrong … there are plenty of other data types and connections available through R, but those are the ones we’ll deal with most in this book. 14.3.1 What is clean data The Checking Your Data section of this DataCamp tutorial has a good outline of what makes good data, but in general it should: Have a single header row with well-formed column names. One column name for each column. No merged cells. Short names are better than long ones. Spaces in names make them harder to work with. Use and _ or . between words. Remove notes or comments from the files. Each column should have the same kind of data: numbers vs words, etc. Each row should be a single thing called an “observation.” The columns should describe that observation. Data rarely comes clean like that. There can be many challenge in importing and cleaning data. We’ll face some of those challenges here. 14.4 Create a new project We did this in our first lesson, but here are the basic steps: Launch RStudio Use the +R button to create a New Project in a New Directory Name the project yourfirstname-special-ed and put it in your ~/Documents/rwd folder. Use the + button to use R Notebook to start a new notebook. Change the title to “TEA Special Education.” Delete the other boilerplate text. Save the file as 01-import.Rmd. 14.4.1 The R Package environment We have to back up from the step-by-step nature of this lesson and talk a little about the R programming language. R is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful. What happens is developers will find it difficult to do a certain task, so they will write an R “Package” of code that helps them with that task. They share that code with the community, and suddenly the R garage has an “ultimate set of tools” that would make Spicoli’s dad proud. One set of these tools is Hadley Wickham’s Tidyverse, a set of packages for data science. These are the tools we will use most in this course. While not required reading, I highly recommend Wickham’s book R for data science, which is free. We’ll use some of Wickham’s lectures in the course. There are also a series of useful cheatsheets that can help you as you use the packages and functions from the tidyverse. We’ll refer to these throughout the course. 14.4.2 Installing and using packages There are two steps to using an R package: Install the package using `install.packages(“package_name”). You only have to do this once for each computer, so I usually do it using the R Console instead of in notebook. Include the library using library(package_name). This has to be done for each Notebook or script that uses it, so it is usually one of the first things in the notebook. We’re going to install several packages we will use in the ratings project. To do this, we are going to use the Console, which we haven’t talked about much yet. REPLACE THIS IMAGE The Console and Terminal Use the image above to orient yourself to the R Console and Terminal. In the Console, type in: install.packages(&quot;tidyverse&quot;) As you type into the Console, you’ll see some type-assist hints on what you need. You can use the arrow keys to select one and hit Tab to complete that command, then enter the values you need. If it asks you to install “from source,” type Yes and hit return. You’ll see a bunch of response in the Console. We’ll need another package, so also do: install.packages(&quot;janitor&quot;) We’ll use some commands from janitor to clean up our data column names, among other things. A good reference to learn more is the janitor vignette. You only have to install the packages once on your computer (though you have to load them every time, which is explained below). 14.4.3 Load the libraries Next, we’re going to tell our R Notebook to use these two libraries. After the metadata at the top of your notebook, use Cmd+option+i to insert an R code chunk. In that chunk, type in the two libraries and run the code block with Cmd+Shift+Return. This is the code you need: library(tidyverse) library(janitor) Your output will look something like this: Libraries imported 14.5 Downloading raw data 14.5.1 Create a directory for your data I want you to create a folder called data-raw in your project folder. We are creating this folder because we want to keep a pristine version of it our original data that we never change or overwrite. This is a basic data journalism principle: Thou shalt not change raw data. In your Files pane at the bottom-right of Rstudio, there is a New Folder icon. Click on the New Folder icon. Name your new folder data-raw. This is where we’ll put raw data. We never write data to this folder. Also create another new folder called data-processed. This is were we write data. We separate them so we don’t accidentally overwrite raw data. Once you’ve done that, they should show up in the file explorer in the Files pane. Click the refresh button if you don’t see them. (The circlish thing at top right of the screenshot below.) Directory made You can ignore the .gitignore and docs folders I have there. Those are for publishing steps we’ll learn later. 14.5.2 Let’s get some data Now that we have a folder for our data, we can download our data into it. I have a copy of the data in the class Github repo. The process to acquire this data is explained in the Special Education assignment in the RWD Mastery Assignments Github repository. MIGHT UPDATE later to download the files directly instead of using download.file In the interest of time, you can just download my copy using the download.file function in R. Add a Markdown headline and text that indicates you are downloading data. You would typically include a link and explain what it is, etc. You can build a link in Markdown with [name of the site](url-to-the-site.html). Create an R chunk and include the following: # cstud15 download download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/main/ed-special-ed/data/CSTUD15.csv?raw=true&quot;, &quot;data-raw/CSTUD15.csv&quot;) # cstud20 download download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/main/ed-special-ed/data/CSTUD20.csv?raw=true&quot;, &quot;data-raw/CSTUD20.csv&quot;) # directory download download.file(&quot;https://github.com/utdata/rwd-mastery-assignments/blob/main/ed-special-ed/data/Directory.csv?raw=true&quot;, &quot;data-raw/Directory.csv&quot;) This download.file function takes at least two arguments: The URL of the file you are downloading, and then the path and name of where you want to save it. When you run this, it should save the file and then give you output similar to this for each of the three files: trying URL &#39;https://github.com/utdata/rwd-mastery-assignments/blob/main/ed-special-ed/data/CSTUD15.csv?raw=true&#39; Content type &#39;text/plain; charset=utf-8&#39; length 331596 bytes (323 KB) ================================================== downloaded 323 KB 14.5.3 Peek at the data file You can inspect the data before you import it into your RNotebook. In the Files pane, click on the data-raw folder to open in. Click on the CSTUD15.csv file until you get the drop down that says View Files. View file The file should open into a new window. It will look like this: ratings file The numbers on the left are row numbers in the file. We can see first row is our column headers and the first column is our CAMPUS ID. This ID identifies our campus, but you’ll notice there is no school name. We’ll have to get that from the Directory.csv file in a minute. At this point the data is only on our computer in a folder within our project. It has not been imported into our RNotebook yet. Close this file now by clicking on the small x next to the file name. 14.5.4 Import csv as data Since we are doing a new thing, we should note that with a Markdown headline and text. Add a Markdown headline: ## Import data Add some text to explain that we are importing the school ratings data. After your description, add a new code chunk (Cmd+Option+i). We’ll be using the read_csv() function from the tidyverse readr package, which is different from read.csv that comes with R. It is mo betta. Inside the function we put in the path do our data, inside quotes. If you start typing in that path and hit tab, it will complete the path. (Easier to show than explain). Add the follow code into your chunk and run it. read_csv(&quot;data-raw/CSTUD15.csv&quot;) You get two results printed to your screen. The first result called “R Console” shows what columns were imported and the data types. It’s important to review these to make sure things happened the way that expected. In this case it looks like it imported CAMPUS as text – noted as col_character(), (the default) – and everything as col_double, which is a number. Note: Red colored text in this output is NOT an indication of a problem. RConsole output The second result spec_tbl_df prints out the data like a table. The data object is called a data frame or tibble, which is a fancy tidyverse version of a data frame that is part of the tidyverse. I will use the term tibble and data frame interchangably. Think of data frames and tibbles like a well-structured spreadsheet. They are organized rows of data (called observations) with columns (called variables) where every item in the column is of the same data type. Data output When we look at the data output into RStudio, there are several things to note: Below each column name is an indication of the data type. This is important. You can use the arrow icon on the right to page through the additional columns. You can use the paging numbers and controls at the bottom to page through the rows of data. The number of rows and columns is displayed at the bottom. The Student Enrollment by Program portion of the Campus Student Information data dictionary is especially helpful in understanding what this data is since the column names are not very telling. Of special note here, we have only printed this data to the screen. We have not saved it in any way, but that will come soon. 14.6 Assign our import to a data frame As of right now, we’ve only printed the data to our screen. We haven’t “saved” it at all. Next we need to assign it to an Robject so it can be named thing in our project environment so we can reuse it. We don’t want to re-import the data every time we use the data. The syntax to create and object in R can seem weird at first, but the convention is to name the object first, then insert stuff into it. So, to create an object, the structure is this: # this is pseudo code. don&#39;t run it. new_object &lt;- stuff_going_into_object Let’s make a object called cstud15 and fill it with our imported tibble. Edit your existing code chunk to look like this. You can add the &lt;- by using Option+- as in holding down the Option key and then pressing the hyphen: cstud15 &lt;- read_csv( &quot;data-raw/cstud15.csv&quot; ) Run that chunk and two things happen: We no longer see the result printed to the screen. That’s because we created a tibble instead of printing it to the screen. In the Environment tab at the top-right of RStudio, you’ll see the cstud15 object listed. Click on the blue play button next to ratings and it will expand to show you a summary of the columns. Click on the name and it will open a “View” of the data in another window, so you can look at it in spreadsheet form. You can even sort and filter it. Close the data view once you’ve looked at it. Since cstud15 is a data frame object, we’ll just call it a data frame henceforth. 14.6.1 Print a peek to the screen Since we can’t see the data after we assign it, let’s print the object to the screen so we can refer to it. Edit your import chunk to add the last two lines of this, including the one with the #: cstud15 &lt;- read_csv( &quot;data-raw/cstud15.csv&quot; ) # peek at the data cstud15 This prints your saved data frame to the screen. The line with the # is a comment within the code chunk. Commenting what your code is important to your future self, and sometimes we do that within the code chunk instead of markdown if it will be more clear. 14.6.2 Glimpse the data There is another way to peek at the data that I prefer because it is more compact and shows you all the columns and data examples without scrolling: glimpse(). In your existing chunk, edit the last line to add the glimpse() function as noted below. I’m showing the return here as well. Afterward I’ll explain the pipe: %&gt;%. cstud15 &lt;- read_csv( &quot;data-raw/cstud15.csv&quot; ) ## Rows: 8646 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): CAMPUS ## dbl (8): CPETBILC, CPETBILP, CPETGIFC, CPETGIFP, CPETSPEC, CPETSPEP, CPETVOC... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # peek at the data cstud15 %&gt;% glimpse() ## Rows: 8,646 ## Columns: 9 ## $ CAMPUS &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;00190300… ## $ CPETBILC &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 10, 6, 1, 3, 15, 2, 1, 26, 20, 104, 54, 130… ## $ CPETBILP &lt;dbl&gt; 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3, 2.3, 0.4, 1.7, 3.7, 1.8, 0… ## $ CPETGIFC &lt;dbl&gt; 27, 15, 18, 23, 0, 9, 3, 12, 18, 5, 23, 7, 17, 33, 18, 4, 0, … ## $ CPETGIFP &lt;dbl&gt; 13.1, 12.5, 7.2, 6.5, 0.0, 3.0, 1.0, 4.7, 8.1, 2.9, 5.7, 6.1,… ## $ CPETSPEC &lt;dbl&gt; 37, 16, 23, 33, 1, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, 27… ## $ CPETSPEP &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 100.0, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4,… ## $ CPETVOCC &lt;dbl&gt; 144, 37, 0, 273, 0, 0, 0, 0, 213, 0, 0, 103, 15, 849, 247, 0,… ## $ CPETVOCP &lt;dbl&gt; 69.9, 30.8, 0.0, 76.9, 0.0, 0.0, 0.0, 0.0, 95.5, 0.0, 0.0, 90… This also shows there are 8,646 rows and 9 columns in our data. Each column is then listed out with its data type and the first several values in that column. 14.6.3 About the pipe %&gt;% We need to break down this code a little: cstud15 %&gt;% glimpse(). We are starting with the data frame cstud15, but then we follow it with %&gt;%, which is called a pipe. It is a tidyverse tool that allows us to take the results of an object or function and pass into another function. Think of it at “AND THEN” the next thing. It might look like there are to arguments inside glimpse(), but what we are actually doing is passing the results of cstud15 into it. IMPORTANT: There is a keyboard command for the pipe %&gt;%: Cmd+Shift+m. Learn that one. 14.6.4 Creating consistent column names A good trait for data journalist is to be anal retentive obsessive. One thing I almost always do after importing data is to run it through a function called clean_names() that makes all the column names lowercase, removes spaces and fixes other things that can cause problems later. clean_names() is part of the janitor package we installed above. Edit your code chunk to look like the code below and run it. cstud15 &lt;- read_csv(&quot;data-raw/cstud15.csv&quot;) %&gt;% clean_names() ## Rows: 8646 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): CAMPUS ## dbl (8): CPETBILC, CPETBILP, CPETGIFC, CPETGIFP, CPETSPEC, CPETSPEP, CPETVOC... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # peek at the data cstud15 %&gt;% glimpse() ## Rows: 8,646 ## Columns: 9 ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;00190300… ## $ cpetbilc &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 10, 6, 1, 3, 15, 2, 1, 26, 20, 104, 54, 130… ## $ cpetbilp &lt;dbl&gt; 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3, 2.3, 0.4, 1.7, 3.7, 1.8, 0… ## $ cpetgifc &lt;dbl&gt; 27, 15, 18, 23, 0, 9, 3, 12, 18, 5, 23, 7, 17, 33, 18, 4, 0, … ## $ cpetgifp &lt;dbl&gt; 13.1, 12.5, 7.2, 6.5, 0.0, 3.0, 1.0, 4.7, 8.1, 2.9, 5.7, 6.1,… ## $ cpetspec &lt;dbl&gt; 37, 16, 23, 33, 1, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, 27… ## $ cpetspep &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 100.0, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4,… ## $ cpetvocc &lt;dbl&gt; 144, 37, 0, 273, 0, 0, 0, 0, 213, 0, 0, 103, 15, 849, 247, 0,… ## $ cpetvocp &lt;dbl&gt; 69.9, 30.8, 0.0, 76.9, 0.0, 0.0, 0.0, 0.0, 95.5, 0.0, 0.0, 90… When writing R code we’ll often string a bunch of functions together with the %&gt;% so for readability we often put the “next” function a new indented line. When you do this, the %&gt;% must come at the end of the previous line, like the clean_names() function above. 14.6.5 Import other data frames We need to import our other two data frames so we can prepare and join them. Create a new chunk for each data set, import them, clean the names and glimpse the data. We’ll talk more about these data in a minute when we prepare it. Name your new data frames cstud20 and directory. cstud20 &lt;- read_csv(&quot;data-raw/cstud20.csv&quot;) %&gt;% clean_names() ## Rows: 8866 Columns: 21 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): CAMPUS, CPNTVHSP ## dbl (19): CPETBILC, CPETBILP, CPETG9XC, CPETGIFC, CPETGIFP, CPETSPEC, CPETSP... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # peek at the data cstud20 %&gt;% glimpse() ## Rows: 8,866 ## Columns: 21 ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;00190304… ## $ cpetbilc &lt;dbl&gt; 0, 1, 0, 0, 3, 12, 7, 3, 5, 14, 1, 6, 126, 85, 96, 20, 92, 13… ## $ cpetbilp &lt;dbl&gt; 0.0, 0.8, 0.0, 0.0, 1.2, 3.6, 2.5, 1.3, 2.8, 3.7, 0.9, 2.5, 1… ## $ cpetg9xc &lt;dbl&gt; 172, 0, 0, 378, 0, 0, 0, 228, 0, 0, 111, 0, 987, 0, 0, 0, 0, … ## $ cpetgifc &lt;dbl&gt; 14, 17, 8, 10, 13, 4, 17, 40, 14, 17, 7, 16, 35, 18, 3, 0, 8,… ## $ cpetgifp &lt;dbl&gt; 8.1, 13.6, 3.1, 2.6, 5.0, 1.2, 6.1, 17.5, 8.0, 4.5, 6.3, 6.6,… ## $ cpetspec &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70, 50, … ## $ cpetspep &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5, 9.9… ## $ cpetvhsc &lt;dbl&gt; 134, 0, 0, 138, 0, 0, 0, 207, 0, 0, 49, 0, 918, 0, 0, 0, 0, 0… ## $ cpetvhsp &lt;dbl&gt; 77.9, 0.0, 0.0, 36.5, 0.0, 0.0, 0.0, 90.8, 0.0, 0.0, 44.1, 0.… ## $ cpetvocc &lt;dbl&gt; 165, 0, 0, 341, 37, 0, 0, 207, 0, 0, 107, 53, 975, 133, 0, 0,… ## $ cpetvocp &lt;dbl&gt; 95.9, 0.0, 0.0, 90.2, 14.3, 0.0, 0.0, 90.8, 0.0, 0.0, 96.4, 2… ## $ cpntbilc &lt;dbl&gt; 0, 1, 0, 0, 3, 12, 7, 3, 5, 14, 1, 6, 126, 85, 96, 20, 92, 13… ## $ cpntbilp &lt;dbl&gt; 0.0, 0.8, 0.0, 0.0, 1.2, 3.6, 2.5, 1.3, 2.8, 3.7, 0.9, 2.5, 1… ## $ cpntg9xc &lt;dbl&gt; 172, 0, 0, 378, 0, 0, 0, 228, 0, 0, 111, 0, 987, 0, 0, 0, 0, … ## $ cpntgifc &lt;dbl&gt; 14, 17, 8, 10, 13, 4, 17, 40, 14, 17, 7, 16, 35, 18, 3, 0, 8,… ## $ cpntgifp &lt;dbl&gt; 8.1, 13.6, 3.1, 2.6, 5.0, 1.2, 6.1, 17.5, 8.0, 4.5, 6.3, 6.6,… ## $ cpntspec &lt;dbl&gt; 34, 17, 31, 44, 29, 49, 26, 19, 28, 45, 11, 15, 103, 70, 51, … ## $ cpntspep &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.7, 9.4, 8.3, 15.9, 11.8, 9.9… ## $ cpntvhsc &lt;dbl&gt; 134, 0, 0, 138, 0, 0, 0, 207, 0, 0, 49, 0, 918, 0, 0, 0, 0, 0… ## $ cpntvhsp &lt;chr&gt; &quot;77.9&quot;, &quot;.&quot;, &quot;.&quot;, &quot;36.5&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;90.8&quot;, &quot;.&quot;, &quot;.&quot;, &quot;4… directory &lt;- read_csv(&quot;data-raw/Directory.csv&quot;) %&gt;% clean_names() ## Rows: 9572 Columns: 34 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (32): County Number, County Name, Region Number, District Number, Distri... ## dbl (2): District Enrollment as of Oct 2019, School Enrollment as of Oct 2019 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # peek at the data directory %&gt;% glimpse() ## Rows: 9,572 ## Columns: 34 ## $ county_number &lt;chr&gt; &quot;&#39;001&quot;, &quot;&#39;001&quot;, &quot;&#39;001&quot;, &quot;&#39;001&quot;, &quot;&#39;0… ## $ county_name &lt;chr&gt; &quot;ANDERSON COUNTY&quot;, &quot;ANDERSON COUNTY… ## $ region_number &lt;chr&gt; &quot;&#39;07&quot;, &quot;&#39;07&quot;, &quot;&#39;07&quot;, &quot;&#39;07&quot;, &quot;&#39;07&quot;, … ## $ district_number &lt;chr&gt; &quot;&#39;001902&quot;, &quot;&#39;001902&quot;, &quot;&#39;001902&quot;, &quot;&#39;… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA… ## $ district_type &lt;chr&gt; &quot;INDEPENDENT&quot;, &quot;INDEPENDENT&quot;, &quot;INDE… ## $ district_street_address &lt;chr&gt; &quot;P O BOX 427&quot;, &quot;P O BOX 427&quot;, &quot;P O … ## $ district_city &lt;chr&gt; &quot;CAYUGA&quot;, &quot;CAYUGA&quot;, &quot;CAYUGA&quot;, &quot;ELKH… ## $ district_state &lt;chr&gt; &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;,… ## $ district_zip &lt;chr&gt; &quot;75832-0427&quot;, &quot;75832-0427&quot;, &quot;75832-… ## $ district_phone &lt;chr&gt; &quot;(903) 928-2102&quot;, &quot;(903) 928-2102&quot;,… ## $ district_fax &lt;chr&gt; &quot;(903) 928-2646&quot;, &quot;(903) 928-2646&quot;,… ## $ district_email_address &lt;chr&gt; &quot;admin1@cayugaisd.com&quot;, &quot;admin1@cay… ## $ district_web_page_address &lt;chr&gt; &quot;www.cayugaisd.com&quot;, &quot;www.cayugaisd… ## $ district_superintendent &lt;chr&gt; &quot;DR RICK WEBB&quot;, &quot;DR RICK WEBB&quot;, &quot;DR… ## $ district_enrollment_as_of_oct_2019 &lt;dbl&gt; 557, 557, 557, 1247, 1247, 1247, 12… ## $ school_number &lt;chr&gt; &quot;&#39;001902001&quot;, &quot;&#39;001902041&quot;, &quot;&#39;00190… ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAY… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR I… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ school_street_address &lt;chr&gt; &quot;P O BOX 427&quot;, &quot;P O BOX 427&quot;, &quot;P O … ## $ school_city &lt;chr&gt; &quot;CAYUGA&quot;, &quot;CAYUGA&quot;, &quot;CAYUGA&quot;, &quot;ELKH… ## $ school_state &lt;chr&gt; &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;,… ## $ school_zip &lt;chr&gt; &quot;75832-0427&quot;, &quot;75832-0427&quot;, &quot;75832-… ## $ school_phone &lt;chr&gt; &quot;(903) 928-2294 ext:012&quot;, &quot;(903) 92… ## $ school_fax &lt;chr&gt; &quot;(903) 928-2239&quot;, &quot;(903) 928-2646&quot;,… ## $ school_email_address &lt;chr&gt; &quot;admin1@cayugaisd.com&quot;, &quot;admin1@cay… ## $ school_web_page_address &lt;chr&gt; &quot;www.cayugaisd.com&quot;, &quot;www.cayugaisd… ## $ school_principal &lt;chr&gt; &quot;DR JOE E SATTERWHITE III&quot;, &quot;MRS SH… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-… ## $ school_enrollment_as_of_oct_2019 &lt;dbl&gt; 172, 125, 260, 378, 0, 258, 334, 27… ## $ school_status &lt;chr&gt; &quot;Active&quot;, &quot;Active&quot;, &quot;Active&quot;, &quot;Acti… ## $ school_status_date &lt;chr&gt; NA, &quot;03/11/1996&quot;, NA, NA, &quot;06/28/20… ## $ update_date &lt;chr&gt; &quot;2/25/2021 5:35:04 AM&quot;, &quot;2/25/2021 … 14.7 Prepare data for joining To meet our story goals here we need pieces of all three of these data sets put together in one dataframe. As such, we’re going to jump ahead with some data wrangling techniques because we need them to prepare and join our data for our next lesson. Some explanations may seem cursory if I plan on getting into more detail later. 14.7.1 Prepare the directory data 14.7.1.1 Selecting directory columns For our Directory data, we only need about a half dozen of the 30+ columms in the data. We will use the select() function to pluck columns out of the dataframe and assign them to a new data frame. Here is the code, then I’ll explain it. directory_cols &lt;- directory %&gt;% select( school_number, school_name, district_name, instruction_type, charter_type, grade_range ) directory_cols %&gt;% glimpse() ## Rows: 9,572 ## Columns: 6 ## $ school_number &lt;chr&gt; &quot;&#39;001902001&quot;, &quot;&#39;001902041&quot;, &quot;&#39;001902103&quot;, &quot;&#39;001903001… ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;KG-12&quot;, &quot;&#39;0… When we use select() we are selecting columns from the data. Inside the function we list them by name or order. There are many variations of select() which we’ll get into later. Note also code indents for the select() function. We write the code this way to we can easily identify and edit the arguments inside the function (our list of columns in this case.) 14.7.1.2 Clean up the school number In this data set, the TEA included a ' before each value in the school_number column. They do this to ensure that it is imported as a text and not a number, which might drop the very important 00 at the beginning of the ID. It is this ID that allows us to match the directory with all other kinds of TEA data about schools. We need to remove that ' but keep the column as a text data type. There are many different ways we could do this but we’ll introduce mutate() and str_replace() to accomplish the task. We’ll get into both of these functions in more detail in a later lesson. Edit your existing code block creating the directory_cols data frame with a new %&gt;% and functions indicated below and run them. I’ll explain them after. directory_cols &lt;- directory %&gt;% select( school_number, school_name, district_name, instruction_type, charter_type, grade_range ) %&gt;% mutate( campus = school_number %&gt;% str_replace(&quot;&#39;&quot;,&quot;&quot;) ) directory_cols %&gt;% glimpse() ## Rows: 9,572 ## Columns: 7 ## $ school_number &lt;chr&gt; &quot;&#39;001902001&quot;, &quot;&#39;001902041&quot;, &quot;&#39;001902103&quot;, &quot;&#39;001903001… ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;KG-12&quot;, &quot;&#39;0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;… What we did there with your mutate() function was create a NEW column called campus that starts with the school_number field AND THEN uses a string replace function str_replace() to replace the ' character with an empty string, or nothing. We named our new column campus because that is that the ID is called in the CSTUD data, and that will make joining easier later. One thing of special note here: We not only used a %&gt;% to append the mutate() function, but we also used a %&gt;% WITHIN the mutate arguments, modifying the school_number field along the way. This is called nesting functions. Not to get to far into the weeds, but the tidyverse %&gt;% makes this is a lot easier to read and understand than base R, which would be: mutate(str_replace(school_number,\"'\",\"\")). 14.7.1.3 Remove the old school number columns Since we don’t need the school_number column anymore, we will remove it with select(), but we put a - before the column name because we want to remove it. Append the new select() lines below to your existing code block. directory_cols &lt;- directory %&gt;% select( school_number, school_name, district_name, instruction_type, charter_type, grade_range ) %&gt;% mutate( campus = school_number %&gt;% str_replace(&quot;&#39;&quot;,&quot;&quot;) ) %&gt;% select(-school_number) directory_cols %&gt;% glimpse() ## Rows: 9,572 ## Columns: 6 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;KG-12&quot;, &quot;&#39;0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;… OK, we are done preparing our new directory_cols data frame. We’ll work next to prepare our CSTUD data. 14.7.2 Prepare CSTUD data We only need the campus and the two special education values from our CSTUD data for this story. If we review our data dictionaries (2015 | 2020) we can see these special education columns are called CPETSPEC (the number of special education students) and CPETSPEP (the percentage of students in the school that are in special education). In 2020, the TEA modified how they define enrollment, so don’t get confused by those figures in 2020 data and later. According to Page 31 of the TAPR Glossary (linked from here): Please note, the Enrollment section of this report is new this year. The definitions below describe the nuances between Membership and Enrollment. If comparing the data shown from this year’s report to previous reports, use the data displayed under Membership. We’ll use select() to capture just the campus, cpetspec and cpetspep columns and put them into new data frames and then glimpse them. But we need to rename the special education columns to include the year they come from so we can tell them apart when they all get joined together. We use the rename() function to do this. Like a lot of things in R, we name the new thing first and then “fill it” afterward. Add a Markdown note that we are preparing the student enrollment data Add a new R chunk and add the following: # set up 2015 cstud15_cols &lt;- cstud15 %&gt;% select( campus, cpetspec, cpetspep ) %&gt;% rename( sped_c_15 = cpetspec, sped_p_15 = cpetspep ) # set up 2020 cstud20_cols &lt;- cstud20 %&gt;% select( campus, cpetspec, cpetspep ) %&gt;% rename( sped_c_20 = cpetspec, sped_p_20 = cpetspep ) # peek at both cstud15_cols %&gt;% glimpse() ## Rows: 8,646 ## Columns: 3 ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;0019030… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 1, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, 2… ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 100.0, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4… cstud20_cols %&gt;% glimpse() ## Rows: 8,866 ## Columns: 3 ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;0019030… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70, 50,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5, 9.… 14.8 Joining the data together This is another case where we’ll just touch on a skill – joining data – that we’ll detail more later. You might review the video about joins for a refresher on the terms and concepts. Our aim here is to create a single data set with a single row for each school that includes information from each of the three data sets. We do this by joining on the campus ID within each of the three data sets. However, as we do so, we’ll end up losing some rows that don’t have matches in the other files. To do the comparison for our story we need the schools that were open in both 2015 and 2020, but some schools existed in only one year or the other, so we’ll drop those. When we join our files, we’ll use an inner_join() to keep only schools with matching records from both 2015 and 2020. This will drop schools that opened after 2015 or closed before 2020. The join() functions (there are several) generally take three arguments: the two tables to join and then by = to say which columns to match on. inner_join(first_table, second_table, by = \"column\") In our case we are using some tidyverse foo, which I’ll explain after you add and run the chunk. Add notes in Markdown to note what you are doing. Create a new code block for the joins Add the following and run it. sped &lt;- directory_cols %&gt;% inner_join(cstud15_cols) ## Joining, by = &quot;campus&quot; sped %&gt;% glimpse() ## Rows: 8,087 ## Columns: 8 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;KG-12&quot;, &quot;&#39;0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 1, 36, 29, 27, 28, 16, 34, 16, 25, 96… ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 100.0, 11.8, 9.6, 10.5, 12.6, 9… Here is what’s going on here: We start with our new dataframe we are creating: sped andbwe use &lt;- to fill it. To fill sped we start with the directory data and then pipe it %&gt;% into an inner_join() function as the first table argument. Inside the inner_join() we name the second column we are joining, cstud15_cols. We did not specify which columns to join by = so it did what is called a “natural join,” using all the variables with common names across the two tables. It noted that with the message: Joining, by = “campus”. Note we went from 9,572 rows in our directory data to just 8,087 rows with our joined data. This inner_join kept just the records that had the same campus ID values in both tables. Now we will amend our join statement to also join the more recent data. Append the join statement as follows: sped &lt;- directory_cols %&gt;% inner_join(cstud15_cols) %&gt;% inner_join(cstud20_cols) ## Joining, by = &quot;campus&quot; ## Joining, by = &quot;campus&quot; sped %&gt;% glimpse() ## Rows: 8,002 ## Columns: 10 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;E… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 4… ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, … ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 1… The second join worked the same way as the first. It also reducee the number of rows down to 8,002 as it dropped more schools that are not present in all the data sets. We’ve done quite a bit here if you think about it: We imported three different data sets We selected only the columns we needed from each of the three columns We sometimes mutated data needed adjusting, like the school_number. We renamed our merging data so we can better identify them later. We did the actual merge. Now that we have all this data together, we can start doing some math and other calculations to see how the numbers of special education students have changed over time after the Legislature changed rules. We’ll do that in the next lesson. 14.9 Exporting data 14.9.1 Single-notebook philosophy I have a pretty strong opinion that you should be able to open any RNotebook in your project and run it from top to bottom without it breaking. In short, one notebook should not be dependent on the previous running of another notebook, unless it is to provide an output file that can be accessed later by another notebook. This is why I had you name this notebook 01-import.Rmd with a number 1 at the beginning. We’ll number our notebooks in the order they should be run. It’s an indication that before we can use the notebook 02-computations.Rmd (next lesson!) that the 01-import.Rmd notebook has to be run first. But what is important is we will create an exported file from our first notebook that can be used in the second one. Once we create that file, the second notebook can be opened and run at any time. Why make this so complicated? It might seem like overkill for this project … after all, the csv file imported cleanly and we don’t need to do anything further to it except lowercase column names. The answer is consistency. When you follow the same project structure each, you quickly know how to dive into that project at a later date. If everyone on your team uses the same structure, you can dive into your teammates code because you already know how it is organized. If we separate our importing and cleaning into it’s own file to be used by many other notebooks, we can fix future cleaning problems in ONE place instead of many places. One last example to belabor the point: It can save time. I’ve had import/cleaning notebooks that took 20 minutes to process. Imagine if I had to run that every time I wanted to rebuild my analysis notebook. Instead, the import notebook spits out clean file that can be imported in a fraction of that time. This was all a long-winded way of saying we are going to export our data now. 14.9.2 Exporting as rds We are able to pass cleaned data between notebooks because of a native R data format called .rds. When we export in this format it saves not only rows and columns, but also the data types. (If we exported as CSV, we would potentially have to re-fix data types when we imported again.) We will use another readr function called write_rds() to create our file to pass along to the next notebook, saving the data into the data-processed folder earlier. We are separating it from our data-raw folder because “Thou shalt not change raw data” even by accident. By always writing data to this different folder, we help avoid accidentally overwriting our original data. Create a Markdown headline ## Exports and write a description that you are exporting files to .rds. Add a new code chunk and add the following code: sped %&gt;% write_rds(&quot;data-processed/01_sped.rds&quot;) So, we are starting with the sped data frame that we saved earlier. We then pipe %&gt;% the result of that into a new function write_rds(). In addition to the data, the function needs to who where to save the file, so in quotes we give the path to where and what we want to call the file: \"data-processed/01_sped.rds\". Remember, we are saving in data-processed because we never export into data-raw. We are naming the file starting with 01_ to indicate to our future selves that this output came from our first notebook. We then name it, and use the .rds extension. 14.10 Review of what we’ve learned so far Most of this lesson has been about importing and combining data, with some data mutating thrown in for fun. (OK, I have an odd sense of what fun is.) Importing data into R (or any data science program) can sometimes be quite challenging, depending on the circumstances. Here we were working with well-formed data, but we still used quite a few tools from tidyverse packages like readr (read_csv, write_rds) and dplyr (select, mutate). You’ve learned a lot already: About using RMarkdown and RNotebooks to document and organize your work About R packages like the tidyverse and janitor, and how they can add new tools to your data Swiss Army knife About preserving data integrity by separating your raw and processed data You learned how to import csv data, pluck out the parts you need an join them into something you can use Next we’ll dive a little deeper into mutate, creating the calculations from our data that we need to reach our story goals. 14.11 Resources This DataCamp tutorial on imports covers a ton of different data types and connections. "],["sped-transform.html", "Chapter 15 Transform 15.1 Rewite tasks 15.2 Goals for the section 15.3 Introducing dplyr 15.4 Start a new R Notebook 15.5 Record our goals 15.6 Setup 15.7 Import our data 15.8 Explore instruction type 15.9 Filter() 15.10 Combining filters 15.11 Select() 15.12 How to describe the change 15.13 Mutate() 15.14 Export your computations 15.15 More dplyr in next lesson", " Chapter 15 Transform MID-UPDATE v1.0 15.1 Rewite tasks Return to fill in story gaps fix lists 15.2 Goals for the section In this lesson we will continue with the Special Education project you began in the last chapter. Our goals are: To build good data journalism and programming practices. Use the dplyr tools to select, filter, sort and create new columns of data with mutate. You might read this article to get an idea. 15.3 Introducing dplyr One of the packages within the tidyverse is dplyr. Dplyr allows us to transform our data frames in ways that let us explore the data and prepare it for visualizing. It’s the R equivalent of common Excel functions like sort, filter and pivoting. There is a cheatsheet on the dplyr that you might find useful. Common dplyr functions (Some slides/images included here are used with permission from Hadley and Charlotte Wickham.) 15.4 Start a new R Notebook As I explained at the end of our last lesson, it’s a good practice to separate your downloading/import code from your other notebooks. We are creating a new notebook to handle all our computations, too, in case we want to do different analysis in different notebooks going forward. Open your special-ed project if you haven’t already. Make sure the 01-import.Rmd file is closed. Under the Session menu in RStudio, choose Restart R. This cleans our environment so we don’t have left-over objects from our previous session, though you still might see history in your Console. Create a new R Notebook and set a new title of “Special education computations.” Remove the boilerplate language and add a description of our goals: To explore an analyze our school ratings data. Mention that you have to run the other notebook first in case your someone else (or your future self) comes here first. Save your file as 02-computations.Rmd. 15.5 Record our goals One thing I like to do at the top of a notebook is outline what is I’m trying to do. What do we want to learn about these school ratings? Add a Markdown headline ## Goals. Create a bullet list of things you might want to find. Use a * or - to start each new line. - Which campus gained the most (count difference) special education students from 2015 to 2020? - Which campus has the highest share (percent) of special education students in 2020? - Which campus had the greatest change in share of in special education students between 2015 and 2020. - How many AISD schools would be above the special education &quot;audit threshold&quot; of 8.5% in 2020 if it were still in effect? How does those numbers compare to 2015? We will filter our list to focus on &quot;Regular Instructional&quot; schools to exclude alternative, disciplinary or schools part of the justice system. We will filter our list to focus on “Regular Instructional” schools to exclude alternative, disciplinary or schools part of the justice system. 15.6 Setup The first code chunk in every notebook should be a listing of all the packages used. We put them at the top so users can see if they need to install the packages. For now, we need the tidyverse and janitor packages (which I include in every notebook). Create a new Markdown headline ## Import Add a new code chunk (Cmd-Shift+i!) Add the tidyverse and janitor libraries Run the chunk library(tidyverse) library(janitor) 15.6.1 Naming our chunks It is good practice to name your chunks of code to make it easier to navigate throughout your R notebook. Let’s do this with our setup chunk. In your setup chunk, change {r} to {r setup}: {r setup} library(tidyverse) (You have to imagine the three tickmarks that begin/end the code chunk above. I can’t display those in the instructions.) You can’t have multi-word names. You must use a - or _, like multi-word. By doing this, you can then use the notebook navigation to move to different parts of your RNotebook. R Notebook navigation The {r} part of a chunk can take a number of options in addition to a name, controlling what is displayed or knitted to your final report. We aren’t going to get into it now, but you might see it in other notebooks or my code. 15.7 Import our data Add a Markdown headline and description that you are loading the data. Add a code chunk and give it a name of import. Add the code below and then run the chunk: sped &lt;- read_rds(&quot;data-processed/01_sped.rds&quot;) sped %&gt;% glimpse() ## Rows: 8,002 ## Columns: 10 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART … ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART IS… ## $ instruction_type &lt;chr&gt; &quot;REGULAR INSTRUCTIONAL&quot;, &quot;REGULAR INSTRUCTIONAL&quot;, &quot;RE… ## $ charter_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;E… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 4… ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, … ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 1… What we’ve done is import the data we processed at the end of our 01-import.Rmd notebook and assigned it to a data frame called sped and glimpsed it for reference. We are now back to where we ended with the first notebook. 15.8 Explore instruction type Our first task is to filter our data to remove any schools not rated by normal standards, like disciplinary schools and the like. Before we do this, we’ll do a quick exploration of the data available in the instruction_type field using count(). This is a “summarize” concept that we’ll cover more in the next chapter, but it is super useful so we’ll touch on it here. Create a Markdown section with a headline and text that you are exploring instruction_type. Add a code chunk and name it “explore-type.” Add this code and run it, then I’ll explain it. sped %&gt;% count(instruction_type) ## # A tibble: 4 × 2 ## instruction_type n ## &lt;chr&gt; &lt;int&gt; ## 1 ALTERNATIVE INSTRUCTIONAL 397 ## 2 DAEP INSTRUCTIONAL 121 ## 3 JJAEP INSTRUCTIONAL 112 ## 4 REGULAR INSTRUCTIONAL 7372 What you’ve done here is very similar to a pivot table in Excel where you count the rows based on values. We start with the data frame We pipe into the function count() We feed count the column name where we want to count the records by value. The n column is the number of rows. We’ll talk more about count() and other summary functions in the next chapter. It is super useful to peek at your data and use it to check filters and such. 15.9 Filter() We can use dplyr’s filter() function to capture a subset of the data, like our “REGULAR INSTRUCTIONAL” schools. It works like this: dplyr filter function IMPORTANT: When you see a slide like the one above, please note that .data might be the data that you have piped %&gt;% into the function. In fact, it usually is. Let’s put this into practice. 15.9.1 Filter for Regular Instructional schools From or count we know there are four types of schools: ALTERNATIVE INSTRUCTIONAL, DAEP INSTRUCTIONAL, JJAEP INSTRUCTIONAL and REGULAR INSTRUCTIONAL. Now that we know what the “regular” schools are called, we can build a filter to find them. Add a header ## Filter fun to your notebook. Add text that you filtering for regular schools. Add a chunk, insert and run the following: The first box shown here is the code. The second is the result. The result is prettier in RStudio. sped %&gt;% filter(instruction_type == &quot;REGULAR INSTRUCTIONAL&quot;) ## # A tibble: 7,372 × 10 ## school_name district_name instruction_type charter_type grade_range campus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 CAYUGA H S CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 2 CAYUGA MIDDLE CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 3 CAYUGA EL CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-05 00190… ## 4 ELKHART H S ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 5 ELKHART MIDD… ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 6 ELKHART EL ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-02 00190… ## 7 ELKHART INT ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;03-05 00190… ## 8 FRANKSTON H S FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 9 FRANKSTON MI… FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 10 FRANKSTON EL FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-05 00190… ## # … with 7,362 more rows, and 4 more variables: sped_c_15 &lt;dbl&gt;, ## # sped_p_15 &lt;dbl&gt;, sped_c_20 &lt;dbl&gt;, sped_p_20 &lt;dbl&gt; When you run this, you’ll see we now have only that you the about 7,372 rows instead of the 8,002 that we started with, and that also matches our “count” from above. The arguments inside the filter() function work in this order: What is the column (or variable) you are search in. What is the logical test. In our case above we are looking for matches. What is the value (or observation) you are looking for. Note the two equals signs == there. It’s important to use two of them when you are looking for “equal,” as a single = will not work, as that means something else in R. 15.9.2 Logical tests There are a number of these logical test operations: dplyr logical tests 15.9.3 Common mistakes with filter Some common mistakes that happen when using filter. 15.9.3.1 Use two == signs for “true” DON’T DO THIS: sped %&gt;% filter(district_name = &quot;AUSTIN ISD&quot;) DO THIS: sped %&gt;% filter(district_name == &quot;AUSTIN ISD&quot;) 15.9.3.2 Forgetting quotes DON’T DO THIS: ratings %&gt;% filter(grade_range == 09-12) DO THIS: ratings %&gt;% filter(district_name == &quot;09-12&quot;) 15.10 Combining filters You can filter for more than one thing at a time by using an operator: &amp;, |. !. Edit your existing code block to following code block and run it. Then look at the code and the result and then write out what the filter is doing in your own words. sped %&gt;% filter( instruction_type == &quot;REGULAR INSTRUCTIONAL&quot;, charter_type %&gt;% is.na() ) ## # A tibble: 6,962 × 10 ## school_name district_name instruction_type charter_type grade_range campus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 CAYUGA H S CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 2 CAYUGA MIDDLE CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 3 CAYUGA EL CAYUGA ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-05 00190… ## 4 ELKHART H S ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 5 ELKHART MIDD… ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 6 ELKHART EL ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-02 00190… ## 7 ELKHART INT ELKHART ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;03-05 00190… ## 8 FRANKSTON H S FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;09-12 00190… ## 9 FRANKSTON MI… FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;06-08 00190… ## 10 FRANKSTON EL FRANKSTON ISD REGULAR INSTRUCT… &lt;NA&gt; &#39;EE-05 00190… ## # … with 6,952 more rows, and 4 more variables: sped_c_15 &lt;dbl&gt;, ## # sped_p_15 &lt;dbl&gt;, sped_c_20 &lt;dbl&gt;, sped_p_20 &lt;dbl&gt; We rewrote the first filter to indent it differently so it was more clear that we have two logical tests here. The second test has a nested function, testing the charter_type field to see if it is blank, or is.na(), which is a special case. We could have written this line as is.na(charter_type). If you use a comma , or ampersand &amp; to separate two tests, then both of them have to be true. If you want OR, then you use a bar character | (the shift-key version of the backslash, above the Return key. That | is also sometimes referred to as a pipe character, but I’ll call it “bar” since we also have the %&gt;%.) Boolean operators 15.10.1 Common mistakes with combining filters Some things to watch when trying to combine filters. 15.10.1.1 Collapsing multiple tests into one DON’T DO THIS: sped %&gt;% filter(district_name == &quot;AUSTIN ISD&quot; | &quot;BASTROP ISD&quot;) DO THIS: sped %&gt;% filter(district_name == &quot;AUSTIN ISD&quot; | district_name ==&quot;BASTROP ISD&quot;) BUT EVEN BETTER: # Adding in Hays to the list as well sped %&gt;% filter(district_name %in% c(&quot;AUSTIN ISD&quot;, &quot;BASTROP ISD&quot;, &quot;ROUND ROCK ISD&quot;)) If you want to combine a series of strings in your filter, you have to put them inside a “concatenate” function, which is shortened to c(), as in the example above. We’ll end up using this a lot. 15.11 Select() When we were preparing our CSTUD data we used the select() function to select or exclude specific columns. We need to append to the filter we’ve been using to remove the instruction_type and charter_type columns now that we are done with them. In the filter code chunk you’ve been building, modify it i the following ways: Add a select function and remove the charter_type and instruction_type columns. Assign the result to a new data frame called sped_reg. Print the data frame to the screen so you can poke around. sped_reg &lt;- sped %&gt;% filter( instruction_type == &quot;REGULAR INSTRUCTIONAL&quot;, charter_type %&gt;% is.na() ) %&gt;% select(-charter_type, -instruction_type) sped_reg ## # A tibble: 6,962 × 8 ## school_name district_name grade_range campus sped_c_15 sped_p_15 sped_c_20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CAYUGA H S CAYUGA ISD &#39;09-12 001902… 37 18 34 ## 2 CAYUGA MIDDLE CAYUGA ISD &#39;06-08 001902… 16 13.3 17 ## 3 CAYUGA EL CAYUGA ISD &#39;EE-05 001902… 23 9.2 31 ## 4 ELKHART H S ELKHART ISD &#39;09-12 001903… 33 9.3 44 ## 5 ELKHART MIDD… ELKHART ISD &#39;06-08 001903… 36 11.8 29 ## 6 ELKHART EL ELKHART ISD &#39;EE-02 001903… 29 9.6 48 ## 7 ELKHART INT ELKHART ISD &#39;03-05 001903… 27 10.5 26 ## 8 FRANKSTON H S FRANKSTON ISD &#39;09-12 001904… 28 12.6 19 ## 9 FRANKSTON MI… FRANKSTON ISD &#39;06-08 001904… 16 9.2 28 ## 10 FRANKSTON EL FRANKSTON ISD &#39;EE-05 001904… 34 8.4 44 ## # … with 6,952 more rows, and 1 more variable: sped_p_20 &lt;dbl&gt; Note there are some other really cool ways to use select. Here is an example capturing columns that end with \"_name\". (The head() function just gives us the first few rows of data since we don’t need to show the whole data set.) You don’t have to do this … I’m just showing you. sped_reg %&gt;% select(ends_with(&quot;_name&quot;)) %&gt;% head() ## # A tibble: 6 × 2 ## school_name district_name ## &lt;chr&gt; &lt;chr&gt; ## 1 CAYUGA H S CAYUGA ISD ## 2 CAYUGA MIDDLE CAYUGA ISD ## 3 CAYUGA EL CAYUGA ISD ## 4 ELKHART H S ELKHART ISD ## 5 ELKHART MIDDLE ELKHART ISD ## 6 ELKHART EL ELKHART ISD 15.12 How to describe the change Now that we have all our data in the same data sped_reg, we can use create new columns compare the different years. Before we get into how, let’s talk about describing change. We have two values for each year to work with: The “Count” of special education – spec15 and spec20 – which is the actual number of students in the program; and the “Percentage” of students in special education out of the total in that school – spep15 and spep20. We want to describe the change from one year to the next. You might review the Numbers in the Newsroom chapter on Measuring Change (p26) for further study. Here are some examples: 15.12.1 Describing the count changes We can show the simple difference (or actual change) in the count of students from one year to the next. We’ll assume there were 4 students in 2015 and 10 in 2020: New Count - Old Count = Simple Difference. Example: 10 - 4 = 6. “The school served six more special education students in 2020 (10 students) compared to 2015 (four students).” We can show the percent change in the count of students from one year to the next: ((New Count - Old Count) / Old Count) * 100 = Percent change. Example: ((10 - 4) \\ 4) * 100 = 150%. “The number of special education students served increased 150% from four in 2015 to 10 in 2020.” 15.12.2 Describing percentage differences We also have the percentage of special education students in the school, which could be important. This is the share of students that are in the program compared to the total students in the school. We can find the percentage point difference from one year to the next using simple difference again, but we have the describe the change as the difference in percentage points: New Percentage - Old Percentage = Percentage Point Difference. Example: 15.5% - 11% = 4.5 percentage points (NOT 4.5%). “The share of students in special education grew by 4.5 percentage points, from 11% in 2015 to 15.5% in 2020.” We can find the percent change of share from one year to the next, but we have to again be very specific about what we are talking about … the growth (or decrease) of the share of students in special education. ((New Percentage - Old Percentage) / Old Percentage * 100) = Change in share of students. Example: ((15.5 - 11) / 11) * 100 = 40.9. “The share of students in special education grew 40% from 11% of students in 2015 to 15.5% of students in 2020.” This describes the growth in the share of students in the program, not the number of special education students overall. Describing a “percentage point difference” to readers can be difficult, but perhaps less confusing than describing the “percent change of a percent.” Great, so which do we use for this story? That depends on what you want to describe. Schools that have fewer special education students to begin with will show a more pronounced percent change with any fluctuation. Then again, a school that has a large percentage of students could be gaining a lot of students with a small percentage change. In the end, we might need to use all of these values to describe different kinds of schools. We are talking about human beings, so perhaps the counts are important. 15.13 Mutate() The mutate() function allows us to change a column of data or create new columns based on others. We can do calculations across columns or change data types. It is useful in calculations and cleaning data. For our Special Education data we need to create several calculations. 15.13.1 Simple change of counts To create a column showing the number of students up or down from 2015 to 2020, we use a simple difference between our columns. Let’s set up a new data frame to hold our calculations with a glimpse so we can view them as we go along. Remember from our preparation steps, mutate works like this: first name the new column set it to = the new value specify what the new result should be sped_calcs &lt;- sped_reg %&gt;% mutate( # simple difference sped_c_diff = sped_c_20 - sped_c_15 ) sped_calcs %&gt;% glimpse() ## Rows: 6,962 ## Columns: 9 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART H S… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART ISD&quot;,… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;001… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, … ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4, 1… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5… ## $ sped_c_diff &lt;dbl&gt; -3, 1, 8, 11, -7, 19, -1, -9, 12, 10, -5, -11, 7, 26, 23… You can see the first value differences in the glimpse above: 34 - 37 = -3. If that is difficult to see we could build a quick look to see just those columns: sped_calcs %&gt;% select(school_name, sped_c_15, sped_c_20, sped_c_diff) %&gt;% head() ## # A tibble: 6 × 4 ## school_name sped_c_15 sped_c_20 sped_c_diff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CAYUGA H S 37 34 -3 ## 2 CAYUGA MIDDLE 16 17 1 ## 3 CAYUGA EL 23 31 8 ## 4 ELKHART H S 33 44 11 ## 5 ELKHART MIDDLE 36 29 -7 ## 6 ELKHART EL 29 48 19 You always want to check your mutates to make sure they work! We will continue to build on our sped_calc data frame with new calculations. 15.13.2 Calculate percent change For this calculation we have the more complex task of the percent-change math. We also have to round the result to a readable number, accomplished by piping the math result into the round_half_up() function, which takes the argument of how many places to round the result. Modify your calculations to include the sped_c_prccng result: sped_calcs &lt;- sped_reg %&gt;% mutate( # simple diff sped_c_diff = sped_c_20 - sped_c_15, # percent change spec_c_prccng = ((sped_c_20 - sped_c_15) / sped_c_15 * 100) %&gt;% round_half_up(1), # simple diff of percent ) sped_calcs %&gt;% glimpse() ## Rows: 6,962 ## Columns: 10 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART H S… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART ISD&quot;,… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;001… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, … ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4, 1… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5… ## $ sped_c_diff &lt;dbl&gt; -3, 1, 8, 11, -7, 19, -1, -9, 12, 10, -5, -11, 7, 26, 23… ## $ spec_c_prccng &lt;dbl&gt; -8.1, 6.3, 34.8, 33.3, -19.4, 65.5, -3.7, -32.1, 75.0, 2… Check your math: ((34 - 37) / 37 * 100) = 8.108. 15.13.3 Calculate percentage point difference This is another simple difference, but this time with the percentage fields. sped_calcs &lt;- sped_reg %&gt;% mutate( # simple diff sped_c_diff = sped_c_20 - sped_c_15, # percent change sped_c_prccng = ((sped_c_20 - sped_c_15) / sped_c_15 * 100) %&gt;% round_half_up(1), # simple diff of percent sped_p_ppd = sped_p_20 - sped_p_15 ) sped_calcs %&gt;% glimpse() ## Rows: 6,962 ## Columns: 11 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART H S… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART ISD&quot;,… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;001… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, … ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4, 1… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5… ## $ sped_c_diff &lt;dbl&gt; -3, 1, 8, 11, -7, 19, -1, -9, 12, 10, -5, -11, 7, 26, 23… ## $ sped_c_prccng &lt;dbl&gt; -8.1, 6.3, 34.8, 33.3, -19.4, 65.5, -3.7, -32.1, 75.0, 2… ## $ sped_p_ppd &lt;dbl&gt; 1.8, 0.3, 2.7, 2.3, -0.6, 4.8, -1.1, -4.3, 6.7, 3.1, -4.… Again, check your work: 19.8 - 18.0 = 1.8. Remember, the challenge with using this value in writing is to couch it as percentage point difference and not a percentage or percent change. 15.13.4 Calculating our special education threshold As you may recall, the point of the Chronicle’s story was that schools that had 8.5% or higher of special education students had to go through hoops, forcing schools to try to avoid putting students in these programs. The reason we are comparing 2015 data to 2020 is to see if there are more schools with higher percentages of special education students. The way we’ll do that is to create a “threshold” column for each year. We’ll use mutate() to create a column that says “Y” if the school is at or above the threshold, and “N” if it is not. Once we do this for both years, we can use some summaries to count them. This process will introduce case_when() which compares to the IF= function in Excel. It works like this within a mutate: new_column = case_when( test ~ &quot;result_if_true&quot;, TRUE ~ &quot;result_if_false&quot; ) If that last TRUE confuses you that it is the false answer, then yeah … me too. The power of this is you can add multiple test by adding more lines before the TRUE line, but we don’t need that here. Update your sped_calcs code chunk to include both of our threshold cases. sped_calcs &lt;- sped_reg %&gt;% mutate( # simple diff sped_c_diff = sped_c_20 - sped_c_15, # percent change sped_c_prccng = ((sped_c_20 - sped_c_15) / sped_c_15 * 100) %&gt;% round_half_up(1), # simple diff of percent sped_p_ppd = sped_p_20 - sped_p_15, # meets threshold 8.5% 2015 sped15_thsh = case_when( sped_p_15 &gt;= 8.5 ~ &quot;Y&quot;, TRUE ~ &quot;N&quot; ), # meets threhhold 8.5% 2020 sped20_thsh = case_when( sped_p_20 &gt;= 8.5 ~ &quot;Y&quot;, TRUE ~ &quot;N&quot; ) ) sped_calcs %&gt;% glimpse ## Rows: 6,962 ## Columns: 13 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART H S… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART ISD&quot;,… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;001… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, … ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4, 1… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5… ## $ sped_c_diff &lt;dbl&gt; -3, 1, 8, 11, -7, 19, -1, -9, 12, 10, -5, -11, 7, 26, 23… ## $ sped_c_prccng &lt;dbl&gt; -8.1, 6.3, 34.8, 33.3, -19.4, 65.5, -3.7, -32.1, 75.0, 2… ## $ sped_p_ppd &lt;dbl&gt; 1.8, 0.3, 2.7, 2.3, -0.6, 4.8, -1.1, -4.3, 6.7, 3.1, -4.… ## $ sped15_thsh &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;… ## $ sped20_thsh &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;… And then, of course, check the result to make sure it is right. Again, if you are having trouble seeing it in the glimpse, then build yourself a new R chunk to show you columns germane to your inspection: sped_calcs %&gt;% select( school_name, sped_p_15, sped15_thsh, sped_p_20 , sped20_thsh ) %&gt;% head(20) ## # A tibble: 20 × 5 ## school_name sped_p_15 sped15_thsh sped_p_20 sped20_thsh ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CAYUGA H S 18 Y 19.8 Y ## 2 CAYUGA MIDDLE 13.3 Y 13.6 Y ## 3 CAYUGA EL 9.2 Y 11.9 Y ## 4 ELKHART H S 9.3 Y 11.6 Y ## 5 ELKHART MIDDLE 11.8 Y 11.2 Y ## 6 ELKHART EL 9.6 Y 14.4 Y ## 7 ELKHART INT 10.5 Y 9.4 Y ## 8 FRANKSTON H S 12.6 Y 8.3 N ## 9 FRANKSTON MIDDLE 9.2 Y 15.9 Y ## 10 FRANKSTON EL 8.4 N 11.5 Y ## 11 NECHES H S 14 Y 9.9 Y ## 12 NECHES EL 9.8 Y 5.8 N ## 13 PALESTINE H S 10.7 Y 10.4 Y ## 14 PALESTINE J H 9.3 Y 13.5 Y ## 15 NORTHSIDE PRI 5 N 9.7 Y ## 16 WASHINGTON EARLY CHILDHOOD CENTER 9.6 Y 8.4 N ## 17 SOUTHSIDE EL 7.7 N 14.2 Y ## 18 STORY INT 9.2 Y 12.9 Y ## 19 WESTWOOD H S 10.5 Y 12.2 Y ## 20 WESTWOOD J H 9.7 Y 15 Y 15.14 Export your computations We’re going to export this data so we can use it in multiple notebooks going forward. It might be overkill in this case, but this will allow us to do future notebooks on different districts if we want. Build a new export chunk and use write_rds() to create a file 02_sped_calcs.rds. sped_calcs %&gt;% write_rds(&quot;data-processed/02_sped_calcs.rds&quot;) Again, we saved our processed data away from the raw data. We used a number to match which notebook it came from, and we used a descriptive name so we know what is inside the file. 15.15 More dplyr in next lesson We have covered a lot in our last two lessons, including some of our Basic Data Journalism functions of select, filter and join, and we learned to create new variables with with mutate. There are several more dplyr functions that we haven’t used yet, but will in the next lesson: arrange(), group_by() and summarize(). "],["sped-summarize.html", "Chapter 16 Summarize 16.1 New task, new notebook 16.2 Sorting with arrange() 16.3 Aggregates with summarize() 16.4 Group_by() 16.5 More on summarize 16.6 Review what we learned", " Chapter 16 Summarize FIRST DRAFT AND NEEDS REWRITE fix lists In this lesson we’ll actually get answers to the questions we posed when we started this quest. All of these questions refer to Austin ISD schools: Which campus gained the most (count difference) special education students from 2015 to 2020? Which campus has the highest share (percent) of special education students in 2020? Which campus had the greatest change in share of in special education students between 2015 and 2020. How many AISD schools would be above the special education “audit threshold” of 8.5% in 2020 if it were still in effect? How does those numbers compare to 2015? 16.1 New task, new notebook We’ve been organizing our work into different RNotebooks that serve a single purpose. We started with downloading, importing and joining our data. We built a new notebook to handle our computations, and we’ll build yet another one here for our analysis. How you break up this work is really up to you, your colleagues you work with, and the needs of the project. For instance, I typically include computations in either my import/clean or analysis notebooks, but I felt there were reasons to separate in this instance: The import step included downloading and processing of data. As I work through analysis, I often restart R and re-run my notebooks and I didn’t want to re-download and process that data each time. Even though my immediate goal was to look at Austin ISD data, I wanted to preserve state-level data for possible comparison later. So I prepared the data for the entire state, leaving the Austin filtering for later. Your needs may vary by project, but strive to live by these rules: Don’t change or write over original data. One way to accomplish this is keep raw data in a separate folder and never write to it. Notebooks should run independently. You should be able to open it and do Run All and it work. (It’s OK to import data from a previous notebook, but R objects should not carry over.) If notebooks must be run in series, then indicate that in the name of the file. Use descriptive names for everything. Avoid generic terms in file names like “data” or “myfile.” This is a long-winded way of saying this: Time to start a new notebook. 16.1.1 Create analysis notebook If you haven’t already, make sure you have the project open, but close all files and restart R. Create a new RNotebook. Update the title so “Special Education AISD analysis.” Remove the boilerplate code. Save the file and name it 03-analysis-aisd.Rmd. Create your first chunk and name it “setup.” Add the tidyverse and janitor libraries. library(tidyverse) library(janitor) 16.1.2 Import We need to import our data using the read_rds() function. Create a new chunk. Name it import. Add the following sped_calcs &lt;- read_rds(&quot;data-processed/02_sped_calcs.rds&quot;) # peek at it sped_calcs %&gt;% glimpse() ## Rows: 6,962 ## Columns: 13 ## $ school_name &lt;chr&gt; &quot;CAYUGA H S&quot;, &quot;CAYUGA MIDDLE&quot;, &quot;CAYUGA EL&quot;, &quot;ELKHART H S… ## $ district_name &lt;chr&gt; &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;CAYUGA ISD&quot;, &quot;ELKHART ISD&quot;,… ## $ grade_range &lt;chr&gt; &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-05&quot;, &quot;&#39;09-12&quot;, &quot;&#39;06-08&quot;, &quot;&#39;EE-0… ## $ campus &lt;chr&gt; &quot;001902001&quot;, &quot;001902041&quot;, &quot;001902103&quot;, &quot;001903001&quot;, &quot;001… ## $ sped_c_15 &lt;dbl&gt; 37, 16, 23, 33, 36, 29, 27, 28, 16, 34, 16, 25, 96, 44, … ## $ sped_p_15 &lt;dbl&gt; 18.0, 13.3, 9.2, 9.3, 11.8, 9.6, 10.5, 12.6, 9.2, 8.4, 1… ## $ sped_c_20 &lt;dbl&gt; 34, 17, 31, 44, 29, 48, 26, 19, 28, 44, 11, 14, 103, 70,… ## $ sped_p_20 &lt;dbl&gt; 19.8, 13.6, 11.9, 11.6, 11.2, 14.4, 9.4, 8.3, 15.9, 11.5… ## $ sped_c_diff &lt;dbl&gt; -3, 1, 8, 11, -7, 19, -1, -9, 12, 10, -5, -11, 7, 26, 23… ## $ sped_c_prccng &lt;dbl&gt; -8.1, 6.3, 34.8, 33.3, -19.4, 65.5, -3.7, -32.1, 75.0, 2… ## $ sped_p_ppd &lt;dbl&gt; 1.8, 0.3, 2.7, 2.3, -0.6, 4.8, -1.1, -4.3, 6.7, 3.1, -4.… ## $ sped15_thsh &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;… ## $ sped20_thsh &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;… 16.1.3 Filter to Austin ISD Since we are seeking answers about our local district, let’s filter the data to just those schools. Create a new chunk and call is aisd. Use filter() to find rows with district_name of “AUSTIN ISD.” Use select() to remove the district_name and campus columns. Assign the result to a new data frame called aisd. Print the data frame to the screen to you can browse through it. aisd &lt;- sped_calcs %&gt;% filter(district_name == &quot;AUSTIN ISD&quot;) %&gt;% select(-district_name, -campus) aisd ## # A tibble: 109 × 11 ## school_name grade_range sped_c_15 sped_p_15 sped_c_20 sped_p_20 sped_c_diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AUSTIN H S &#39;09-12 183 8.8 223 9.5 40 ## 2 NAVARRO EARL… &#39;09-12 198 12.3 241 15.8 43 ## 3 MCCALLUM H S &#39;09-12 154 9.4 144 8.2 -10 ## 4 NORTHEAST EA… &#39;09-12 157 12.8 162 14.2 5 ## 5 TRAVIS EARLY… &#39;09-12 199 15.2 200 17 1 ## 6 CROCKETT ECHS &#39;09-12 228 15.3 238 15.5 10 ## 7 ANDERSON H S &#39;09-12 146 6.6 190 8.6 44 ## 8 BOWIE H S &#39;09-12 212 7.4 282 9.9 70 ## 9 LBJ ECHS &#39;09-12 111 13.2 128 15.1 17 ## 10 AKINS H S &#39;09-12 289 10.9 351 12.8 62 ## # … with 99 more rows, and 4 more variables: sped_c_prccng &lt;dbl&gt;, ## # sped_p_ppd &lt;dbl&gt;, sped15_thsh &lt;chr&gt;, sped20_thsh &lt;chr&gt; 16.2 Sorting with arrange() Our first question we were tasked to answer is: Which campus gained the most (count difference) special education students from 2015 to 2020? In our computations notebook we created the sped_c_diff column for just this purpose. We just need to “arrange” the data so we can see the highest number at the top! 16.2.1 About arrange The arrange() function sorts data. dataframe %&gt;% arrange(column_name) It will sort in ascending (A-Z or 1-10) by default. If you want to sort the column be descending order (Z-A or 10-1), then you add on desc() function: dataframe %&gt;% arrange(desc(column_name)) The above code wraps your sorting column with the desc() function. I’m more apt to write this the tidyverse way, as the pipe makes the order of operations more clear to me: dataframe %&gt;% arrange(column_name %&gt;% desc()) 16.2.2 Most new special education students So, let’s take our example data and arrange it with the highest count diff at the top. Add a markdown headline ## Most new special education students. Add a chunk called cntdiff-highest, add the code below and run it. aisd %&gt;% arrange(sped_c_diff %&gt;% desc()) %&gt;% head(10) ## # A tibble: 10 × 11 ## school_name grade_range sped_c_15 sped_p_15 sped_c_20 sped_p_20 sped_c_diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BOWIE H S &#39;09-12 212 7.4 282 9.9 70 ## 2 BLAZIER EL &#39;EE KG-06 78 8.1 142 15.5 64 ## 3 AKINS H S &#39;09-12 289 10.9 351 12.8 62 ## 4 SMALL MIDDLE &#39;06-08 108 10.8 163 13.3 55 ## 5 HART EL &#39;EE-05 49 6.9 104 15.7 55 ## 6 BARANOFF EL &#39;EE KG-05 64 6.5 116 11.4 52 ## 7 SUMMITT EL &#39;EE-05 55 7.1 105 12.4 50 ## 8 PATTON EL &#39;EE-05 52 5.5 101 10.6 49 ## 9 ALLISON EL &#39;EE-06 41 8.4 88 17.5 47 ## 10 HILL EL &#39;EE-05 44 5 90 9 46 ## # … with 4 more variables: sped_c_prccng &lt;dbl&gt;, sped_p_ppd &lt;dbl&gt;, ## # sped15_thsh &lt;chr&gt;, sped20_thsh &lt;chr&gt; OK, so Bowie High School is at the top with 70 new special education students. Bowie is the largest high school in the district, so perhaps that is not a surprise. 16.2.3 The head() command I’ve used the head() command above and a couple of times but haven’t really explained it. Basically it just gives you the first six rows of data unless you tell it you want a specific number of rows, like I asked for 10 above. There is also a tail() command that gives you the bottom rows of the data. I often use head() when I only need to see a couple of rows because .Rmd files get bigger when you display more data. While printing a 1000-row table to the screen only shows us 10 rows at a time, it has all 1000 rows stored in memory. 16.2.4 Highest share of special education students in 2020 We can reveal the answer for our next question by using arrange() with the sped_p_20 column. aisd %&gt;% arrange(sped_p_20 %&gt;% desc()) %&gt;% head(10) ## # A tibble: 10 × 11 ## school_name grade_range sped_c_15 sped_p_15 sped_c_20 sped_p_20 sped_c_diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BROOKE EL &#39;EE-05 43 12.4 71 24.6 28 ## 2 ZAVALA EL &#39;EE-06 61 15.3 59 23.8 -2 ## 3 DAWSON EL &#39;EE-05 74 22.2 86 23.7 12 ## 4 GARCIA YMLA &#39;06-08 80 20.5 92 23.1 12 ## 5 WIDEN EL &#39;EE-05 70 11.9 102 22.9 32 ## 6 MARTIN MIDDLE &#39;06-08 107 19.3 122 22.6 15 ## 7 WILLIAMS EL &#39;EE-05 79 15.4 93 22.2 14 ## 8 BEDICHEK MIDDLE &#39;06-08 166 17.1 187 22 21 ## 9 GOVALLE EL &#39;EE-05 57 10.5 79 21.7 22 ## 10 CUNNINGHAM EL &#39;EE-05 72 17.8 83 20.5 11 ## # … with 4 more variables: sped_c_prccng &lt;dbl&gt;, sped_p_ppd &lt;dbl&gt;, ## # sped15_thsh &lt;chr&gt;, sped20_thsh &lt;chr&gt; It looks like Brooke Elementary has the highest share of special education students, almost 1 in 4. 16.2.5 Greatest change in share of special education students Our last question to answer with arrange is who had the “greatest change in the share of special education students.” That’s a mouthful and the answer is nuanced, too. Let’s start by arranging by the percent change in the count: sped_c_prccng. aisd %&gt;% arrange( sped_c_prccng ) ## # A tibble: 109 × 11 ## school_name grade_range sped_c_15 sped_p_15 sped_c_20 sped_p_20 sped_c_diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NORMAN-SIMS … &#39;EE-06 34 11.4 12 9.5 -22 ## 2 METZ EL &#39;EE-05 40 11.2 22 10.4 -18 ## 3 SANCHEZ EL &#39;EE-05 38 8.8 21 7.8 -17 ## 4 EASTSIDE MEM… &#39;09-12 109 17.6 72 17.2 -37 ## 5 BRYKER WOODS… &#39;EE KG-06 22 5.6 16 4 -6 ## 6 PICKLE EL &#39;EE-05 48 6.5 37 7.3 -11 ## 7 WINN EL &#39;EE-06 49 14.6 40 14.2 -9 ## 8 REILLY EL &#39;EE-05 35 12.7 29 11.3 -6 ## 9 ANDREWS EL &#39;EE-06 62 9.5 52 14 -10 ## 10 OAK SPRINGS … &#39;EE-05 39 12.7 33 14.4 -6 ## # … with 99 more rows, and 4 more variables: sped_c_prccng &lt;dbl&gt;, ## # sped_p_ppd &lt;dbl&gt;, sped15_thsh &lt;chr&gt;, sped20_thsh &lt;chr&gt; Seven schools more than doubled their share of special education students. Wooldridge Elementary went from 35 to 79 students, a 132 percent increase. We can also look at the change in the percentage point change by sorting on sped_p_ppd. aisd %&gt;% arrange(sped_p_ppd %&gt;% desc()) ## # A tibble: 109 × 11 ## school_name grade_range sped_c_15 sped_p_15 sped_c_20 sped_p_20 sped_c_diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BROOKE EL &#39;EE-05 43 12.4 71 24.6 28 ## 2 GOVALLE EL &#39;EE-05 57 10.5 79 21.7 22 ## 3 WIDEN EL &#39;EE-05 70 11.9 102 22.9 32 ## 4 PEREZ EL &#39;EE-05 58 7.2 95 17.2 37 ## 5 WOOTEN EL &#39;EE-05 66 9.1 84 18.4 18 ## 6 ALLISON EL &#39;EE-06 41 8.4 88 17.5 47 ## 7 HART EL &#39;EE-05 49 6.9 104 15.7 55 ## 8 ORTEGA EL &#39;EE-05 30 9.2 50 17.8 20 ## 9 WOOLDRIDGE EL &#39;EE-05 34 6 79 14.6 45 ## 10 ZAVALA EL &#39;EE-06 61 15.3 59 23.8 -2 ## # … with 99 more rows, and 4 more variables: sped_c_prccng &lt;dbl&gt;, ## # sped_p_ppd &lt;dbl&gt;, sped15_thsh &lt;chr&gt;, sped20_thsh &lt;chr&gt; Brooke Elementary went from 12.4 percent to 24.6 percent, climbing from 43 to 71 students. It amounted to a 65 percent change in the share of students. But which number makes more sense for our story? Both are accurate. Ask yourself which one a reader could wrap their head around the easiest. I would say the percent change gives the reader a better picture and more properly describes the issue. 16.3 Aggregates with summarize() Our last question is: How many AISD schools would be above the special education “audit threshold” of 8.5% in 2020 if it were still in effect? How does those numbers compare to 2015? To answer this we need to introduce a new concept and function: summarize(), and it’s companion group_by(). The summarize() and summarise() functions compute tables about your data. They are the same function, as R supports both the American and European spelling of summarize. I don’t care which you use. Learn about your data with Summarize() Much like the mutate() function, we list the name of the new column first, then assign to it the function we want to accomplish using =. To demonstrate, let’s find the average number (or mean) of special education students in AISD schools in 2020. (You don’t need to do this, just see it.) aisd %&gt;% summarize( mean_sped_students = sped_c_20 %&gt;% mean() ) ## # A tibble: 1 × 1 ## mean_sped_students ## &lt;dbl&gt; ## 1 90.4 You might also see that calculation as: mean_sped_students = mean(sped_c_20). Both are correct. I flip back and forth, using tidyverse pipe method when I think it is more clear. Others may disagree. A mean (or average in common terms) is a way to use one number to represent a group of numbers. It works well when the variance in the numbers is not great. median() is another way, and sometimes better when there are high or low numbers that would unduly influence a mean. It’s the “middle” value. 16.4 Group_by() The summarize() function is an especially useful in combination with another function called group_by(), which allows us to categorize our data by common values before aggregating on thos values. You might review the Group &amp; Aggregate video that explains the concept. Group by This is easier to understand when you can see an example, so let’s do it. 16.4.1 Group and count If we want to know how many schools how many AISD schools were above the the 8.5% threshold in 2015 (our `sped_) aisd %&gt;% group_by(sped15_thsh) %&gt;% summarize(schools = n()) ## # A tibble: 2 × 2 ## sped15_thsh schools ## &lt;chr&gt; &lt;int&gt; ## 1 N 43 ## 2 Y 66 Let’s break this down: The group_by has organized (or “grouped”) all our data by the sped15_thsh value first (N vs Y) Then inside summarize() we counted the number n() of rows. We named that new column “schools,” because that is what we are really counting. 16.4.2 The count() function shortcut We count row A LOT in data science, so there is a convenience function to do this: count(). We used it earlier when we wanted to know our instruction types. Let’s rewrite this with count: aisd %&gt;% count(sped15_thsh) ## # A tibble: 2 × 2 ## sped15_thsh n ## &lt;chr&gt; &lt;int&gt; ## 1 N 43 ## 2 Y 66 Under the hood, count() is doing the same thing as the “group and summarize” function above it. 16.4.3 Comparing thresholds for 2015 and 2020 When we use count, we just feed it the columns that we want to group data by. And we can feed it more than one column. In our case, we want to know the number of schools for both 2015 and 2020, so let’s feed count with both columns: Add a Markdown header: # Comparing thresholds 2015 and 2020. Create a new chunk and name it thsh-compare. Add the following code: aisd %&gt;% count(sped15_thsh, sped20_thsh) ## # A tibble: 4 × 3 ## sped15_thsh sped20_thsh n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 N N 7 ## 2 N Y 36 ## 3 Y N 2 ## 4 Y Y 64 To break down our results here: Our first row (N and N) is schools below the threshold in both years: 7. The second ( N and Y) is schools that were below in 2015 but above in 2020: 36. The third (Y and N) is the schools that were above in 2015, but dropped in 2020: 2. The last (Y and Y) are the schools above in both years: 64. With this we can write a sentence describing how many schools climbed above 8.5% (36) and how many dropped (just 2). Note the count column has a header of “n,” for the number of rows. We can make this a little prettier by renaming that column. Modify your code chunk to add the rename() function below: aisd %&gt;% count(sped15_thsh, sped20_thsh) %&gt;% rename(schools = n) ## # A tibble: 4 × 3 ## sped15_thsh sped20_thsh schools ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 N N 7 ## 2 N Y 36 ## 3 Y N 2 ## 4 Y Y 64 16.5 More on summarize Let’s review the group_by and summarize combo: We start with our data frame. We then group_by the data by column or columns we want to consider. If we printed the data frame at this point, we won’t really see a difference. The group_by() function always needs another function to see a result. We then summarize the grouped data. Common summaries are to count rows n(), or do match on the results in a column like mean() (the average), median() (middle value) or sum() (adds together). If we want to name our new column, we do so first: new_name = mean(col_name). We can do multiple calculations within the summarize() function, like this: Group and summarize 16.5.1 Ignoring NA values Sometimes you’ll try to calculate a mean() with summarize and get an error because there are blank values. We don’t have an example in our special education data, but consider this data set of water wells, trying to find the average depth of the well: borehold_depth. Attempt to find the mean We get an error because some rows of the data don’t have a value for borehole_depth. They are NA values, or Not Available. We can apply a function inside summarize() called na.rm, which “removes” NA values before doing the summary. Like this: NAs removed from summarize 16.6 Review what we learned In this lesson used arrange() to sort our data, summarize() and the shortcut count() to find aggregates, like the number of rows based on values within the data. We’ll do lots more of this in lessons to come. Next we’ll practice all the things we’ve learned with a new data set. "],["how-to-tackle-a-new-dataset.html", "Chapter 17 How to tackle a new dataset 17.1 NEEDS REVIEW 17.2 Start by listing questions 17.3 Understand your data 17.4 Counting and aggregation 17.5 Cleaning up categorical data 17.6 Time as a variable 17.7 Explore the distributions in your data", " Chapter 17 How to tackle a new dataset 17.1 NEEDS REVIEW For those unfamiliar with exploring data, starting the process can be paralyzing. How do I explore when I don’t know what I’m looking for? Where do I start? Every situation is different, but there are some common techniques and some common sense that you can bring to every project. 17.2 Start by listing questions It’s likely you’ve acquired data because you needed it to add context to a story or situation. Spend a little time at the beginning brainstorming as list of questions you want to answer. (You might ask a colleague to participate: the act of describing the data set will reveal questions for both of you.) I like to start my RNotebook with this list. 17.3 Understand your data Before you start working on your data, make sure you understand what all the columns and values mean. Look at your data dictionary, or talk to the data owner to make sure you understand what you are working with. To get a quick summary of all the values, you can use a function called summary() to give you some basic stats for all your data. Here is an example from the Top 100 Billboard data we used in a class assignment. Summary of billboard data A summary() will show you the data type for each column, and then for number values it will show you the min, max, median, mean and other stats. 17.4 Counting and aggregation A large part of data analysis is counting and sorting, or filtering and then counting and sorting. It’s possible you may need to reshape your data using gather() or spread() before you can do the mutating or grouping and summarizing you need. Review the Tidy data chapter for more on that. 17.4.1 Counting rows based on a column If you are just counting the number of rows based on the values within a column (or columns), then count() is the key. When you use count() like this, a new column called n is created to hold the count of the rows. You can then use arrange() to sort the n column. (I’ll often rename n to something more useful. If you do, make sure you arrange() by the new name.) In this example, we are counting the number of rows for each princess in our survey data, the arranging by 'n then in decending order. survey %&gt;% count(princess) %&gt;% arrange(n %&gt;% desc()) princess n Mulan 14 Rapunzel (Tangled) 7 Jasmine (Aladdin) 6 Ariel (Little Mermaid) 5 Tiana (Princess and the Frog) 2 Aurora (Sleeping Beauty) 1 Belle (Beauty and the Beast) 1 Merida (Brave) 1 Snow White 1 17.4.2 Sum, mean and other aggregations If you want to aggregate values in a column, like adding together values, or to find a mean or median, then you will want to use group_by() on your columns of interest, then use summarize() to aggregate the data in the manner you choose, like sum(), mean() or the number of rows n(). Here is an example where we use group_by and summarize() to add together values in our mixed beverage data. In this case, we had multiple rows for each name/address group, but we wanted to add together total_receipts() for each group. receipts %&gt;% group_by(location_name, location_address) %&gt;% summarize( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) location_name location_address total_sales WLS BEVERAGE CO 110 E 2ND ST 35878211 RYAN SANDERS SPORTS 9201 CIRCUIT OF THE AMERICAS BLVD 20714630 W HOTEL AUSTIN 200 LAVACA ST 15435458 ROSE ROOM/ 77 DEGREE 11500 ROCK ROSE AVE 14726420 THE DOGWOOD DOMAIN 11420 ROCK ROSE AVE STE 700 14231072 The result will have all the columns you included in the group, plus the columns you create in your summarize statement. You can summarize more than one thing at a time, like the number of rows numb_rows = n() and average of the values average = mean(column_name). 17.4.3 Creating columns to show difference Sometimes you need to perform math on two columns to show the difference between them. Use mutate() to create the column and do the math. Here’s a pseudo-code example: new_or_reassigned_df &lt;- df %&gt;% mutate( new_col_name = (part_col / total_col) * 100 ) 17.5 Cleaning up categorical data If you are going to count our summarize rows based on categorical data, you might want to make sure the values in that column are clean and free of typos and values that might be better combined. In class we did this with the proposed_use column in our wells data in the Cleaning chapter. Review that chapter for examples. Some strategies you might use: Create a count() of the column to show all the different values and how often they show up. You might want to use mutate() to create a new column and then update the values there. Again, see the Cleaning chapter for examples. If you find you have hundreds of values to clean, then come see me. There are some other tools like OpenRefine that you can learn farily quickly to help. 17.6 Time as a variable If you have dates in your data, then you almost always want to see change over time for different variables. Summarize records by year or month as appropriate and create a Bar or Column chart to show how the number of records for each time period. Do you need to see how different categories of data have changed over time? Consider a line chart that shows those categories in different colors. If you have the same value for different time periods, do might want to see the change or percent change in those values. You can create a new column using mutate() to do the math and show the difference. Do you need the mean (average), median or sum of a column, or certain values within columns? The the group_by() and summarize() functions are likely your tool to discover those values. 17.7 Explore the distributions in your data We didn’t talk about histograms in class, but sometimes you might want see the “distributon” of values in your data, i.e. how the values vary within the column. Are many of the values similar? A histogram can show this. Here is an example of a histogram from our wells data exploring the borehole_depth. Each bar represents the number of wells broken down in 100ft depth increments (set with binwidth=100). So the first bar shows that most of the wells (more than 7000) are less than 100 feet deep. wells %&gt;% ggplot(aes(x = borehole_depth)) + geom_histogram(binwidth = 100) Borehole depth histogram While there are wells deeper than 1000 feet, they are so few they don’t even show on the graphic. You’ll rarely use a histogram as a graphic with a story because they are more difficult to explain to readers. But they do help you to understand how much values differ within a column. 17.7.1 More on histograms If you google around, you might see other ways to create a histogram, including hist() and qplot(). You might stick with the ggplot’s geom_histogram() since you already are familiar with the syntax. Tutorial on histograms using ggplot from DataCamp. R Cookbook on histograms. "],["verbs.html", "Chapter 18 Verbs 18.1 NEEDS REVIEW 18.2 Import/Export 18.3 Data manipulation 18.4 Aggregation 18.5 Math", " Chapter 18 Verbs 18.1 NEEDS REVIEW fix lists An opinionated list of the most common Tidyverse and other R verbs used with data storytelling. 18.2 Import/Export read_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(\"path/to/file.csv\"). write_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(\"path/to/file.rds\"). readxl is a package we didn’t talk about, but it has read_excel() that allows you to import from an Excel file, including specified sheets. clean_names() from the library(janitor) package standardizes column names. 18.3 Data manipulation select() to select columns. Example: select(col01, col02) or select(-excluded_col). rename() to rename a column. Example: rename(new_name = old_name). filter() to filter rows of data. Example: filter(column_name == \"value\"). See Relational Operators like ==, &gt;, &gt;= etc. See Logical operators like &amp;, | etc. See is.na tests if a value is missing. distinct() will filter rows down to the unique values of the columns given. arrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc()) mutate() changes and existing column or creates a new one. Example: mutate(new_col = (col01 / col02)). gather() collapses columns into two, one a key and the other a value. Turns wide data into long. Example: gather(key = \"new_key_col_name\", value = \"new_val_col_name\", 3:5) will gather columns 3 through 5. Can also name columns to gather. spread() turns long data into wide by spreading into multiple columns based on as key. 18.4 Aggregation count() will count the number rows based on columns you feed it. group_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. Example: group_by(song, artist) %&gt;% summarize(weeks = n(), top_chart_position = min(peak_position)) 18.5 Math These are the function often used within summarize(): n() to count the number of rows. n_distinct() counts the unique values. sum() to add things together. mean() to get an average. median() to get the median. min() to get the smallest value. max() for the largest. +, -, *, / are math operators similar to a calculator. "],["chart-examples.html", "Chapter 19 Chart examples 19.1 NEEDS REVIEW 19.2 Bar charts 19.3 Column chart 19.4 A line chart 19.5 Scatterplot 19.6 Histogram 19.7 Titles, labels and other cleanup 19.8 Interactivity with plotly 19.9 Resources", " Chapter 19 Chart examples 19.1 NEEDS REVIEW Under construction This will be a annotated list of chart code and examples from this book. It does not include the construction of the data frame used in the plot. (I’ll try to go back and add links.) 19.2 Bar charts ggplot(wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.45) + labs(title = &quot;Wells by county&quot;, x = &quot;&quot;, y = &quot;Number of wells drilled&quot;) Wells by county with title 19.2.1 Explanaton This is a geom_bar() using stat=\"identity\". See below for a similar one using geom_col() which assumes the identity of the category. The geom_text() line adds the numbers to the top of the bars. vjust moves those number up vertically. The labs() add the title and modifies the x and y labels. The x value is set to blank because the county labels on the bar is enough. 19.3 Column chart This is the same as above, but using geom_col which inheriently understands the stat=\"identity\" problem. ggplot(wells_by_county, aes(x = county, y = wells_count)) + geom_col() + geom_text(aes(label=wells_count), vjust=-0.45) + labs(title = &quot;Wells by county&quot;, x = &quot;&quot;, y = &quot;Number of wells drilled&quot;) It looks the same as above, and is probably the better choice. 19.4 A line chart ggplot(wells_county_year, aes(x=year_drilled, y=wells_drilled)) + geom_line(aes(color=county)) + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) Wells drilled by county by year 19.4.1 Explanation A geom_line() needs either a group= or a color= to “split” the lines on a category across the graphic. This example does this by setting a specific aes() color value in the geom_line() call. In this example, the title and pretty x and y labels are added with labs() 19.5 Scatterplot ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + # added color aesthetic geom_smooth() MPG: disply vs mpg with class 19.5.1 Explanation This plot uses the mpg data ggplot. It is comparing displ (the size of an engine) to mpg, the miles per gallon of the car. The goem_point() adds a new aesthetic to color the dots based on another value in the data, the class. The geom_smooth() plot adds a line showing the average of the points at each position. This helps you determine if there is a relationshiop between the two variables. 19.6 Histogram A histogram is a distribution of a number in your data. It’s super useful in helping understand your data, but difficult to explain to readers. We’ll use the well’s data column borehole_depth to explore this. The question is this: Are most wells shallow or deep? We can explore this with the histogram. wells %&gt;% ggplot(aes(x = borehole_depth)) + geom_histogram(binwidth = 100) + labs(title = &quot;Depth of wells&quot;, subtitle = &quot;More than 7,500 of the 18,000 wells drilled since 2005 are less than 100-feet deep.&quot;, y = &quot;Wells drilled&quot;, x = &quot;Depth in 100ft increments&quot;) The histogram chart defaults to splitting into 30 even groups, but you can change that in a couple of ways. Using bin = 40 will set a different number of bins. Using binwidth = 100 will create bins based on your x value. That yields this chart: Histogram example You can add labels to note the number of records in each bin with the stat_bin() function, which is a bit different than other ggplot functions. Here’s an example in the last line of this plot: wells %&gt;% ggplot(aes(x = borehole_depth)) + geom_histogram(binwidth = 100) + labs(title = &quot;Depth of wells&quot;, subtitle = &quot;More than 7,500 of the 18,000 wells drilled since 2005 are less than 100-feet deep.&quot;, y = &quot;Wells drilled&quot;, x = &quot;Depth in 100ft increments&quot;) + stat_bin(binwidth = 100, geom=&quot;text&quot;, aes(label = ..count..), vjust=-.5) 19.7 Titles, labels and other cleanup Titles and labels are added as new layers on a graphic. These examples just show the line that adds the new layer. 19.7.1 Title and axis labels yourchartsofar + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) 19.7.2 Flip the graphic Sometime you want to turn a graphic 90 degrees to you can read long values, or make a bar chart horizontal instead of vertical. yourchartsofar + coord_flip() 19.7.3 Fix exponential numbers on an axis yourchartsofar + scale_y_continuous(labels=comma) You can do the same for scale_x_continuous() if needed. 19.8 Interactivity with plotly This requires a library in addition to the tidyverse called library(plotly). The idea is that you: create a ggplot graphic. save that entire graphic to a new R object. Call the ggplotly() function with that new variable. saved_plot &lt;- wells_county_year %&gt;% ggplot(aes(x = year_drilled, y = wells_drilled, color = county)) + geom_line() + geom_point() + labs(title = &quot;Wells drilled per county&quot; ) # shove into ggploty ggplotly(saved_plot) This screen shot below is not interactive, but you can see the hover tool tip displayed. Wells by county with ggplotly 19.9 Resources In addition to the examples below, you might also look at these resources: The R Graph Gallery R Graphics Cookbook Plotly R, especially the Plotly ggplot2 Library ggplot themeshttps://ggplot2.tidyverse.org/reference/ggtheme.html and R Graph theme list "],["publishing-your-findings.html", "Chapter 20 Publishing your findings 20.1 NEEDS REVIEW 20.2 Examples 20.3 How to publish your projects 20.4 Making branded graphics", " Chapter 20 Publishing your findings 20.1 NEEDS REVIEW A work in progress fix lists Exploring a data set in R is different than publishing your findings. When you are exploring, you will ask a lot of questions and create a lot of plots that will lead to nothing. That is OK. Keep them, and write notes to your future self why you think each plot is NOT interesting, so when you return to it later you’ll see you already studied that path. This is a document for your current and future self. But once you’ve done all your exploring and FOUND things – findings that become sentences in your story – you may want to (you should) create a new notebook that focuses on these findings and how they relate to the story. You can use this more formal R notebook as a way to explain to readers and others the specific ways you came to your conclusions. This is a document for the public, and should be written as such. Get editing help to make sure you are clear and concise in your writing. Some things to consider: Include links to the original data. Explain what it is and how it applies to your story. Include a link to the published story if you have it. Include data dictionaries or similar as files or links so others can see and use the same materials you used to understand the data. If that understanding came through interviews, explain that and include the sources when possible. If you are pulling from a dataset that will change over time, include the dates you pulled the data. Save a copy of the raw data you used in your final version of the story, if possible, and explain that the data may change as records are added to the original. If you cleaned or modified to the data, include those steps so they can be repeated. It is sometimes useful to split those steps into a separate notebook and export/import (as an .rds file) into subsequent notebooks. For each finding, use a headline and text to explain the code block that follows. After the code block, write out your finding and how it relates to your story. Unless there is a good reason not to, stick with the findings you actually used in your story. Don’t waste readers’ time going down paths that were not reported. 20.2 Examples Not all of these examples are in R. Some are done in Python, but the theory is the same. Buzzfeed lists all their data analysis in a special Github repo. Their computational journalists use both R and Python, depending on the author or project. The L.A. Times also publishes a list of their data analysis projects in their Github account. Most of their analysis is done in Python (usually in files that end with .ipynb) and the depth of the annotations vary. The Washington Post Investigative team has begun hosting analysis from their stories in their their Github repo. Statesman A Question of Restraint Demolition permits in Austin Baltimore Sun Data Desk Trend CT SRF Data, the data investigative unit of a Swiss TV/Radio network, has robust documentation of their R analysis for stories. 20.3 How to publish your projects Needs expounding: what is the HTML file Explain about the HTML file and how it differs from the .Rmd file. Explain Advantages to separate Github repos Use chunk options to hide output or code that is not relevant to the reader. 20.3.1 Using Github Pages Needs research and expounding If you are saving your projec to Github, you can set your RMarkdown documents to knit the HTML versions of your documents into a /docs/ folder. As such, you can use Github Pages to publish your docs folder. In the metadata for your RMarkdown document, include the output lines outlined below. --- title: &quot;R Notebook&quot; output: html_document: df_print: paged knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = &quot;docs&quot;) }) --- 20.4 Making branded graphics We haven’t explored many ways to change the theme or looks of our ggplot graphics, but here are some notes worth exploring. ggplot themes can be added with one line. Branding ggplot graphs Note this article about BBC using R, ggplot. BBC created the bblot package to set BBC default styles, and BBC R cookook as a collection of tips and tricks to build their styled graphics. It’s just an example of you can customize R graphics. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
