[
["index.html", "Reporting with Data in R About this class About the author Other resources", " Reporting with Data in R Christian McDonald 2019-04-22 About this class This collection of lessons is intended to support the class Reporting With Data, taught by me, Christian McDonald, at the School of Journalism, Moody College of Communication, University of Texas at Austin. I’m a strong proponent of Scripted Journalism, a method of committing data-centric journalism in a programmatic, repeatable and transparent way. There are a myriad of programming languages that further this, including Python (pandas and Jupyter) and JavaScript (Observable), but we’ll be using R, RMarkdown and RStudio. R is a super powerful, open-source programming language for data that is deep with features and an awesome community of users who build upon it. No matter the challenge before you in your data storytelling, there is probably a package available to help you solve that challenge. Probably more than one. There is always more than one way to do things in R. This course is an opinionated collection of lessons intended to teach students new to R and programming for the expressed act of committing journalism. As a beginner course, I strive to make it as simple as possible, which means I may not go into detail about alternative (and possibly better) ways to accomplish tasks. About the author I’m a career journalist who most recently served as Data and Projects Editor at the Austin American-Statesman before coming to the University of Texas at Austin full-time in Fall 2018. I’ve taught data-related course at UT since 2013. UT Github: utdata Github: critmcdonald Twitter: crit Email: christian.mcdonald@utexas.edu Other resources This text stands upon the shoulders of giants and by design does not cover all aspects of using R. Here are some other useful books, tutorials and sites dedicated to R. R Journalism Examples, a companion piece of sorts to this book with example code to accomplish specific tasks. It is a work-in-progress, and quite nascent at that. R for Data Science by Hadley Wickham and Garrett Grolemund. The Tidyverse site, which has tons of documentation and help. The RStudio Cheatsheets. R Graphics Cookbook The R Graph Gallery another place to see examples. R for Journalists site by Andrew Tran, a reporter at the Washington Post. A series of videos and tutorials on using R in a journalism setting. Practical R for Journalism by Sharon Machlis, an editor with PC World and related publications, she is a longtime proponent of using R in journalism. ## Resources This DataCamp tutorial on imports covers a ton of different data types and connections. "],
["install.html", "Chapter 1 Install Party 1.1 Installing R 1.2 Installing RStudio 1.3 Class project folder", " Chapter 1 Install Party Let’s get this party started. NOTE: R and RStudio are already installed on lab computers. 1.1 Installing R Our first task is to install the R programming language onto your computer. There are a number of “mirrors” which have the software. Go to the download site. Go down to USA and choose one of the links there. They should all work the same. Click on the link for your operating system. The following steps will differ slightly based on your operating system. For Macs, you want the “latest package” For Windows, you want the “base” package. You’ll need to decide whether you want the 32- or 64-bit version. (Unless you’ve got a pretty old system, chances are you’ll want 64-bit.) Here’s hoping it will be self explanatory after that. 1.2 Installing RStudio RStudio is an “integrated development environment” – or IDE – for programming in R. Basically, it’s the program you will use when doing work for this class. Go to https://www.rstudio.com and find the “Download RStudio” button. Find the “Free” versions and find the installer for your operating system and download it. Install it. Should be like installing any other program. 1.3 Class project folder To keep things consistent and help with troubleshooting, I’d like you to save your work in the same location all the time. On both Mac and Windows, every user has a “Documents” folder. Open that folder. (If you don’t know where it is, ask me to help you find it.) Create a new folder called “rwd”. Use all lowercase letters. When we create new “Projects”, I want you to always save them in the Documents/rwd folder. "],
["intro.html", "Chapter 2 Introduction to R 2.1 RStudio tour 2.2 Updating preferences 2.3 Starting a new Project 2.4 Using R Notebooks 2.5 Turning in our projects", " Chapter 2 Introduction to R 2.1 RStudio tour When you launch RStudio, you’ll get a screen that looks like this: Rstudio launch screen 2.2 Updating preferences There is a preference in RStudio that I would like you to change. By default, the program wants to save a the state of your work (all the variables and such) when you close a project, but that is not good practice. We’ll change that. Go to the RStudio menu and choose Preferences Under the General tab, uncheck the first four boxes. On the option “Save Workspace to .Rdata on exit”, change that to Never. Click OK to close the box. 2.3 Starting a new Project When we work in RStudio, we will create “Projects” to hold all the files related to one another. This sets the “working directory”, which is a sort of home base for the project. Click on the second button that has a green +R sign. That brings up a box to create the project with several options. You want New Directory (unless you already have a Project directory, which you don’t for this.) For Project Type, choose New Project. Next, for the Directory name, choose a new name for your project folder. For this project, use “firstname-first-project” but use YOUR firstname. I want you to be anal about naming your folders. It’s a good programming habit. Use lowercase characters. Don’t use spaces. Use dashes. For this class, start with your first name. Rstudio project name, directory When you hit Create Project, your RStudio window will refresh and you’ll see the yourfirstname-first-project.Rproj file in your Files list. 2.4 Using R Notebooks For this class, we will almost always use R Notebooks. This format allows us to write text in between our blocks of code. The text is written in a language called R Markdown, a juiced-up version of the common documentation syntax used by programmers, Markdown. It’s not hard to learn. Here’s a Markdown guide. 2.4.1 Create your first notebook Click on the button at the top-left of RStudio that has just the green + sign. Choose the item R Notebook. This will open a new file with some boilerplate R Markdown code. At the top between the --- marks, is the metadata. This is written using YAML, and what is inside are commands for the R Notebook. Don’t sweat the YAML syntax too much right now, as we won’t be editing it often. Next, you’ll see a couple of paragraphs of text that describes how to use an R Notebooks. It is written in R Markdown, and has some inline links and bold commands, which you will learn, Then you will see an R code chunk that looks like the figure below. R code chunk Let’s take a closer look at this: The three back tick characters ( found at the top left on your keyboard) followed by the {r} indicate that this is a chunk of R code. The last three back ticks say the code chunk is over. The {r} bit can have some parameters added to it. We’ll get into that later. The line plot(cars) is R programming code. We’ll see what those commands do in a bit. The green right-arrow to the far right is a play button to run the code that is inside the chunk. The green down-arrow and bar to the left of that runs all the code in the Notebook up to that point. 2.4.2 Save the .Rmd file Do command-s or hit the floppy disk icon to save the file. It will ask you what you want to name this file. Call it 01-first-file.Rmd. When you do this, you may see another new file created in your Files directory. It’s the pretty version of the notebook which we’ll see in a minute. In the metadata portion of the file, give your notebook a better title. Replace “R Notebook” in the title: &quot;R Notebook&quot; code to be “Christian’s first notebook”, but use your name. 2.4.3 Run the notebook There is only one chunk to run in this notebook, so: Click on the green right-arrow to run the code. You should get something like this: Cars plot What you’ve done here is create a plot chart of a piece of sample data that is already inside R. (FWIW, It is the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s.) But that wasn’t a whole lot of code to see there is a relationship with speed vs stopping distance, eh? 2.4.4 Adding new code chunks The text after the chart describes how to insert a new code chunk. Let’s do that. Add a return after the paragraph of text about code chunks, but before the next bit about previews. Use the keys Cmd+Option+I to add the chunk. Your cursor will be inserted into the middle of the chunk. Type in this code in the space provided: # update 52 to your age age = 52 (age - 7) * 2 Change for “52” to your real age. With your cursor somewhere in the code block, use the key command Cmd+Shift+Return, which is the key command to RUN ALL LINES of code chunk. NOTE: To run an individual line, use Cmd+Return while on that line. Congratulations! The answer given at the bottom of that code chunk is the socially-acceptable maximum age of anyone you should date. Throwing aside whether the formula is sound, let’s break down the code. # update 52 to your age is a comment. It’s a way to explain what is happening in the code without being considered part of the code. age = 52 is assigning a number (52) to a variable name (age). A variable is a placeholder. It can hold numbers, text or even groups of numbers. They are key to programming because they allow you to change the value of the variable as you go along. The next part is simple math: (age - 7) * 2 takes the value of age and subtracts 7, then multiplies by 2. When you run it, you get [1] 90. That means there was one observation, and the value was “90”. For the record, my wife is much younger than that. Now you can play with the age variable assignment to test out different ages. 2.4.5 Practice adding code chunks Now, on your own, add a similar code chunk that calculates the minimum age of someone you should date, but using the formula (age / 2) + 7. Add a comment in the code that explains what it is for. 2.4.6 Preview the report The rest of the boilerplate text here describes how you can Preview and Knit a notebook. Let’s do that now. Press Cmd+Shift+K to open a Preview. This will open a new window and show you the “pretty” notebook that we are building. Preview is a little different than Knit, which runs all the code, then creates the new knitted HTML document. It’s Knit to HMTL that you’ll want to do before turning in your assignments. 2.4.7 The toolbar One last thing to point out before we turn this in: The toolbar that runs across the top of the R Notebook file window. The image below explains some of the more useful tools, but you REALLY should learn and use keyboard commands when they are available. R Notebook toolbar 2.4.8 Knit the final workbook Save your File with Cmd+S. Use the Knit button in the toolbar to choose Knit to HTML. 2.5 Turning in our projects If you now look in your Files pane, you’ll see you have four files in our project. (Note the only one you actually edited was the .Rmd file.) Files list The best way to turn in all of those files into Canvas is to compress them into a single .zip file that you can upload to the assignment. In your computer’s Finder, open the Documents/rwd folder. Follow the directions for your operating system linked below to create a compressed version of your yourname-final-project folder. Compress files on a Mac. Compress flies on Windows. Upload the resulting .zip file to the assignment for this week in Canvas. Here is what the compression steps looks like on a Mac: Compress file: Mac If you find you make changes to your R files after you’ve zipped your folder, you’ll need to delete the zip file and do it again. Because we are building “repeatable” code, I’ll be able to download your .zip files, uncompress them, and the re-run them to get the same results. Well done! You’ve completed the first level and earned the Beginner badge. "],
["import.html", "Chapter 3 Importing data 3.1 Goals for this section 3.2 Data types 3.3 Create a new project 3.4 Let’s get some data 3.5 Import csv as data 3.6 Assign our data to a data frame 3.7 Inspect the data 3.8 Turn in your project 3.9 Practice assignment: Import census", " Chapter 3 Importing data 3.1 Goals for this section Learn a little about data types available to R. Practice organized project setup. Learn about R packages, how to install and import them. Learn how to import CSV files. Introduce the Data Frame/Tibble. We will do this through working with well drilling reports from the Texas Water Development board. We’ll pull reports from counties in the Austin MSA and be able to see the kinds of wells dug, where they are and the pace of drilling. 3.2 Data types After installing and launching RStudio, the next trick is to import data. Depending on the data source, this can be brilliantly easy or a pain in the rear. It all depends on how well-formatted is the data. In this class, we will primarily be importing Excel files, CSVs (Comma Separated Value) and APIs (Application Programming Interface). CSVs are a kind of lowest-common-denominator for data. Most any database or program can import or export them. Excel files are good, but are often messy because humans get involved. There are often multiple header rows, columns used in multiple ways, notes added, etc. Just know you might have to clean them up before using them. APIs are systems designed to respond to programming. In the data world, we often use the APIs by writing a query to ask a system to return a selection of data. By definition, the data is well structured. You can often determine the file type of the output as part of the API call, including … JSON (or JavaScript Object Notation) is the data format preferred by JavaScript. R can read it, too. It is often the output format of APIs, and prevalent enough that you need to understand how it works. We’ll get into it more later. Don’t get me wrong … there are plenty of other data types and connections available through R, but those are the ones we’ll deal with most in the class. 3.2.1 What is clean data The Checking Your Data section of this DataCamp tutorial has a good outline of what makes good data, but in general it should: Have a single header row with well-formed column names. Once column name for each column. No merged cells. Short names are better than long ones. Spaces in names make them harder to work with. Use and _ or . between words. Remove notes or comments from the files. Each column should have the same kind of data: numbers vs words, etc. Each row should be an In our first lesson, we’ll be using a CSV file that has several rows of description at the top. We’ll be able to skip those description lines when we import. 3.3 Create a new project We did this in our first lesson, but here are the basic steps: Launch RStudio Use the +R button to create a New Project in a New Directory Name the project yourfirstname-wells and put it in your ~/Documents/rwd folder. Use the + button to use R Notebook to start a new notebook. Change the title to “Wells drilled in Austin MSA”. Delete the other boilerplate text. Save the file as 01-wells-import.Rmd. 3.3.1 The R Package environment We have to back up from the step-by-step nature of this lesson and talk a little about the R programming language. R is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful. What happens is developers will find it difficult to do a certain task, so they will write an R “Package” of code that helps them with that task. They share that code with the community, and suddenly the R garage has an “ultimate set of tools” that would make Spicoli’s dad proud. One set of these tools is Hadley Wickham’s Tidyverse, a set of packages for data science. These are the tools we will use most in this course. While not required reading, I highly recommend Wickham’s book R for data science, which is free. We’ll use some of Wickham’s lectures in the course. There are also a series of useful cheatsheets that can help you as you use the packages and functions from the tidyverse. We’ll refer to these throughout the course. 3.3.2 Installing and using packages There are two steps to using an R package: Install the package using install.packages(&quot;package_name&quot;). You only have to do this once for each computer, so I usually do it using the R Console instead of a script. Include the library using library(package_name). This has to be done for each Notebook or script that uses it, so it is usually one of the first things in the notebook. We’re going to install several packages we will use in the Wells project. To do this, we are going to use the Console, which we haven’t talked about much. The Console and Terminal Use the image above to orient yourself to the R Console and Terminal. In the console, type in install.packages(&quot;tidyverse&quot;) and hit return. You’ll see a bunch of commands work through your Console. Remember that you only have to install a package to your computer once. We’ll need another package, so also do: install.packages(&quot;janitor&quot;) We’ll use some commands from janitor to clean up our data column names, among other things. A good reference to learn more is the janitor vignette. I start every data project with these two packages. 3.3.3 Load the libraries Next, we’re going to tell our R Notebook to use these two libraries. After the metadata at the top of your notebook, use Cmd+option+i to insert an R code chunk. In that chunk, type in the two libraries and run the code block with Cmd+shift+Return. It will look like this: Libraries imported 3.4 Let’s get some data I could’ve supplied you with the raw data, but it is not hard to find and grab yourself, so let’s do that. Because I want you to teach you good data project skills, I want you first to make a folder to store your raw data. It’s good practice to separate your raw data from any other output or data. It will make it easy for others to find, and can help you avoid overwriting that raw data, which should remain a pristine copy. In the Files pane, use the New Folder button to create a folder called data-raw. (I typically make a data-out or similar folder for any files that I create.) In a web browser, go to the Texas Water Development Board Driller Reports page and then click on the Well Reports Search by County and Use link. In the County drop down, choose: Bastrop, Caldwell, Hays, Travis and Williamson counties. In the Proposed Use column, choose Select All. Click View Reports. You’ll get 400+ returns. Look for the floppy disk/arrow icon that is the download button. Choose CSV (comma delimited). That file should end up in your Downloads folder. Use your finder to move this into your project folder in the data-raw folder you created. This document has some descriptions of fields we may need later. 3.4.1 Inspect the data We want to look at the data so we understand it. In the Files pane, click on the data-raw folder to open in. Click on the WellsRpts_County_Use.csv file until you get the drop down that says View Files. View file When you choose that, you’ll get a warning that it is a big file. It should open it just fine, into a new window. It will look like this: Wells file The numbers on the left are row numbers in the file. Because lines will wrap in your window, those numbers let you know where each line starts. Note that the real header for this file starts as line 5, which means when we import this file, we need to skip the first four lines. You can close this file now. 3.5 Import csv as data Now we need to start adding some text to indicate what we are doing, which right now we are importing the file. So, write some text in Markdown that describes where the data came from and what it is. Write where the data came from. Include the link to the web page. Include which counties were included, and which After your descriptions, add a new code chunk (Cmd+option+i). Inside the chunk, add the following and hit return, then I’ll explain: read_csv(&quot;data-raw/WellRpts_County_Use.csv&quot;) read_csv() is the function we are using the load the data. This version from the readr package in the tidyverse is different from read.csv that comes with R. It is mo betta. Inside the parenthesis is the path to our data, inside quotes. If you start typing in that path and hit tab, it will complete the path. (Easier to show than explain). This prints two things to our notebook, which are shown as tabs. The first resultc called “R Console” shows what columns were imported and the data types. It’s important to review these to make sure things happened the way that you want. When I look at this, I’m struck that the column names all start with “Textbox”, which wasn’t what I expected when I was looking at the data on the website. (FWIW, the fact the text is in red is NOT an indication of a problem.) Show cols The second result prints out the data like a table. The data object is called a Tibble, which is a fancy version of a data frame that is part of the tidyverse. I will often call a tibble a “data frame”, which is the generic R from of this data structure. Think of data frames and tibbles like a well-structured table in a spreadsheet. They are organized rows of data with columns where every item in the column is of the same data type. Show imported data What went wrong? Remember that our data doesn’t really start until line five. We need to modify our import to skip the first for lines. But how does we find out how to do that? Help is on the way!! 3.5.1 Help files Another tab over by your Files pane is the Help pane. Click on the Help pane In the search box, type in read_csv and hit Return. What you get in return is information about that function. Any function loaded into RStudio also comes with these help files. The documentation style might look foreign at first, but you’ll get used to reading them. If we look through this one, we can see there is a skip = x option we can add to our import to skip lines. Modify the import line to this and then rerun the entire chunk with Cmd+shift+Return: read_csv(&quot;data-raw/WellRpts_County_Use.csv&quot;, skip = 3) I first tried skip = 4, but then it didn’t properly use the header row, I think because the readr package skips empty rows by default. 3.6 Assign our data to a data frame As of right now, we’ve only printed the data to our screen. We haven’t “saved” it at all. What we need to do next is “assign” it to a data frame. It’s kind of weird, but the convention in R is to work from right to left. We name things before we fill them with stuff. So, to create a data frame, the structure is this: new_data_frame &lt;- stuff_going_into_new_data_frame We have our stuff as the output of our read_csv() function … now we need to assign it to a data frame we will call wells. Edit your existing code chunk to look like this: wells &lt;- read_csv(&quot;data-raw/WellRpts_County_Use.csv&quot;, skip = 3) Run that chunk and two things happen: We no longer see the result printed to the screen. That’s because we created a data frame instead. In the Environment tab at the top-right of RStudio, you’ll see wells listed. Click on the blue play button next to wells and it will expand to show you a summary of the columns. Click on the name and it will open a “View” of the data in another window, so you can look at it, sort of like a spreadsheet. 3.7 Inspect the data Now that you are looking at the data, take a look at both the data types in the Environment tab, along with the View of the data. Take special care to look at the data types and examples to see if they make sense to you. Some things to consider: Are numbers actually numbers or characters? Are there numbers that should be strings, like ZIP codes? Have integers been imported as double numbers or vice versa? Are dates imported properly? 3.7.1 Write notes about things to change In text below the table output, I want you to write out a list of all the things you might have to fix in this data frame. We’ll fix them in the next lesson, as well as start looking more closely at the data. 3.8 Turn in your project Congratulations! You have created a new project in R and imported data. That is a feat of skill worth celebrating, so we will turn in this in as an assignment. Save your .Rmd file. Use the Preview/Knit button to Knit your report to HTML. Look your report over and make sure you like it. If you need to, edit your .Rmd file, save, reKnit. When you are ready, go under the File menu to Close project. Go into your computer’s finder and locate your firstnanme-wells project. Create a .zip file of the folder. Upload it to the proper assignment in Canvas. 3.9 Practice assignment: Import census To practice these skills on your own, you’ll create a new project and use new data. You’ll work on it through multiple lessons, applying what you’ve learned along the way. Create a new Project called “firstname-census-practice”. You’ll want to save that inside your “rwd” folder so you can use it later. We’ll keep building on it. Create a new folder in your project called “data-raw”. Download this CSV file and put it into your data-raw folder: DEC_10_SF1_TX_County_population.csv. The data is 2010 Census populations by county and race for Texas. Start a new R Notebook with a good title and filename. Write text to describe the data set. Import the data using read_csv() and print the data to the screen. Compare the imported data to the original csv file and note any problems you might see with the column names or data types that you might want to fix. Save, Knit, and Zip the project folder and upload to the “Practice: Import” assignment. "],
["columns.html", "Chapter 4 Columns 4.1 Goals for this section 4.2 Relaunch the Wells project 4.3 Clean up column names 4.4 Clean data within columns 4.5 Export the data 4.6 Turn in wells 4.7 Practice assignment: Clean census names", " Chapter 4 Columns I’m a bit anal about cleaning up column names in my data frames, because it makes them easier to work with later. As such, I’m going to show you three different ways to clean or edit column names. 4.1 Goals for this section Use the janitor plugin to clean columns names Mass rename columns with a pattern match Rename individual columns Fix data columns and other data types 4.2 Relaunch the Wells project Launch RStudio. It will probably open to the last project you worked on. Open your Wells project. There are several ways you can accomplish this: If you’ve had the project open before, you can use the drop down in the top-right of RStudio to see a list of recent projects, and choose it from there. Or, under the File menu to Recent projects and choose it. Or, under File you can use Open Project… and go to that folder and choose it. Look in the Files pane to the right and fine your 01-wells-import.Rmd file and open it. Use the Run button in the R Notebook toolbar to Run All of the chunks, which will load all your data and load the data frame from our last assignment. However you do it, make real sure you are in your wells project and that you are in your 01-wells-import.Rmd file. 4.3 Clean up column names At the end of our last lesson, we printed the wells data frame to the screen and clicked through the columns to look at the data. I like my column names to be standardized: Spaces in names make them harder to work with. Use and _ or . between words. Short names are better than long ones. lower_case is nice. 4.3.1 Clean names with janitor I use function from the “janitor” package called clean_names that will standardize column names. Often, this is all I need to do. After your list of things to fix, write a Markdown headline ## Clean column names. Using the ## makes this a smaller headline than the title. (The more #’s the smaller the headline.) The idea is to use these to organize your code and thoughts. Explain in text that we’ll use janitor to clean the column names. Insert a new code chunk (Cmd+shift+i should be second nature by now.) Insert the name of your wells data frame and run it to inspect the column names again. These are not too bad, but they are a mix of upper and lowercase names, and some of them are rather long. We’ll try the janitor clean_names function first. In a new code chunk, start with the wells data frame. wells %&gt;% clean_names() And you’ll get a result like this: Columns cleaned with janitor 4.3.1.1 About the pipe Let’s take take a moment to talk about this %&gt;% text you see there. (It’s called a pipe for dumb reasons I don’t want to get into). Think of it as the “And THEN do THIS” key. It takes the result of what is on the left, and allows you to then THEN do the thing on the right. We will use the %&gt;% pipe command a lot, so it is worth knowing that the keyboard command Cmd+shift+m will give you that string of characters. I did NOT invent this keyboard command, but you might remember that Professor McDonald taught it to you. It will serve you well. Remember: Think of the %&gt;% command as “Then”. So, we first have the wells data frame, THEN we are cleaning the names. 4.3.1.2 Assign clean names back to wells Now, we haven’t actually changed the names for realz, we just printed the data frame to our screen with those new names. We have to assign those changes somewhere to keep them. We could create a new data frame, but in this case, we’ll just replace our current wells with our new wells, similar to how we filled wells the first time with our raw data. Like this: wells &lt;- wells %&gt;% clean_names() The result no longer prints to our screen, but we can look at our Environment tab and check on our results, if we want. This is a start, but we still have some problems: We have some long names, like “well_report_tracking_number2”. We have an annoying trailing “2” at the end of all the column names. 4.3.2 Simple renaming of individual columns Renaming individual columns is pretty simple, and is another example of the “piping” concept of data %&gt;% do_something(). In this case, we’ll be useing the rename() function, which works like this: rename(new_column_name = old_column_name)` The = assignment works from-right-to-left, just of like &lt;- does when we put data into a data frame. So, let’s apply this to our wells data frame: Add Markdown text that says we are going to rename two columns: well_report_tracking_number2 and plugging_report_tracking_number2. Add a new code chunk and print the wells data frame. Now go back and add a %&gt;%pipe and the rename function, like the following: wells %&gt;% rename(well_number = well_report_tracking_number2) Do Cmd+Return to run that line, and you’ll see the first column name has changed. Note that is still ONE line of code, even though it is written in multiple lines. We can rename more than one column at at time if we separate the assignments with commas. Now, let’s edit this to also change the plugging_report_tracking_number2 column, which is the last column of the data. In the same code chunk, add a comma and a return before the ending ). add the new column mapping, like this: wells %&gt;% rename(well_number = well_report_tracking_number, plug_number = plugging_report_tracking_number) Note the indents in the code there. RStudio probably indented it properly for you, but it’s done that way so you can visually see that these two lines are are related. We could do all the column names that way to remove the “2”, but I want to show you a different way below to change them all at once. 4.3.2.1 Assign renamed columns back to wells Now, again, we have printed these column name chages to our screen, but we have not yet saved them back to the wells data frame. We need to assign it back to wells. Add wells &lt;- to the beginning of the code chunk so what had printed to the screen is now instead being pushed into the wells data frame. wells &lt;- wells %&gt;% rename(well_number = well_report_tracking_number2, plug_number = plugging_report_tracking_number2) NOTE: An important concept: Now that you have permanently changed the wells data frame, you can’t run that same code chunk again or it will give you an error. Why? because “well_report_tracking_number2” doesn’t exist anymore, so R can’t find it to change it! If you need to re-run that chunk, you need to first re-run all the code above it, either by using the down-arrow play button on the chunk or going up to the Run menu and chosing “Run All Chunks Above”. 4.3.3 Mass renaming of columns While we could individually rename all the columns to remove the trailing “2” on all the names, there is a way to do it all at once. It’s the last way we’ll discuss how to change column names, as I’ve already broken my rule of showing you only one way to do something. This is worth it because it saves time and introduces a couple of useful concepts. We can access all the column names of a data frame with a generic R function called names(), and we can use a pattern matching replacement called str_replace() to change them. Write in text that we are going to change the names of all the columns to remove the “2”. Create a new code block and insert this: names(wells) Run that, and you get a return like this, which is a list of all the column names of your data frame. [1] &quot;well_number&quot; &quot;type_of_work2&quot; &quot;proposed_use2&quot; &quot;owner_name2&quot; [5] &quot;county2&quot; &quot;well_address2&quot; &quot;coord_dd_lat2&quot; &quot;coord_dd_long2&quot; [9] &quot;grid_number2&quot; &quot;drilling_start_date2&quot; &quot;drilling_end_date2&quot; &quot;borehole_depth2&quot; [13] &quot;driller_signed2&quot; &quot;driller_company&quot; &quot;license_number2&quot; &quot;plug_number&quot; Cool, we can now use the %&gt;% pipe to “THEN” perform actions on those names, just like we did on the data frame above. The next function we’ll use to do that is str_replace() which allows us to search and replace strings. It works like this: `str_replace(data, &quot;search_pattern&quot;, &quot;replacement_text&quot;)` In our case, the “data” will be passed into it with the pipe. We want to replace “2” with nothing, so we’ll do this: names(wells) %&gt;% str_replace(&quot;2&quot;, &quot;&quot;) Let’s break that down again: We start with names(wells) which gives us a list of our column “names”. We use the %&gt;% to THEN apply a new function called str_replace(), which allows use to replace strings of text. Our data has been passed in by the pipe, so our our next argument is to search for the character &quot;2&quot; in all the names. And we replace those 2s with and empty string &quot;&quot;, which essentially deletes them. 4.3.3.1 Assign the replaced column names back into names(wells) Like our other examples, we have to now save our changes, but this one is a little different. Since we are only working with the names of the data with names(wells), we need to assign the back the same way, into names(wells). Add names(wells) &lt;- to the first line of the code chunk so we assign the names back. names(wells) &lt;- names(wells) %&gt;% str_replace(&quot;2&quot;,&quot;&quot;) 4.4 Clean data within columns Now that we’ve cleaned up our column names, our next task is to clean up some of the data itself. 4.4.1 Using lubridate to clean dates Fixing dates in generic R can be a semi-complicated process. Luckily, the tidyverse package lubridate makes date conversions simple. The package was included when we installed the tidyverse package, but we need to add the library. Go back to the top of your R Notebook where the libraries are loaded, and add this line and run it: library(lubridate). Return back to the bottom of the Notebook and add in Markdown a headline and text describing that you will use lubridate to convert the date fields. Insert a new code chunk and add and run this, then I’ll explain it: wells %&gt;% mutate(drilling_start_date = mdy(drilling_start_date)) Go head and click over to the drilling_start_date column so you can see the converted date. We started with the wells. We then piped the results into mutate(), which “mutates” data within columns. Mutate is not just for dates and we will use it to change and create all kinds of changes in the future. The first argument of mutate is the name of the new column. In this case, we are changing the existing column, so we are using drilling_start_date. = is the assignment operators. What is on the right will be put into the left. mdy(drilling_start_date) is the lubridate function. We are telling lubridate that the format of the existing text field is in Month/Day/Year format. Lubridate is smart enough to realize the / separates the dates, and it would also understand if the separators were - or .. That’s kind of weird. We are telling lubridate that we are starting with mdy so it will convert and show it back to us in yyyy-mm-dd, which is standard database data format. 4.4.1.1 Your turn It’s time for you to use some of the skills you’ve learned already to accomplish a couple of easy tasks: Update the mutate() function above to also update drilling_end_date field to a date. Hint: mutate is a tidyverse function just like rename, so it works similarly. Assign the changes you’ve made back to the wells data frame and then reprint it to make sure it’s all good. 4.4.2 Fix the bore hole depth If you look at the CSV data, the borehole_depth is and integer (a number without a decimal point), but it was imported as a &lt;dbl&gt; number with decimals. This could cause us problems later if we wanted to math on these, so we’ll convert this to an integer using mutate(). Add a Markdown headline and description to describe our actions. Add a code chunk and add the following and run it: wells %&gt;% mutate(borehole_depth = as.integer(borehole_depth)) This will reassign that column as an integer. How did I know to use as.integer? I Googled “r convert float to integer” and found this tutorial and this Stack Overflow article. A side note about this: I didn’t realize this might be a problem until a later lesson. If I found a problem like this in Excel, I would have to redo all my steps, but since I’m using a script, I was able to make this change and then rerun the notebook. As a last step, we have to reassign our mutated data frame back to wells, so change the first line to wells &lt;- wells %&gt;%. 4.5 Export the data It’s not a bad idea to organize a project into multiple R Notebooks. I’ll often create my first notebook to complete the tasks of downloading and cleaning up data, and then create a new one to handle analysis, etc. (This is why I had you name the files 01-wells.Rmd.) It’s possible to output the data frame you have created with all the changes and datatypes into a special .rds format that will re-import into R in exactly the same form. We’ll do that now. Create a new Markdown header and text description to explain that you are exporting the data. Use the Files pane to create a New Folder called data-out. (If the folder doesn’t exist already, you’ll get an error trying save the file.) Create a new code chunk and add the following and run it: wells %&gt;% saveRDS(&quot;data-out/wells_01.rds&quot;) To break that down: We are staring with wells We are piping that into the saveRDS() function. We are giving saveRDS() the path to save the file. DO NOT update this to assign back into the wells data frame. We don’t have to do that here since our output is an external file. DO use your Files pane to make sure it worked and you actually saved out the file. 4.6 Turn in wells Congratulations! You finished this chapter, having renamed columns and changed data types. Depending on where we are in the week, you may be asked to turn this in at this stage. In any event, you should save and Knit your files. 4.7 Practice assignment: Clean census names You will start with the “census-practice”&quot; project that you started in the previous chapter, so the first step is to open that project in RStudio. The goal here is to rename the columns in the data to shorter, more-friendly names, such as “black” instead of “Not Hispanic - Black alone”, or “american_indian” instead of “Not Hispanic - American Indian alone”. Use clean_names() to standardize them. Use either rename() or str_replace() on names() to rename the columns. It doesn’t matter to me how what method you use to change them, as long as it gets done. Make sure each step is documented in Markdown with good headlines and descriptions. Save the resulting data out as an “.rds” file into a data-out folder. Save, Knit, Zip and submit your project folder to the “Practice: Columns” assignment. "],
["transform.html", "Chapter 5 Transform 5.1 Goals for the section 5.2 Introducing dplyr 5.3 Start a new R Notebook 5.4 Record our goals 5.5 Name your code chunks 5.6 Import our data 5.7 Filter() 5.8 Combining filters 5.9 Arrange() 5.10 Multi-step operations 5.11 Select() 5.12 Mutate() 5.13 Summarize() 5.14 Group_by() 5.15 Transform review 5.16 Turn in your in-class project 5.17 Practice assignment: Transforms on census", " Chapter 5 Transform 5.1 Goals for the section Pay more attention to the Markdown to record our goals, actions and explain our code. We’ll name our code chunks, too. Use the dplyr tools to filter, sort and create new columns of data. 5.2 Introducing dplyr One of the packages within the tidyverse is dplyr ( cheatsheet ) which allows us to transform our data frames in ways that let us explore the data and prepare it for visualizing. It’s the R equivalent of common Excel functions like sort, filter and pivoting. dplyr functions (Some slides included here are used with permission from Hadley and Charlotte Wickham.) 5.3 Start a new R Notebook As I explained at the end of our last lesson, it’s a good practice to separate your import/cleaning functions from your analysis functions into separate notebooks, so we’ll create a new one for our analysis. Launch RStudio and open your wells project. Create a new R Notebook and set a new title of “Wells exploration and analysis”. Remove the boilerplate language and add a description of our goals: To explore an analyze our wells project. Mention that you have to run the other notebook first in case your someone else (or your future self) comes here first. Save your file as 02-wells-explore.Rmd. 5.4 Record our goals What do we want to learn about these wells? Look over the columns and some of the values in them and come up with a list of at least five things you might want to learn from the data. Add a Markdown headline ## Goals. Create a bullet list of things you might want to find. Use a * or - to start each new line. We’ll review some of your ideas in class. 5.5 Name your code chunks By adding the word setup after our the {r} at the beginning, then we can find that chunk in our navigation drop down at the bottom of the R Notebook window. R Notebook navigation 5.6 Import our data Add a Markdown headline and description that you are loading the data. Add a code chunk named import with the following: Wells data imported Now we are back to where we ended with the first notebook. 5.7 Filter() We can use dplyr’s filter() function to capture a subset of the data, like all the wells in Travis County. It works like this: dplyr filter function Let’s filter our wells data to just those in Travis County. We are going to use the %&gt;% pipe function to do this to the wells data frame. Travis wells When you run this, you’ll see that you the about 9000 rows instead of the 18,000+ of the full data set. Note the two equals signs there ==. It’s important two use two of them, as a single = will not work, as that means something else. There are a number of these logical test operations: dplyr logical tests 5.7.1 Filter your turn Create new code blocks and filter for each of the following: Wells with a proposed use of Irrigation. Wells at least 1000 feet deep. (HINT: If you are filtering on a number, don’t put it in quotes, or it will become text instead.) One more that might help you answer one of your goals you listed above. 5.7.2 Common mistakes with filter Some common mistakes that happen when using filter. 5.7.2.1 Use two == signs for “true” DON’T DO THIS: wells %&gt;% filter(county = &quot;Travis&quot;) DO THIS: wells %&gt;% filter(county == &quot;Travis&quot;) 5.7.2.2 Forgetting quotes DON’T DO THIS: wells %&gt;% filter(county == Bastrop) DO THIS: wells %&gt;% filter(county == &quot;Bastrop&quot;) 5.8 Combining filters You can filter for more than one thing at a time by separating more than one test with a comma. wells %&gt;% filter(county == &quot;Travis&quot;, proposed_use == &quot;Irrigation&quot;) If you use a comma to separate tests, then both tests have to be true. If you want OR, then you use a pipe | (the shift-key above the backslash.) Boolean operators 5.8.1 Your turn combining filters Your quest is to filter to wells in Travis or Williamson counties that have a start date in 2018. BIG HINT: If you include library(lubridate) in your notebook then you can access the year of a field with year(name_of_date_field). 5.8.2 Common mistakes with combining filters Some things to watch when trying to combine filters. 5.8.2.1 Collapsing multiple tests into one DON’T DO THIS: wells %&gt;% filter(county == &quot;Travis&quot; | &quot;Williamson&quot;) DO THIS: well %&gt;% filter(county == &quot;Travis&quot; | county == &quot;Williamson&quot;) BUT EVEN BETTER: wells %&gt;% filter(county %in% c(&quot;Travis&quot;, &quot;Williamson&quot;)) If you want to combine a series of strings in your filter, you have to put them inside a “concatenate” function, which is shortened to c(), as in the example above. 5.9 Arrange() The arrange() function sorts data. dataframe %&gt;% arrange(column) Or, to arrange in descending order (biggest on top): dataframe %&gt;% arrange(desc(column)) So, let’s sort our data by the borehole depth: wells %&gt;% arrange(borehole_depth) You’ll have to scroll the columns over to see it, and the depths start at zero, which is not very sexy. As journalists, we usually want to see the largest (or deepest) thing, so we can arrange the column in descending order with this: Deepest wells Now we see some deep wells … 3300 feet when I pulled my test data. 5.10 Multi-step operations But what if you want to both filter and arrange? It is possible to chain piped opperations together. Let’s find the deepest well in Travis County: wells %&gt;% filter(county == &quot;Travis&quot;) %&gt;% arrange(desc(borehole_depth)) 5.10.1 Your turn to combine and pipe Find a list of the deepest irrigation wells in Travis County in 2017. Use the pipe to string together your functions. 5.11 Select() As we’ve worked with borehole_depth it’s been kind of a pain to tab through all the fields to see the result we want. The select() function allows you to choose which fields to display from a data frame. If we are only interested in the owner and borehole_depth from our previous query of deepest wells in Travis, then we we can pipe the results to a select function. It works by listing the column names inside the function. You can use - before a column name to remove it. One the end of your previous code chunk, add a pipe, then select(owner_name, borehole_depth): wells %&gt;% filter(county == &quot;Travis&quot;) %&gt;% arrange(desc(borehole_depth)) %&gt;% select(owner_name, borehole_depth) The order of all these operations matter. If you use select() that removes a column, you cannot later use filter on that removed column. 5.12 Mutate() We used the mutate() function with our data cleaning, but let’s dive more into it. mutate() allows us to change data based on a formula. We can assign the change back to an existing column or create a new one. Create columns with mutate() In the example above: gapminder is the source data frame. gdp is the new column being created. it comes first. = gdpPercap * pop is the function. It is multiplying the the two columns that are in the gapminder data frame. The applied function doesn’t have to be math. It could be pulling part of a string or any number of things. We’ll use this to create a new “year_drilled” column that has just the year that well was started. It will help us plot data later. We’re going to do this is two steps. We’ll first write the function to make sure it working like we want, then we’ll assign the result back to the wells data frame. Mutate and select This added our new column to the end of the data frame. 5.12.1 Your turn to mutate Modify the above mutate function to also add a month_drilled column. 5.12.2 Document and save new columns Before the code chunk, write out what we are doing. Add a Markdown headline and description of our task, which was to add a “year_drilled” and “month_drilled” column. Name the chunk by adding add_year_month inside the {r} part of the chunk. Edit the first line wells %&gt;% to wells &lt;- wells %&gt;% to assign the mutate result back to our wells data frame. wells &lt;- wells %&gt;% mutate( year_drilled = year(drilling_start_date), month_drilled = month(drilling_start_date) ) As we’ve seen before, when we assign the result back to wells, the data frame will no longer print to the screen anymore. That’s OK. Inspect the wells data frame within the Environment tab and to make sure it was created properly. (If you really want to check the data on your screen, you could use head(wells) to see just the several lines.) As you may recall from our lesson on column renaming, we can create more than one column within the same mutate() function by separating them with commas. 5.12.3 Export the mutated data We actually want to keep these new columns to use later, so let’s do a quick export to save them for later. wells %&gt;% saveRDS(&quot;data-out/wells_02.rds&quot;) 5.13 Summarize() The summarize() and summarise() functions compute tables about your data. They are the same function, as R supports both the American and European spelling of summarize. I don’t care which you use. Learn about your data with Summarize() Much like the mutate() function, we list the name of the new column first, then assign to it the function we want to accomplish using =. Let’s find the average borehole_depth of all the wells. Attempt to find the mean But, our return isn’t good? What’s up with that? 5.13.1 ignoring na In short, you can’t divide by zero or a NULL or NA value. I’ll show you how to ignore them, but first we should find out how many there are: Find NA values Take a look at this and guess what is happening. Clearly is.na is a thing. How is it being used? There are 22 records returned out of 18k+. Can we safely exclude them without affecting the results? I think so. We can apply a similar function na.rm function inside our summarise() function to remove the missing values before the calculation, like this: NAs removed from summarize A mean (or average in common terms) is a way to use one number to represent a group of numbers. It works well when the variance in the numbers is not great. Median is another way, and sometimes better when there are high or low numbers that would unduly influence a mean. 5.13.2 Your turn with summarise Like filter and mutate, you can do more than one calculation within a summarize function. Edit the code chunk above in two ways: Make sure to name the code chunk, something like depth_summaries. Modify the summarize function to also create a median_depth summary. Look at your dplyr cheat sheet or google to find out how. 5.14 Group_by() The summarise() function is an especially useful in combination with another function called group_by(), which allows us to pivot tables to count and measure data by its values. Group by This is easier to understand when you can see an example, so let’s do it. 5.14.1 Group and count So, we have more than 18,000 wells, but we don’t know how many of each kind. We could filter them one-by-one, but there is an easier way. Group and count Let’s break this down: We start with the wells data frame. We then group_by the data by the proposed_use. If we print the data frame at this point, we won’t really see a difference. The group_by() function always needs another function to see a result. We then summarise the grouped data. In this case, we are creating a column called count, and we are assigning to is a special function n() which counts the number of records within each group. The result is for each unique value in the prospose_use column, we get the number of records that have that have that value. We then arrange the resulting table in descending order by our new column, count, so we can see which value has the most records. We can see that “Domestic” wells are more prevalent by a wide margin. If page through the list, you’ll see that to get an accurate count of each type of well, we’ll need to do some data cleaning. We’ll do that at another time. Let’s walk through another example: Group and summarise 5.14.2 Your turn to group How many wells were drilled in each county? Use the same group_by and summarise method to make a table that counts wells drilled in each county. Since you can summarise by more than one thing, try to find the count and average (mean) borehole_depth of wells by proposed use. You can copy the first summary we did and work from that, editing the summarise statement. 5.14.3 Counting only We’ll use summarize to do more than count, but if counting is all you want to know, there is an easier way. (I’ll try not to show you too many alternate methods … there are many ways to do everything, but this is worth knowing.) well %&gt;% count(proposed_use) It creates a column named “n” with the count. You could then use rename(new = old) to call it something else, like “wells_drilled”. 5.15 Transform review This has been a lot to learn, but it is the basics of just about any data analysis … to filter, select, arrange, group and summarise values. And to create new variables with with mutate. Next, we’ll start plotting some of this data so we can see it. 5.16 Turn in your in-class project At this point, you’ll want so save, knit to HTML and then close your project. Zip up the folder and turn it into the assignment in Canvas. 5.17 Practice assignment: Transforms on census For this practice assignment, you’ll continue with your “census-practice” project. The goal here is: For each race in the data, find the county with the highest percentage for that race. You’ll use the dplyr commands from this lesson to do it. Start a new notebook that imports the cleaned data from the last assignment. Start the notebook name with “02-” so you know the order to run them in the future. Use mutate() to create a new column for each race that calculates the percentage for that race. You might create columns names like “hispanic_prc” with the formula “(hispanic / total_populaton) * 100”. Assign those values back to the “census” data frame. Create a series of code chunks, one for each race that does this: Arrange the data so the county with the highest percentage of that race is on top, then use select() to show these columns: The county, total population, that race total, and the percentage of that race. Make sure that each each action is clearly described in Markdown headlines and text, and that each code chunk is named. If you feel like you are repeating yourself a lot on this assignment and feel like there should be a better way, I assure you there is. We will get to that. Save, Knit, Zip and upload your project to the “Practice: Transform with dplyr” assignment. "],
["cleaning.html", "Chapter 6 Cleaning 6.1 Goals for this section 6.2 Taking stock of the data 6.3 Setup and import 6.4 Clean the proposed_use column 6.5 Export your updated data frame 6.6 Future addition", " Chapter 6 Cleaning 6.1 Goals for this section Create a new notebook for cleaning data Throughout the notebook, we want to explain our thoughts and goals in Markdown. Each code block should have a human readable explanation of the goal or task. Import most recent data Create cleaned proposed_use column Export data for next notebook 6.1.1 Resources Strings chapter from Hadley Wickham’s book, specifically about str_replace(). RDocumentation on str_replace(). stringr cheatsheet. 6.2 Taking stock of the data As we were looking at the proposed_use field in the wells data, we found that the values there were pretty dirty, with misspellings and unofficial designations. If we look at the official designations for Proposed Use on page 10 of the data user manual, we see there are 14 official designations, none with any of various spellings of Piezo, which looks to be monitor wells. We need to create a clean version of the proposed_use column to use with our analysis and visualizations. Typically when I discover a situation like this, I go back to my first “import and cleaning” notebook and make changes there so the work can carry through to all subsequent notebooks, but in this case we’ll just make our changes in a new notebook and then document and export the changes for future work. 6.3 Setup and import Create a new R Notebook with a title “Wells cleaning” and a filename of 03-wells-cleaning.Rmd. In Markdown, write down our purpose and goals in your own words. Set up the tidyverse library and import the data/wells_02.rds file that we exported at the end of our last notebook. (If you don’t recall how to do this, look at your last notebook, but update the code to reflect the new filename.) For this block and all others, make sure you have a Markdown description of the goal or task. 6.4 Clean the proposed_use column 6.4.1 Count values in a column Let’s look again at the the values in the proposed_use column of the wells data. One way to see all the unique values and also find out how many there are is to use the count() function, which is a simple pivot table. wells %&gt;% count(proposed_use) Which gives us a list that looks something like this: proposed_use n AG WELL 3 Closed-Loop Geothermal 246 Commercial 1 De-watering 33 Domestic 8408 Environmental Soil Boring 3719 Ground Well for Electric Substation 2 Industrial 102 Injection 61 Irrigation 1493 IRRIGATION/TESTWELL 1 Monitor 3354 Monitor-VMP 2 peizometer 1 Peizometer 8 piezo 1 Piezo 12 piezometer 43 Piezometer 25 Piezometer Installation 1 PLUGGING 1 Public Supply 174 Rig Supply 14 Soil Vapor Monitor 10 Stock 259 Surface Slab Repair 1 Test Well 266 Unknown 5 Vapor Monitoring Point 1 VAPOR POINT 1 We have 30 different values here that we need to combine into at most 14 categories, which are the official ones listed on page 10 of the data user manual. After looking through it and doing some Googling, I came to a couple of conclusions: Anything named “piezo” or a variant should be a Monitor well. Anything named “vapor” should be a Monitor well. Anything that isn’t on the official list should be Other. TBH, if I was writing stories about this data, I would call the TWDB and make my sure that my educated assumptions are correct. But they seem reasonable given the documentation. 6.4.2 Change values in a column So, our goal here is to create a new column that starts with the value of proposed_use, but then we search through the values for things like “piezo” and set them to something more useful. We’ll utilize a stringr function called str_replace() using regular expressions to do this, within a mutate() function, which we know we can use to create or make changes in a column data. And, of course, we need to figure out how to do it before we save it, so let’s work through it. Start by calling the wells data frame, then using mutate to create a new column from proposed_use, and then count the rows from the new column. For now, it will be the same as it was for proposed use but we’ll fix that. Note that we will work through several steps in this mutate function before we save it back into wells. wells %&gt;% mutate( use_clean = proposed_use ) %&gt;% count(use_clean) 6.4.2.1 Convert to lowercase It will be much easier for us to deal with different spellings of words if everything was lower case. “Piezo” is different than “piezo”, so let’s convert everything to lower case. wells %&gt;% mutate( use_clean = tolower(proposed_use) ) %&gt;% count(use_clean) Now our results are all lowercase. It looks something like this: use_clean n ag well 3 closed-loop geothermal 246 commercial 1 de-watering 33 6.4.2.2 Clean up the piezo-ish values We still have four different versions of “piezo” and the like, which need to be labeled as “monitor”. You might make a mental note of how many values you currently have for “monitor” now so we can notice how that value grows as we fix our piezos. (I have 3354 in our example, but the number will be different depending on when the data was pulled from TWDB.) We can continue to stack changes inside our mutate() function to deal with this using str_replace(). There are three arguments to the str_replace() function: what column are we working on. what pattern are we looking for, as a regular expression. what value do we want it to be. We have created a new column use_clean that we want to continue to modify, so it is both our target and our source of our subsequent mutates. The pattern we want is anything that starts with the word “piezo” and “peizo” with anything that follows. The regex expression for “anything” is .*, so piezo.* with catch “piezo”, “piezometer” and “piezo installation”. We also want to set any value with those terms to “monitor”, so we set up our string replace function: str_replace(use_clean, &quot;piezo.*&quot;, &quot;monitor&quot;) and add it to our list of mutates: wells %&gt;% mutate( use_clean = tolower(proposed_use), use_clean = str_replace(use_clean, &quot;piezo.*&quot;, &quot;monitor&quot;) ) %&gt;% count(use_clean) If we look at the results of that change, we see we are left with the one misspelled “peizometer”. We can fix that by adding an “or” section to our search pattern, using the regular expression key |. wells %&gt;% mutate( use_clean = tolower(proposed_use), use_clean = str_replace(use_clean, &quot;piezo.*|peizo.*&quot;, &quot;monitor&quot;) ) %&gt;% count(use_clean) Check your results and make sure that there are no longer any versions of “piezo” in use_clean. You can keep stacking these str_replace() mutates to clean further values. 6.4.2.3 Your turn: str_replace functions Now it’s up to you to add more mutate strings to clean the rest of the column to get the 14 official “Proposed Use” designations listed below. It would make sense to organize your new mutate lines in logical ways, like perhaps to capture all the terms that would go into “other” together using the | in your search pattern, like we did with piezo and peizo example above. Here’s the official list: closed-loop geothermal de-watering domestic environmental soil boring industrial injection irrigation monitor other public supply rig supply stock test unknown 6.4.2.4 Double-check our results It might make sense to do one last check of our conversions before reassigning all of changes back to the wells data frame. Change the last count() function to the following: count(use_clean) #change this line distinct(proposed_use, use_clean) # to this line The result looks something similar to this: proposed_use use_clean Irrigation irrigation Domestic domestic Monitor monitor Public Supply public supply Environmental Soil Boring environmental soil boring Closed-Loop Geothermal closed-loop geothermal Industrial industrial Piezo monitor Stock stock Piezometer monitor Test Well test Unknown unknown piezometer monitor De-watering de-watering Ground Well for Electric Substation other VAPOR POINT monitor Piezometer Installation monitor Surface Slab Repair other Monitor-VMP monitor peizometer monitor piezo monitor Vapor Monitoring Point monitor Injection injection Peizometer monitor Commercial other IRRIGATION/TESTWELL test Soil Vapor Monitor monitor PLUGGING other AG WELL other Rig Supply rig supply This will allow you to double check that all your conversions happened properly. 6.4.3 Reassign your changes back to the data frame Once you have all your column changes worked out, you need to fix it up to reassign the values to the data frame, and then add Markdown commentary above it to explain the purpose of the code chunk. Edit your code chunk to remove the distinct() or count() functions at the end. Edit the code chunk to reassign the values back to wells. wells &lt;- wells %&gt;% mutate( use_clean = tolower(proposed_use), use_clean = str_replace(use_clean, &quot;piezo.*|peizo.*&quot;, &quot;monitor&quot;), # your other str_replace items ... ) 6.5 Export your updated data frame Let’s again export our data so we can use it in a new notebook. Since we are in our third notebook of this project, let’s name the file wells_03.rds since this is coming out of our third notebook. saveRDS(wells, &quot;data-out/wells_03.rds&quot;) 6.6 Future addition While this data doesn’t support it, it would be good to have here an example of how you might take a column made up of codes, like school accountability ratings being “M”, “I”, “A”, etc. and convert them to their readable values like like “Met standard”, “Needs Improvement” and “Alternative standard”. "],
["graphics.html", "Chapter 7 Graphics 7.1 Goals for this section 7.2 Introduction ggplot 7.3 The basic ggplot template 7.4 Let’s plot! 7.5 Plotting our wells data 7.6 Wells per county over time 7.7 Your turn: Build a line chart 7.8 Review of ggplot 7.9 Plotly for more interactive graphics 7.10 Resources and further reading", " Chapter 7 Graphics 7.1 Goals for this section An introduction to the Grammar of Graphics We’ll make charts! 7.2 Introduction ggplot ggplot2 is the data visualization library within Hadley Wickham’s tidyverse. It uses a concept called the Grammar of Graphics, the idea that you can build every graph from the same components: a data set, a coordinate system, and geoms – the visual marks that represent data points. With a hat tip to Matt Waite, the main concepts are: data: which data frame you are pulling from aesthetics: the specific data from the data frame which we are going to plot geometries: the shape the data is going to take scales: any transformations we might make on the data layers: how we might lay multiple geometries over top of each other to reveal new information. facets: which means how we might graph many elements of the same data set in the same space The challenge to understand here is for every graphic, we start with the data, and then describe how to layer plots or pieces on top of that data. 7.3 The basic ggplot template The template for a basic plot is this. (The &lt;&gt; denote that we are inserting values there.) ggplot(data = &lt;DATA&gt;, aes(mapping = &lt;MAPPINGS&gt;)) + &lt;GEOM&gt;(&lt;ADDITONAL_MAPPINGS&gt;) ggplot() is our function. We feed into it the data we wish to plot. aes() stands for “aesthetics”, and it describes the column of data we want to plot, and how, like which column is on the x axis and which is on the y axis. These are called mappings, which we show in our template with &lt;MAPPINGS&gt;. They typically look like this: aes(x = col_name_x, y = col_name_y). Now matter what type of chart we are building (bar chart, scatterplot, etc) we have to tell it which columns to show on the chart. The + is the equivalent of %&gt;% in our tidyverse data. It means we are adding a layer, and it should always be at the end of the line, not at the beginning of the next. &lt;GEOM_FUNCTION&gt; is the type of chart or addition we are adding. They all start with the term geom_ like geom_bar, which is what we will build with this example. It will take the mappings we supplied and plot them on the type of geom_ we choose. &lt;ADDITIONAL MAPPINGS&gt; if a geom_ requires it, we can specify additional columns/axis mapping combinations to that geom_. We don’t always have or need them. There are some ways to simplify this, and some ways to complicate it. Let’s simplify first: It is implied that the first thing fed to ggplot is the data, so you don’t have to write out data = unless there is ambiguity. The aes() values are also implied as mappings, so you don’t have to write out mapping = unless there is ambiguity. ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;) + &lt;GEOM&gt; ) One of the ways we make things complicated, is we layer different geometries. We might start with a scatterplot, and then add a reference line on top of it, which is a new geometry. Each goem_ can specify their own mappings. ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;(aes(&lt;SPECIFIC_MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;(aes(&lt;SPECIFIC_MAPPINGS&gt;)) 7.4 Let’s plot! 7.4.1 Set up our Notebook Create a new RNotebook. Title it “Wells visualizations” and name the file 04-charts.Rmd. Load the following libraries: tidyverse, lubridate. library(tidyverse) library(lubridate) 7.4.2 Scatterplot One of the better ways to see this in action for the first time is build a scatterplot showing the relationship between two numbers. Unfortunately, our wells data does not have two such values, so we’ll explore this using a data set that is already built into ggplot2, mpg. Take a look at the mpg by calling it like a data frame. mpg It looks something like this, which shows the first and last couple of rows: manufacturer model displ year cyl trans drv cty hwy fl class audi a4 1.8 1999 4 auto(l5) f 18 29 p compact audi a4 1.8 1999 4 manual(m5) f 21 29 p compact audi a4 2 2008 4 manual(m6) f 20 31 p compact audi a4 2 2008 4 auto(av) f 21 30 p compact … … … … . … . .. .. . … volkswagen passat 2 2008 4 manual(m6) f 21 29 p midsize volkswagen passat 2.8 1999 6 auto(l5) f 16 26 p midsize volkswagen passat 2.8 1999 6 manual(m5) f 18 26 p midsize volkswagen passat 3.6 2008 6 auto(s6) f 17 26 p midsize The data is a subset of fuel economy data from 1999 and 2008 for 38 popular cars. Don’t get too hung up on the data, it is just for examples. The size of an engine is shows in the column displ. The Audi A4 has a 1.8 liter engine. The column hwy is the fuel rating for highways. Well also us the class column, which categorizes the type of vehicle. What kind of relationship might you expect between the size of the engine and highway mileage? Let’s use our plot to show this. If our basic template is like this: ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;) + &lt;GEOM&gt; Now, let’s put our data in here. Our goal here is to show how the hwy number (y axis) changes as the displ number gets bigger (x axis.) ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() Which gets us a our first chart: MPG: hwy vs displ We can see there is a relationships of sort here, but ggplot has some additional geometries to help us see this, including geom_smooth(). Since we have already the mappings in the main ggplot() call, all we have to do is add the new geom. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth() # new plot. don&#39;t forget the + on previous line MPG: displ vs hwy with smooth line Let’s add one more visual cue (or aesthetic) to this graphic by coloring the dots based on the class of the vehicle. Since we want this aesthetic to apply only to the geom_point(), we have to add the aes() value there. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + # added color aesthetic geom_smooth() MPG: disply vs mpg with class Looking at that graphic, what values might you want to learn more about? 7.5 Plotting our wells data For bar charts and line charts, we can return to our wells data, so let’s import what we had from our last notebook. wells &lt;- readRDS(&quot;data-out/wells_03.rds&quot;) 7.5.1 Total wells per county 7.5.1.1 Shape our data If we are plotting wells per county, we need to first build a data frame that counts the number of wells for each county. We can use the same count() function that we used when we cleaned our data. wells_by_county &lt;- wells %&gt;% count(county) %&gt;% rename(wells_count = n) wells_by_county Let’s break this down: The first line creates the new data frame wells_by_county, starting with our wells data frame. We apply the count() function on the “county” column. This makes our basic pivot table. On the third line, we rename the “n” column that was created by count(), so it is more descriptive, calling it wells_count. So now we have a data frame with two columns: county and wells_count. We print it on the fourth line so we can inspect it. 7.5.1.2 Plot our wells by county Here is the verbose plot for our counties. ggplot(data = wells_by_county) + geom_bar(mapping = aes(x = county, y = wells_count), stat = &quot;identity&quot;) On the first line we tell ggplot() that we are using the we wells_by_county data. On the next, we apply the geom_bar() function to make a bar chart. It needs two things: The mapping, which are the aesthetics. We well it to plot county on the x (horizontal) axis, and wells_count on the y (vertical) axis. Because county is a category instead of a number, we have to use the stat = &quot;identity&quot; value to describe that we are using values within county to separate the bars. This is a special thing for bar charts. One of those things that drive you nuts. Basic county plot Our less verbose way to do this looks like this: ggplot(wells_by_county, aes(x=county, y=wells_count)) + geom_bar(stat = &quot;identity&quot;) 7.5.2 Add a layer of text labels For each new thing that we add to our graphic, we add it with +. In this case, we want to add number labels to show the wells_count for that county. ggplot(data = wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.25) # adds the numbers on bars Basic county plot In this case, we are just adding another layer, the geom_text(). It requires some additional aesthetics, like what label= we want to use. The vjust= moves the numbers up a little. Change the number and see what happens. The last layer we want to add here is a Title layer. The function for labels is called labs() and it takes an argument of title = &quot;&quot; You can also change your x and y axis names, etc. ggplot(data = wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.25) labs(title = &quot;Number of wells drilled by county&quot;) # adds the title Wells by county with title Congratulations! You made your first ggplot() chart. Not particularly revealing, but it does show that Travis County has WAY more wells than the other counties. Let’s see how those trends play out over time. 7.6 Wells per county over time Our next chart will be a line chart to show how the number of wells drilled has changed over time within each county. Again, it will help us to think about what we are after and then build our data frame to match. In this case, we want to plot the “number of wells” for each county, by year. That means we need a data frame that has columns for county, year and the number of wells. To get that, we have to use group and summarize. Sometimes it helps to write out the steps of everything before you to do it. Start with the wells data frame. Filter to 2003 or later, because that’s when the system came online. Group by the county and year_drilled fields. Summarize to create a count of the number of wells_drilled. Set all of the above to a new data frame, wells_county_year. Start a plot with the new data. Set x (horizontal) to be year_drilled and y (vertical) to be wells_drilled, and color to be the county. 7.6.1 Work up the data frame wells %&gt;% filter(year_drilled &gt;= 2003) %&gt;% group_by(county, year_drilled) %&gt;% summarise( wells_drilled = n() ) This gives you a table similar to this: county year_drilled wells_drilled Bastrop 2003 110 Bastrop 2004 99 Bastrop 2005 97 … … … Caldwell 2003 40 Caldwell 2004 32 Caldwell 2005 40 We call this long data, because each row contains a single observation, instead of wide data, which would have a column for each observation. Once you are have the data formatted, set it to fill a new data frame called wells_county_year. 7.6.2 Draw the plot Remember the formula for a basic plot: ggplot(&lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt; and if all our mappings are the same, they can go into the ggplot function. ggplot(wells_county_year, aes(x=year_drilled, y=wells_drilled)) + geom_line(aes(color=county)) + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) Wells drilled by county by year How easy would it be to add points for every year to make each data point stand out? 7.6.3 Your turn: Add layers Add a new layer geom_point() and see what happens Add a labels layer to add a title, like we did in the bar chart above. 7.6.4 Dates as numbers and the problems they cause There was one point during my work on this graphic when my x axis did not fall evenly on years, and I figured it was because the year_drilled field was a number and not a date. It’s possible to fix that by including the library(lubridate) and then mutating the year_drilled column like this: mutate( year_drilled = ymd(year_drilled, truncated=2L) ) %&gt;% 7.7 Your turn: Build a line chart Now, I’d like you to build a line chart that shows how the different kinds of wells drilled has changed over time. Here’s a major hint: It’s very much like the line chart you just built, but with different columns. You’ll need so start at creating a data frame with the correct data. 7.8 Review of ggplot Exploring with graphics are one of the more powerful features of working with R. It takes a bit to get used to the Grammar of Graphics and ggplot2 and it will be frustrating at first. But be assured it can do about anything once you learn how, and being able to fold in these charts with your thoughts and analysis in a repeatable way will make you a better data journalist. By design, every chart in ggplot starts with the same three things: data, a geometric coordinate system, and a mapping of the aesthetics, including the x and y values. ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt; If your graphic is simple, there may be less verbose ways to write it as ggplot will assume your are passing it data first, and that aes() functions are for mapping. 7.9 Plotly for more interactive graphics At the risk of adding yet a little more complexity I want to introduce you to Plotly. I see two ways you might find Plotly interesting: 7.9.1 ggplotly ggplotly allows you to port your ggplot graphic into Plotly so they have interactive tooltips. The tutorial examples are also not bad for a general ggplot reference. After installing and loading the Plotly library, giving your chart hover tips is as easy as assigning your plot to an object (p in the example below), and then calling that with the ggplotly() function: p &lt;- ggplot(mpg, aes(x=displ, y=hwy)) + geom_point() + geom_smooth() ggplotly(p) ggplotly example The black label above appears when you hover on the graphic. 7.9.2 More with plot_ly() function You can gain a little more control over your Plotly graphic if you build them using the plot_ly() function instead of ggplot(), but you have to learn a new syntax. It’s still based on the Grammar of Graphics, so it’s not hard … just different. For example, for our “Wells by County and Year” graphic we did earlier looks like this: wells_county_year %&gt;% plot_ly(x = ~year_drilled, y = ~wells_drilled, name = ~county, type = &quot;scatter&quot;, mode = &quot;lines+markers&quot;) And it ends up looking like this: Plotly example 7.9.3 Plotly’s freemium model It appears that you can use these open source libraries without charge from Plotly. They do also have a hosting service to allow you to embed charts in other websites, which can get into a pay tier of their service. 7.10 Resources and further reading The ggplot2 documentation and ggplot2 cheatsheets. R for Data Science, Chap 3. Hadley Wickam dives right into plots in his book. R Graphics Cookbook has lots of example plots. Good to harvest code and see how to do things. The R Graph Gallery another place to see examples. "],
["tidy.html", "Chapter 8 Tidy data 8.1 Goals for this section 8.2 What is tidy data 8.3 Tidyr package 8.4 The tidyr verbs 8.5 Set up the mixbev project 8.6 Create an explore notebook 8.7 What might we learn about this dataset 8.8 Add years and months values 8.9 Campus bars 8.10 West Campus student hot spots 8.11 Campus-area bar sales over time 8.12 Types of sales within student bars 8.13 How gather() works 8.14 Applying gather() to beer, wine and liquor 8.15 The spread() function 8.16 Practice assignment: Exploring the top seller 8.17 Bonus: Top 5 sellers in Travis County over three years 8.18 Bonus: Total sales by county", " Chapter 8 Tidy data Data “shape” can be important when you are trying to work with and visualize data. In this chapter we’ll discuss “tidy” data and how this style of organization helps us. Slides by Hadley Wickham are used with permission from the author. 8.1 Goals for this section Explore what it means to have “tidy” data. Learn gather(), spread() and other tidyr verbs. Use Mixed Beverage Gross Receipts to explore shaping data. We’ll introduce the RSocrata package to get the data. Explore and chart the alcohol data to practice our skills. 8.2 What is tidy data “Tidy” data is well formatted so each variable is in a column, each observation is in a row and each value is a cell. Our first step in working with any data is to make sure we are “tidy”. Tidy data definition It’s easiest to see the difference through examples. The data frame below is of tuberculosis reports from the World Health Organization. Each row is a set of observations (or case) from a single country for a single year. Each column describes a unique variable. The year, the number of cases and the population of the country at that time. A tidy table Table2 below isn’t tidy. The count column contains two different type of values. A tidy table When our data is tidy, it is easy to manipulate. We can use functions like mutate() to calculate new values for each case. Manipulate a tidy table 8.3 Tidyr package When our data is tidy, we can use the tidyr package to reshape the layout of our data to suit our needs. In the figure below, the table on the left is “wide”. There are are multiple year columns describing the same variable. It might be useful if we want to calculate the difference of the values for two different years. It’s less useful if we want plot on a graphic because we don’t have columns to map as X and Y values. The table on the right is “long”, in that each column describes a single variable. It is this shape we need when we want to plot values on a chart. We can then set our “Year” column as an X axis, our “n” column on our Y axis, and group by the “Country”. Wide vs long 8.4 The tidyr verbs We’ll use functions within the tidyr package to manipulate data to our liking, depending on our need. Tidy verbs 8.5 Set up the mixbev project We’re going to work in a new project with new data for this assignment. I will try to get you up and running as quickly as possible. In RStudio, choose File &gt; New Project Walk through the steps to create a New Project in a New Directory called yourname-mixbev. Once you have your project, create a new RNotebook. Save the file and name it 01-mixbev-import.Rmd. Go to this link and copy the text and replace everything in your RNotebook. There are a couple of things we need to do before you run this notebook: In the R Console, run install.packages(&quot;RSocrata&quot;) Use the Files pane to create a new folder called data-raw so we have a place to save our data. Now use Cmd-option-R (or go to Run &gt; Run All) to run the notebook. Running that notebook will download three years of data from Travis County establishments and save it into your data-raw folder in your project. How that is done is all documented in that notebook, but we may spend some time in class explaining going over it. 8.6 Create an explore notebook Now that we have data we don’t have to download it again. Let’s create a new RNotebook to import and explore it (and learn Tidyr while we are at it). Create a new RNotebook. Save the file as 02-mixbev-explore.Rmd. Update the title in the metadata. Remove the boilerplate below the data. Add the code below, then Restart R and Run All Chunks. library(tidyverse) library(lubridate) library(scales) # import the data receipts &lt;- readRDS(&quot;data-raw/receipts_api.rds&quot;) 8.6.1 Peruse the data In the environment window, click on the receipts data frame so it opens and you can look at the data. Some key things to know about the data: Columns with location_ are about a specific restaurant or bar selling alcohol. Columns with taxpayer_ are about the owners of that establishment. The monetary amounts for _receipts are total sales numbers for that establishment in that month. The obligation_end_date is the last day of the month for those sales. The liquor type sales like beer and wine should all add up to the total_receipts, but sometimes type sales are blank. I don’t trust cover_charge_receipts at all. There are several other columns we won’t deal with in this lesson. So these are NOT the number of drinks sold. It’s the amount of money brought in for the total sale of each type of liquor within that month. The total_receipts is used to calculate tax paid to the state on those sales based on a formula. See the record layout on Socrata for more information. 8.7 What might we learn about this dataset If we look at the data set, there are a series of questions we might ask it. How have total sales changed over the past three years? Who has sold the most alcohol over the past three years? Which campus-area bars have sold the most, and what are their sales trends? Do these campus-area bars sell more beer, wine or liquor? Have the number of establishments that sell alcohol increased? If so, where? There are others for sure, but for this lesson we’ll concentrate on campus-area bars you might be familiar with. 8.8 Add years and months values We will end up doing a lot of summaries based on year and month of the data. It will be easier to do that if we create some new columns that have those values. receipts &lt;- receipts %&gt;% mutate( # sales_year = year(obligation_end_date_yyyymmdd), sales_year = year(obligation_end_date_yyyymmdd) %&gt;% as.character(), sales_month = month(obligation_end_date_yyyymmdd, label = TRUE) ) I specifically coerced sales_year into a string because the year as a continuous number was causing problems with plots later when I got an axis mark for “2016.5”. Probably not the best solution, but it worked. 8.9 Campus bars Let’s take a look at sales around campus. Make a data frame of just the receipts from the 78705 area. uni_area &lt;- receipts %&gt;% filter( location_zip == &#39;78705&#39; ) # peek at the result uni_area %&gt;% head() 8.9.1 Total 78705 sales leaders in 2018 Let’s make a quick table to add total_receipts for 2018 so we can find the top selling bars from last year. We can do that by filtering for 2018, grouping by location and address, and then summing together the total receipts. uni_area %&gt;% filter(sales_year == 2018) %&gt;% group_by(location_name, location_address) %&gt;% summarize( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) %&gt;% head(10) And we get this: location_name location_address total_sales EXECUTIVE EDUCATION AND CONFERENCE CENTER 1900 UNIVERSITY AVE 1684571 SPIDER HOUSE 2908 FRUTH ST 1575812 CAIN &amp; ABEL’S 2313 RIO GRANDE ST 1051230 TRUDY’S TEXAS STAR CAFE 409 W 30TH ST 1012269 THE HOLE IN THE WALL 2538 GUADALUPE ST 827799 HOTEL ELLA/GOODAL’S KITCHEN &amp; BAR 1900 RIO GRANDE ST 707431 DOC’S MOTORWORKS 38TH 1106 W 38TH ST 561019 THE LOCAL PUB AND PATIO 2610 GUADALUPE ST 544261 VIA 313 PIZZA RESTAURANT II 3016 GUADALUPE ST 531878 THE BACK LOT 606 MAIDEN LN 494638 8.10 West Campus student hot spots Looking at the list above, let’s filter our original data to some top student hangouts that we know are still open. student_bars &lt;- receipts %&gt;% filter( location_name %in% c( &quot;SPIDER HOUSE&quot;, &quot;CAIN &amp; ABEL&#39;S&quot;, &quot;TRUDY&#39;S TEXAS STAR CAFE&quot;, &quot;THE HOLE IN THE WALL&quot;, &quot;THE LOCAL PUB AND PATIO&quot; ) ) # check the results student_bars %&gt;% count(location_name, location_address) Note the filter() function above. In order to feed in a list of location names into the filter, I used the %in% operator (instead of ==) and I put list of locations into a c() function. The C stands for concatenate, FWIW. After creating the student_bars data frame above, I used count() to make sure we caught all the bars and made sure each had the same number of reports. Which looks like this: location_name location_address n CAIN &amp; ABEL’S 2313 RIO GRANDE ST 36 SPIDER HOUSE 2908 FRUTH ST 36 THE HOLE IN THE WALL 2538 GUADALUPE ST 36 THE LOCAL PUB AND PATIO 2610 GUADALUPE ST 36 TRUDY’S TEXAS STAR CAFE 409 W 30TH ST 36 Note I didn’t save the count() function back to the data frame. I just viewed that to the notebook so we could check our work. 8.11 Campus-area bar sales over time 8.11.1 Visualizing your visualization Now, if we are interested in charting it helps to think about what we need and how to shape our data to get it. Let’s start with charting how sales at each bar have changed over the past three years. Since we are looking at value over time for multiple things, a line chart will probably work best. If our basic line chart works like this: ggplot(&lt;DATA&gt;, aes(x=&lt;COL_VALUE&gt;, y=&lt;COL_VALUE&gt;, group=&lt;COL_VALUE&gt;)) + geom_line() We need to figure out how to configure our data to fit the chart. For the “X” value (horizontal) we have the sales_year field to track over time. For the “Y” value (vertical) we want the total sales of each bar for that year. We’ll need to do a summary to get that. We want a line for each campus bar, which means we need a column for location_name for our “group”, too. We create this data frame by using our group_by() on location_name and sales_year and then summarize() to get our total sales by our grouping: student_bars_grouped &lt;- student_bars %&gt;% group_by(location_name, sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) # peek at the results student_bars_grouped %&gt;% head() Which ends up looking this this: location_name sales_year total_sales CAIN &amp; ABEL’S 2016 896114 CAIN &amp; ABEL’S 2017 990631 CAIN &amp; ABEL’S 2018 1051230 SPIDER HOUSE 2016 1797800 SPIDER HOUSE 2017 1721580 SPIDER HOUSE 2018 1575812 Now we can plug in our columns to get our chart: ggplot(student_bars_grouped, aes(x=sales_year, y=total_sales, group=location_name)) + geom_line(aes(color=location_name)) + geom_point() + scale_y_continuous(labels=comma) Campus bar sales over time We’ve added a couple of do-dads to our line chart to make it prettier, like adding a color aesthetic the geom_line() function to color the lines and give us a legend, and a scale with labels to make the Y numbers pretty. 8.11.2 What did we find? Looking at the chart, we can see that Cain &amp; Abel’s is the only top campus-area bar with an increase in alcohol sales over each of the past three years. The Hole in the Wall did have a good 2018, reversing a downward trend. Everywhere else dropped sales each year. 8.12 Types of sales within student bars If we want to see how beer, liquor and wine sales differ at an establishment, we could use a very similar chart, but we would want each line to be the type of alcohol: beer, wine or liquor. But this is how our columns look right now. Beer, wine and liquor are separate columns. Sales columns This is where we introduce gather() from tidyr so we can shape our data to meet our needs. 8.13 How gather() works The gather() function is what we use to change wide data into long data. We are “gathering” all the extra columns into two: one for the value and one to describe it. In the example below, a key column is created called “Year” and a value column called “n” is created to hold the values from each of the yearly columns. For each value, a new row is created, and the column name is used for the “key” and the data is use for the “value”. Show how gather works We can see how this works by tracking a single value from one shape to the other: Show gather with single value Now, how do we define this in code? Gather as a function We are, of course, starting with the data frame and then piping it into the gather() function. The first value we have to give the function is to name our key column. Name the key column And the next value we give it is name of the value column. Name the value column Then we have to give it the range of columns that we want to gather. You can define those in different ways. The first method is a number range of the columns, starting in order of the columns in the data frame. So, for our example above, we want the second, third and fourth column, so we use the range 2:4. Name the range Or, we could supply those columns in other ways, like by their column names: Range as column names Or by specifying which columns we don’t want to gather: Range through deselect 8.14 Applying gather() to beer, wine and liquor Let’s start by making a new data frame with just the columns we need to work with. We are starting with the student_bars data frame we created that has the three years of receipts from just our five campus-area bars: student_sales &lt;- student_bars %&gt;% select(location_name, sales_year, beer_receipts, wine_receipts, liquor_receipts) %&gt;% rename( beer = beer_receipts, wine = wine_receipts, liquor = liquor_receipts ) student_sales %&gt;% head() # to show our results We get a result like this: location_name sales_year beer wine liquor THE LOCAL PUB AND PATIO 2018 24832 163 14202 THE LOCAL PUB AND PATIO 2017 27702 470 16086 THE HOLE IN THE WALL 2018 30374 370 32533 THE HOLE IN THE WALL 2017 34232 174 32789 THE HOLE IN THE WALL 2018 49320 252 42080 CAIN &amp; ABEL’S 2016 34239 456 42543 Now, if we can “gather” the beer, wine and liquor columns into two new columns – alcohol type and sales amount – then we could group and sum by the year and type. The code below builds the new data frame we’ll use for our chart (student_sales_grouped) but you skip the beginning data frame assignment and walk through it line by line to see how it gets built, starting with the student_sales data frame. Once you have it, then assign it back to student_sales_group. student_sales_grouped &lt;- student_sales %&gt;% gather(alcohol_type, sales_by_type, 3:5) %&gt;% group_by(location_name, sales_year, alcohol_type) %&gt;% summarise( sales_sum_year = sum(sales_by_type) ) # peek at the result student_sales_grouped %&gt;% head() We start with the student_sales data frame, where each row has the sales for each month of the year in multiple columns. gather() names our fields “alcohol_type” and “sales_by_type” and gathers our beer, wine and liquor columns. At this point, each row of data has the individual sale of a type of alcohol for each month of each year. group_by() collects our data by the name, year and type. This prepares us to: summarize() adds together all the beer sales for each month are summed up into one row for each year. 8.14.1 Plot sales by type for a single campus-area bar Let’s figure out the line chart with one establishment first: cain &lt;- student_sales_grouped %&gt;% filter(location_name == &quot;CAIN &amp; ABEL&#39;S&quot;) ggplot(cain, aes(x=sales_year, y=sales_sum_year, group=alcohol_type, color=alcohol_type)) + geom_line() We get our chart like this: Cain &amp; Abel’s alcohol sales 8.14.2 Plot sales by type for multiple campus-area bars For this chart, we are going to duplicate what we did for the graphic above, and then instead of applying to the cain data frame, we’ll substitute in our our student_sales_grouped data frame so have all the West Campus bars. And now we can introduce the facet_wrap(), which allows you to duplicate a graphic based on one of the categories in the data. We feed the facet_wrap() function with the variable (the column name) we want to duplicate. Weirdly, it has to start with a tilde, like this: facet_wrap(~location_name). ggplot(student_sales_grouped, aes(x=sales_year, y=sales_sum_year, group=alcohol_type, color=alcohol_type)) + geom_line() + facet_wrap(~location_name) + scale_y_continuous(label=comma) Student bars as a facet wrap 8.14.3 What did we learn? Liquor sales are what is driving Cain &amp; Abel’s increase in revenue. Wine sales are pretty low everywhere and Trudy’s really sells a lot of liquor. It’s those Mexican Martinis and Margaritas, I imagine. 8.15 The spread() function There is another tidyr function called spread() that does the opposite of gather(). We won’t do an example with this lesson, but this is how it generally works. The spread() function A spread() example 8.16 Practice assignment: Exploring the top seller This assignment looks a little further into some specific establishments from the Mixed Beverage Data. Here are the directions, with lots of hints thrown in. Create a new notebook called “03-practice.Rmd”. Use the same libraries and data as the in-class assignment. Find the establishment with highest total receipts in last three years. See the first Bonus section below for the the code. For the top establishment found, create a bar chart with sales by year. Here are some hints: Start with receipts, then filter by the location_name and the location_address. (You have to include location_address because there is more than one location in Ausitn for this chain.) Assign that to a data frame that you can use going forward. Group by sales_year, then summarize to get the sum(total_receipts). Review Summarize if needed. Save the result into a new data frame. Use ggplot() with geom_bar(stat=&quot;identity&quot;). Review the less verbose wells by county for an example of how to write the plot. For that top establishment, create a bar chart with total_receipts each month. Some hints: Don’t over think this. You already have a filtered data set, and you can plot the x axis on the obligation_end_date_yyyymmdd. Because the chart shows trends but it’s hard to see the values for the top months, create a table showing the top monthly total sales by month. Some hints: Select the oblibgation_end_date_yyyymmdd and arrange the total_receipts in descending order. Show just the top by piping into head() For that establishment, create a line chart with sales by alcohol type by year. This one is more challenging, but you have everything you need in the chapter when we did this for Cain &amp; Abel’s. Here are some hints: You can start from the the filtered data that you did with the previous chart. Create a new data frame with just the columns you need: select name, sales_year, beer, wine, liquor Use gather() to collect the beer, wine, liquor columns as alcohol_type and sales_by_type. This is the same as we did in class. Group by sales_year, alcohol_type. Summarize to get the sales_sum_year. Plot as a geom_line chart with x=sales_year, y=sales_sum_year, group and color as alcohol_type. Lastly, to show that once you have code you can re-purpose it, create a line chart of sales by alcohol type by year for the Circuit of the Americas. Your hint: Create a new data frame with just the Circuit of the Americas data. Copy the steps from the last chart, and change out the data frame name. 8.17 Bonus: Top 5 sellers in Travis County over three years Because some chains might have that same location_name but more than one location, we need to group by both the location_name and the location_address fields. Then we can summarize their total sales. receipts %&gt;% group_by(location_name, location_address) %&gt;% summarise( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) %&gt;% head(5) Which gives you something like this: location_name location_address total_sales WLS BEVERAGE CO 110 E 2ND ST 35878211 RYAN SANDERS SPORTS 9201 CIRCUIT OF THE AMERICAS BLVD 20714630 W HOTEL AUSTIN 200 LAVACA ST 15435458 ROSE ROOM/ 77 DEGREE 11500 ROCK ROSE AVE 14726420 THE DOGWOOD DOMAIN 11420 ROCK ROSE AVE STE 700 14231072 What exactly is that top location? (Google it). It looks like that location (hint: a hotel) has sold more that twice the amount as the next hotel. What is the second location? That could be pretty interesting to look further into as well. 8.18 Bonus: Total sales by county So, are alcohol sales going up in Travis County? We can use our new sales_year column to group the data and summarize by the sum of total_sales. receipts %&gt;% group_by(sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) If you want to see the result, we can plot as a bar chart. I’m repeating our code above, but then shoving it into a new data frame, which we use to plot. receipts_group_all &lt;- receipts %&gt;% group_by(sales_year) %&gt;% summarise( total_sales = sum(total_receipts) ) ggplot(receipts_group_all, aes(x=sales_year, y=total_sales)) + geom_bar(stat = &quot;identity&quot;) + scale_y_continuous(labels=comma) The last line in the plot comes from the scales package and it gives us pretty numbers on the Y axis. If you are writing a story about alcohol sales in Travis County over the last three years, you can say that sales have risen over each year. You might go further and compare the sales to the population over the same time period, but we’ll skip that for now. "],
["reordering-factors.html", "Chapter 9 Reordering factors 9.1 Goals for this section 9.2 Create our survey project 9.3 Figuring out our data shape 9.4 Charting the popularity of princess 9.5 Create our princess plot 9.6 Factors 9.7 Reorder princess 9.8 Factors recap 9.9 Practice: Make an ice cream chart 9.10 Resources", " Chapter 9 Reordering factors There is a complexity within R data frames that we need to cover becomes it comes into play when we want to order categorical data within graphics. We are going to handle this through a new project using our Survey data from class. 9.1 Goals for this section Create a new project with our class survey data. Create a chart that uses categorical data. Reorder the values in the chart using fct_reorder(). This is the chart we want to build: Popular princesses 9.2 Create our survey project 9.2.1 Setup In RStudio, choose File &gt; New Project Walk through the steps to create a New Project in a New Directory called yourname-survey. In your project, create a new directory called data-raw. Go to this link in a browser. Do File &gt; Save page as and save the file inside your data-out folder as survey-results.csv. Create a new RNotebook. Save the file and name it 01-survey.Rmd. For this simple example, we’ll only be using one notebook. 9.2.2 Libraries We need two libraries. I think the forcats library is already installed, but if not you’ll have to run the following in your RConsole: install.packages(&quot;forcats&quot;). library(tidyverse) library(forcats) 9.2.3 Import the data Import the data from the csv survey &lt;- read.csv(&quot;data-raw/survey-results.csv&quot;) %&gt;% clean_names() # peek at the data survey %&gt;% head() Peeking at the data, we see it is something like this: class graduating ice_cream princess computer Senior Yes Cookies &amp; Cream Mulan Macintosh Masters candidate Yes Rocky Road Mulan Windows Senior Yes Chocolate Ariel (Little Mermaid) Windows Senior No Mint chocolate chip Mulan Macintosh Junior No Mint chocolate chip Jasmine (Aladdin) Windows Senior No Mint chocolate chip Rapunzel (Tangled) Macintosh 9.3 Figuring out our data shape As we’ve talked about before, it is helpful to think of what we want the graphic to be, even to draw it out, so we can figure out what columns we need for the X and Y axis. The example I showed above is a little weird in that we are really building a bar/column chart, but we’ve turned sideways so we can read the labels. To build the chart, we are really looking at this: Princess before flip What do we need for the x value? Well, we need the total votes for each princess. What do we need for the y value? We need to list each princess that got votes. 9.4 Charting the popularity of princess So, we need a princess column and a votes column. These easiest way to do this is a simple count() summary. Build the count before you assign it back to princess_count so you see what is happening, but this is similar to what we’ve done in the past. princess_count &lt;- survey %&gt;% count(princess) %&gt;% rename(votes = n) %&gt;% arrange(desc(votes)) # peak princess %&gt;% head In order, we are: Assigning the result (which you do at the end), starting from survey. Count the rows of each princess. Rename the n column to votes. Arrange so the most votes are on top. We get this: princess votes Mulan 14 Rapunzel (Tangled) 7 Jasmine (Aladdin) 6 Ariel (Little Mermaid) 5 Tiana (Princess and the Frog) 2 Aurora (Sleeping Beauty) 1 Belle (Beauty and the Beast) 1 Merida (Brave) 1 Snow White 1 9.5 Create our princess plot We are going to use a new chart, geom_col, which is like geom_bar but it already understands the stat=&quot;identity&quot;. princess_count %&gt;% ggplot(aes(x = princess, y = votes)) + geom_col() + coord_flip() + labs(title=&quot;Favorite Disney Princesses in class&quot;, x = &quot;Princess&quot;, y = &quot;Votes&quot;) + geom_text(aes(label = votes), hjust=-.25) And we get this: Princess wrong order Well, that is frustrating. The bars are not in the same order that we arranged them in the data frame. As explained in the R-Graph-Gallery post: This is due to the fact that ggplot2 takes into account the order of the factor levels, not the order you observe in your data frame. You can sort your input data frame with sort() or arrange(), it will never have any impact on your ggplot2 graphic. 9.5.1 Labels and titles Before I get into factors, let me explain the other lines in the graphic above: coord_flip() transposes the bars so they go horizontal instead of vertical. This allows us to read the princess values. labs() allows us to add the title and cleaner axis labels. geom_text() adds the numbers at the end of the bar, with some adjustments to get them off the top of the bars. 9.6 Factors Factors allow you to apply an order (called “levels”) to values beyond being alphabetical. It is super useful when you are dealing with things like the names of months which have a certain order (“Jan”, “Feb”, “March”) which would not be ordered correctly alphabetically. But is is kind of frustrating here. We can improve it by reordering the levels of princess using a function from the forcats package. (forcats is an anagram for “factors”). There are four functions you can use reorder a factor: fct_reorder(): Reordering a factor by another variable. fct_infreq(): Reordering a factor by the frequency of values. fct_relevel(): Changing the order of a factor by hand. fct_lump(): Collapsing the least/most frequent values of a factor into “other”. We will use fct_reorder() to reorder the princess values based on the votes values. fct_reorder() takes two main arguments: a) the factor (or column) whose levels you want to modify, and b) a numeric vector (or column of values) that you want to use to reorder the levels. fct_reorder(what_you_are_reordering, the_col_to_base_it_on) 9.7 Reorder princess While we could do this in the ggplot code, I find it’s easiest to do in the data frame as we shape our data. So, go back up to where we made the princes_count data frame and add a new pipe like this: princess_count &lt;- survey %&gt;% count(princess) %&gt;% rename(votes = n) %&gt;% arrange(desc(votes)) %&gt;% mutate(princess = fct_reorder(princess, votes)) # this line reorders the factors The data frame won’t look any different but if you re-run the ggplot code chunk, you graphic will be reordered. Popular princess 9.8 Factors recap Factors in R allow us to apply “levels” to sort categorical data into a logical order beyond alphabetical. If you are building a graphic that uses a categorical column as one of your aesthetics, then you might need to reorder the factor using fct_reorder() or one of the other functions. It’s easiest to do that using dplyr’s mutate() function on your data frame before you plot. 9.9 Practice: Make an ice cream chart Make the same chart as above, but using the ice_cream counts. Order the column chart by the most popular ice cream. 9.9.1 Turn in this project Save, knit and zip the project and turn it into the Canvas assignment for “Factors”. 9.10 Resources These resources can help you understand the concepts. This post on Reordering a variable in ggplot helped me understand how to reorder factors for graphics. Hadley Wickam’s R for Data Science has a Chapter on factors. For those who really want to learn more about them later. "],
["joins.html", "Chapter 10 Joins 10.1 Goals for this section 10.2 Project setup 10.3 The story 10.4 Explore the data 10.5 About joins 10.6 Build our scatterplot 10.7 How does Texas compare? 10.8 Correlaton test for Texas 10.9 Practice 1: Compare Penn State 10.10 Practice 2: Penalties vs scoring 10.11 Using bind_rows() to merge data sets", " Chapter 10 Joins 10.1 Goals for this section Learn how to join two files based on a common column Explore correlations between two numerical values Practice showing relationships with scatterplots Learn how to merge two files one top of each other This lesson is heavily cribbed from a lesson in Matt Waite’s Sports Data Journalism course at the University of Nebraska. 10.2 Project setup Create your project. Call it yourname-football. Create a data-raw folder so you have a place to download the files. Start a new notebook. 10.2.1 Download the data You can download the files we are using based on their URLs in a Github repository. Once you’ve run this block of code, comment out the three download.file() lines by putting a # at the beginning. You really only need to download the files once. # Downloads the files. Convert to comments once you&#39;ve done this: download.file(&quot;https://github.com/utdata/rwd-r-assignments/blob/master/football-compare/penalties.csv?raw=true&quot;, &quot;data-raw/penalties.csv&quot;) download.file(&quot;https://github.com/utdata/rwd-r-assignments/blob/master/football-compare/scoring_offense.csv?raw=true&quot;, &quot;data-raw/scoring_offense.csv&quot;) download.file(&quot;https://github.com/utdata/rwd-r-assignments/blob/master/football-compare/third_down_conversion.csv?raw=true&quot;, &quot;data-raw/third_down_conversion.csv&quot;) Import the three files: # import the files scoring &lt;- read_csv(&quot;data-raw/scoring_offense.csv&quot;) %&gt;% clean_names() thirddown &lt;- read_csv(&quot;data-raw/third_down_conversion.csv&quot;) %&gt;% clean_names() 10.3 The story The data we are using comes from cfbstats, a website for college football statistics. We will be comparing how third-down conversions might correlate to a football team’s scoring offense. 10.4 Explore the data We have two data sets here. 10.4.1 Third-down conversions year: Year. Goes from 2009-2018 name: Team name. There are 131 different teams. g: Number of games played attempts: Third-down attempts conversions: Third-down attempts that were successful conversion_percent: conversions/attempts * 100 10.4.2 Scoring year: Year name: Team name g: Number of games played td: Touchdowns fg: Field goals x1xp: 1pt PAT made x2xp: 2pt PAT made safety: Safeties points: Total points scored points_g: Points per game Now our goal here is to compare how the conversion_percent might relate to points_g for all teams, and how specific teams might buck the national trend. 10.5 About joins To make our plot, we need to join the two data sets on common fields. We want to start with the scoring data frame, and then add all the columns from the thirddown data frame. We want to do this based on the same year and team. There are several types of joins. We describe these as left vs right based on which table we reference first (which is the left one). In the figure below, we can see which matching records are retained based on the type of join we use. Types of joins In our case we only want records that match on both year and name, so we’ll use an inner_join(). The syntax works like this: new_dataframe &lt;- *_join(first_df, second_df, by = field_name_to_join_on) If you want to use more than one field in the by part like we do, then you define them in a concatenated list: by = &quot;field1&quot;, &quot;field2&quot;). If the fields you are joining on are not named the same, then you can define the relationships: by = c(&quot;a&quot; = &quot;b&quot;). For our project we want to use an inner_join(). Add the code below to your notebook along with notes describing that you are joining the two data sets: offense &lt;- inner_join(scoring, thirddown, by=c(&quot;year&quot;, &quot;name&quot;)) # peak at the new data frame offense %&gt;% head() So, to break this down: Our new combined dataframe will be called offense. We’ll be doing an inner_join(), which is just keep matching records. (They all match, FWIW.) Our “left” table is scoring and our “right” table is thirddown. We are joining on both the year and name columns. Anytime you do a join (or a bind as described below), check the resulting number of rows and columns to make sure they pass the logic test. 10.6 Build our scatterplot We’re trying to show the relationship between conversion_percent and points_g, so we can use those as our x and y values in a geom_point() graphic. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point() First scatterplot 10.6.1 Add a fit line We can see by the shape of the dots that indeed, as conversion percentage goes up, points go up. In statistics, there is something called a fit line – the line that predicts what happens given the data. There’s lot of fit lines we can use but the easiest to understand is a straight line. It’s like linear algebra – for each increase or decrease in x, we get an increase or decrease in x. To get the fit line, we add geom_smooth() with a method. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point() + geom_smooth(method=lm, se=FALSE) # adds fit line First scatterplot The lm means linear method. The se=FALSE removes the confidence interval (based on the standard error) of the prediction. See the geom_smooth() for more information. 10.6.2 Run a correlation test So we can see how important third down conversions are to scoring. But still, how important? For this, we’re going to dip our toes into statistics. We want to find out the correlation coefficient (specifically the Pearson Correlation Coefficient. That will tell us how related our two numbers are. We do that using cor.test(), which is part of R core. cor.test(offense$conversion_percent, offense$points_g) The result is: Pearson&#39;s product-moment correlation data: offense$conversion_percent and offense$points_g t = 30.82, df = 1251, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.6242939 0.6873331 sample estimates: cor 0.6569602 That bottom number is the key. If we square it, we then know exactly how much of scoring can be predicted by third down conversions. (0.6569602 * 0.6569602) * 100 Which gets us 43.15967. So what that says is that 43 percent of a team’s score is predicted by their third down percentage. That’s nearly half. In social science, anything above 10 percent is shockingly predictive. So this is huge if this were human behavior research. In football, it’s not that surprising, but we now know how much is predicted. 10.7 How does Texas compare? Let’s compare how Texas does vs the field by plotting their results on top of the national stats. Create a data frame of the Texas data. tx &lt;- offense %&gt;% filter(name == &quot;Texas&quot;) And now we’ll add some layers to our ggplot graphic. We’re also editing our original geom_point() and geom_smooth() layers to make them light grey so that the Texas plots stand out more. offense %&gt;% ggplot(aes(x = conversion_percent, y = points_g)) + geom_point(color = &quot;light grey&quot;) + # adds light grey color geom_smooth(method=lm, se=FALSE, color = &quot;light grey&quot;) + # adds light grey color geom_point(data = tx, aes(x = conversion_percent, y = points_g), color = &quot;#bf5700&quot;) + # adds Texas points colored burnt orange, of course geom_text(data = tx, aes(x = conversion_percent, y = points_g, label = year)) # adds year labels to see Adding Texas to plot This is good, but the labels for the year are sitting on top of the values. There is an R package called ggrepel that will move those labels off the numbers, which we’ll use with our next set of changes. You might have to run install.packages('ggrepel') to make this work. For this update, we are doing a number of things, adding or modifying layers along the way: Add a goem_smooth() fit line specific to Texas, in burnt orange. We’ll put it before the text so it shows underneath the labels. Modify the geom_text to geom_text_repel to move the labels off the points. Add labs() for a title and such to finish out our graphic. Add theme_minimal() just to improve the looks. ggplot(aes(x = conversion_percent, y = points_g)) + geom_point(color = &quot;light grey&quot;) + geom_smooth(method=lm, se=FALSE, color = &quot;light grey&quot;) + geom_point(data = tx, aes(x = conversion_percent, y = points_g), color = &quot;#bf5700&quot;) + geom_smooth(data = tx, aes(x = conversion_percent, y = points_g), method=lm, se=F, color = &quot;#bf5700&quot;) + geom_text_repel(data = tx, aes(x = conversion_percent, y = points_g, label = year)) + labs(x=&quot;Third-down conversion rate&quot;, y=&quot;Points per game&quot;, title=&quot;Texas&#39; third down success predicts scoreboard&quot;, subtitle=&quot;In 2018 the Longhorns were 18th in the FBS for third down conversions.&quot;, caption=&quot;Source: NCAA&quot;) + theme_minimal() Finished Texas graphic 10.8 Correlaton test for Texas It looks like Texas tracks pretty much along the national average. Let’s do the correlation test for Texas just to compare. cor.test(tx$conversion_percent, tx$points_g) Which yields a correlation of 0.685967. Let’s see how much third-down conversions predict Texas’ scoring per game. (0.685967 * 0.685967) * 100 Which gets us 47.1%, not too far from the national average of 43.2%. 10.9 Practice 1: Compare Penn State Not every team tracks the national average like Texas. Tell me (and show me) how Penn State performs in this same third down conversion vs scoring metric by creating a similar graphic and correlation test from the Penn State data on top of the national data. 10.10 Practice 2: Penalties vs scoring How predictive are penalty yards per game on points per game? Do more disciplined teams score more points than undisciplined ones? How does Texas compare to the rest of the league? Create a new RNotebook to answer these questions. You will need to join penalty data (you’ve already downloaded the file data-raw/penalties.csv file) to the same points-per-games statistics from scoring offense. Make your own scatterplot with a fit line to show the relationships between penalty yards per game vs points per game. Run a correlation test for both the national average and for Texas. What does it say? Write a sentence that explains this to a reader. 10.11 Using bind_rows() to merge data sets We won’t go through an example or do practice sessions, but you should be aware that you can also merge data sets on top of each other when your columns are the same. Let’s say you have a multiple data sets where each year is broken into a different file or data frame. You can “stack” data frames on top of each other with a tidyverse function called bind_rows(). When row-binding, columns are matched by name, and any missing columns will be filled with NA. An example might look like this … Let’s say you have three years of data in three different data frames: fy2016, fy2017, fy2018. And all three data frames have the same column names: donor_type, date, amount. And each data frame has 1000 rows of data. If you want then all in the same file you would do this: combined &lt;- bind_rows(fy2016, fy2017, fy2018) The new data frame combined would have all the same columns, but would have 3000 rows of data. If you needed to know which data frame each row came from, you can name a “group” for each data frame, and then merge them. We will name our groups for each year they come from. combined &lt;- bind_rows( &quot;2016&quot; = fy2016, &quot;2017&quot; = fy2017, &quot;2018&quot; = fy2018, .id = &quot;year&quot; ) You would end up with a new data frame called combined, but it would have four columns: year, donor_type, date, amount. It would have all 3000 rows. All the rows that were pulled from fy2016 would have a year of “2016”, and so on. It is admittedly weird that you name the groups before you specify the data frame the come from, but specify .id before the name of your new column. ¯\\_(ツ)_/¯ "],
["census.html", "Chapter 11 Census 11.1 Goals of this section 11.2 Census Bureau programs 11.3 Using the new data portal 11.4 Using the Census API in R 11.5 Interactive maps with leaflet 11.6 Resources", " Chapter 11 Census THIS IS V0.1 AND NEEDS A GOOD EDIT NEED TO PUBLISH AND LINK TO FULL EXAMPLES The U.S. Census Bureau has a wealth of data that can help journalists tell stories. This chapter is not a comprehensive guide on how to use it, but instead an introduction on some says you can. But we can’t talk about specifics of how do to anything before learning about the different Census programs, but this is a mini overview. 11.1 Goals of this section Introduce three of the more popular Census data sets and how they differ: Decennial Census American Community Survey Population Estimates Program Discuss differences between American FactFinder and data.census.gov. Introduce and demonstrate packages that use the Census Bureau’s API to pull data into R. Introduce packages that allow static and interactive mapping within R, which often comes into play when using Census data. None of these topics are comprehensive. I have lectured on the census in the past and have multiple lessons using different software if you want more general instruction, or you could just dive in and gain your own experience. 11.2 Census Bureau programs The Census Bureau has a series of different data “programs” and data sets, many with their own distribution platforms. They are currently in a development effort to combine distribution of all of them into a new, combined platform: data.census.gov. It is a work in progress. That said, the plan is to stop publishing new data to the much-bemoaned American FactFinder, their “old” site, this summer of 2019. As such, I’ll concentrate on that. That’s to say that while I’m outlining three different programs here, the data is found in the same, well, places. Your default should be to use data.census.gov first, then go to American FactFinder if you can’t find something. We’ll also discuss some R packages that use the Census API, the same system that powers data.census.gov. 11.2.1 Decennial Census Every ten years, the government tries to count everyone in every household in America. The results are used to redraw Congressional Districts, allocate tax dollars and a million other reasons. The number of questions asked each decade has been condensed to center around how many people live in a household, their age, race and ethnicity, their relationship to each other, and if they own or rent the home. The decennial census data is the most accurate snapshot you can get for a single point in time. It’s just limited in the scope of data. Most decennial data has been migrated to the data.census.gov data portal. April 1, 2020 is Census Day for the next count. 11.2.2 American Community Survey The American Community Survey, or ACS, is the method the government uses to collect more detailed data than who lives where. It is quite extensive, with information relevant to almost any beat in Journalism. While the decennial count comes once every ten years, the ACS has been continuous since 2005. During the year a sample of about 1.7% of Americans answer the surveys, and those results are distributed at regular intervals each year. This is fine for large geographic areas with 65,000 people or more. But, when you want results for smaller geographic areas, the bureau combines five years of survey results together, known as a 5-year set. It’s a trade off: New data each year for large areas, but fuzzy 5-year windows for small areas. Each estimate also comes with a “margin of error”, which represents the upper and lower bounds of the estimate with 90% confidence rate. Read more about this in the ACS media guide, but here is a brief example: If an estimate is 2,000 with an MOE of +/- 100, that means that you could take 10 new random samples, and the average of those samples would be between 1,900 and 2,100 for nine out of the 10 samples. As journalists, we don’t typically report the margin of error each time we use an estimate, but we do make sure the MOE is not more than 10% of our estimate if we are basing our story on that number. The ACS allows us to get the most detailed characteristics about our population, like how we travel to work or how much of our income we spend on rent. However, it comes with a degree of uncertainty, especially for smaller geographic areas. It is still very valuable. Most of these tables are available on data.census.gov, with the rest scheduled to be ported this year (2019). New releases will only be published to the data portal. 11.2.3 Populaton Estimates Program PEP uses current data on births, deaths, and migration to calculate population change since the most recent decennial census. Each year they publish table with estimates of population, demographics, and housing units for cities, counties and metro areas. Populations estimates are a great way to see population and demographic changes for large areas. It does not have a margin of error like the ACS because it is based on actual data and not random surveys of a part of the population. These tables are currently only available in FactFinder, but should be ported to the new data.census.gov data portal this year (2019). 11.3 Using the new data portal Again, the data.census.gov data portal is under development and has some known challenges, but it’s the one you should learn first since FactFinder will be retired this year. It is slow, cumbersome and error prone but getting better with each new release. Instead of me writing out directions, I recommend you watch this webinar that demonstrates how to use the portal, outlines the current challenges and development plans for the future. I will provide brief demos of both the data portal (and FactFinder) in class, just so you can see them. 11.3.1 Tips about using downloaded portal data in R When you download a table from the portal, you get a stuffed archive with three files. Here is an example from a 5-year ACS data set for table B19013, which includes median income data: ACSDT5Y2017.B19013_data_with_overlays_2019-04-20T000019.csv is the data. It contains two header rows (arg!) with the first row being coded values for each column. The second row has long descriptions of what is in each column. ACSDT5Y2017.B19013_metadata_2019-04-20T000019.csv is a reference file that gives the code and description for each header in the data. ACSDT5Y2017.B19013_table_title_2019-04-20T000019.txt is a reference file with information about the table. If data is masked or missing, this file will explain the symbols used in the data to describe how and why. The first part of the file names include the program, year and table the data comes from. At the end of the filename is the date and time the data was downloaded from the portal. 11.3.2 Importing downloaded data When I import this data into R, I typically use the read_csv() function and skip the first, less-descriptive row. The second row becomes the headers, which are really long but explain the columns. I then rename them to something shorter. If you are only using selected columns, then you might use select() to get only those you need. Here is an example: tx_income &lt;- read_csv(&quot;data-raw/ACSDT5Y2017.B19013_2019-04-20T000022/ACSDT5Y2017.B19013_data_with_overlays_2019-04-20T000019.csv&quot;, skip = 1) %&gt;% rename( median_income = `Estimate!!Median household income in the past 12 months (in 2017 inflation-adjusted dollars)`, median_income_moe = `Margin of Error!!Median household income in the past 12 months (in 2017 inflation-adjusted dollars)` ) Which yields this: id Geographic Area Name median_income median_income_moe 0500000US48199 Hardin County, Texas 56131 3351 0500000US48207 Haskell County, Texas 43529 6157 0500000US48227 Howard County, Texas 50855 2162 11.3.3 Fields made to join with other data Pay attention to fields named id or geoid or similar nanmes as these are often fields meant to be joined to other tables. They use a FIPS code that define specific geographic areas, and allow you to match similar fields in multiple data sets. This is especially important when it comes to mapping data, as these codes are how you join data to “shape files”, which are a data representation of geographic shapes for mapping. Again, we aren’t going to go into a lot of detail about maps in this lesson, but I’ll show some examples in class. You might also find you want to join data based on geography names, in which case you might need to use dplyr tools to split and normalize those terms so they match your other data set, like changing “Travis County, Texas” to just “Travis”. 11.4 Using the Census API in R You can also import data into R directly from the Census Bureau using their API. There are a number of packages that do this, and they all work a little differently to solve different challenges. Again, I won’t go into great detail about how to use these, but I’ll show examples and provide links to further self-study. Manually downloading census data is usually a multiple-step and multiple-decision process. An advantage to using the API is you can script that decision-making process for consistency. Different packages also provide the data in different formats, which might be beneficial to your goal. 11.4.1 Setting up an API key The Census requires a free API key to use their service. It’s like your personal license, and should not be shared with others. You can sign up for an API key here and then store it on your machine in an .Rnviron file so you don’t have to display it in your code. You only have to set up your API key once on each machine. Once installed, it gets automatically loaded when you restart R. 11.4.2 The censusapi package Hannah Recht of Bloomberg News developed the censusapi to pull data directly from the Census Bureau into R. See the site for examples and documentation on use. This example pulls the same data set we manually downloaded from the portal. We are asking for the median income estimate (B19013_001E) and margin of error (B19013_001M), but we are also getting the total population for the county with B01003_001E, which was not in the “B19013” table. This is another advantage to the API, as we are pulling from multiple tables as once. To do this manually, we would have to pull two separate data sets and merge them. tx_income &lt;- getCensus(name = &quot;acs/acs5&quot;, vintage = 2017, vars = c(&quot;NAME&quot;,&quot;B01003_001E&quot;, &quot;B19013_001E&quot;, &quot;B19013_001M&quot;), region = &quot;county:*&quot;, regionin = &quot;state:48&quot;) Which ends up looking like this: state county NAME B01003_001E B19013_001E B19013_001M 48 199 Hardin County, Texas 55993 56131 3351 48 207 Haskell County, Texas 5806 43529 6157 48 227 Howard County, Texas 36491 50855 2162 See how this is similar to the data we imported from file we downloaded from the data portal? In this case we have a state and county field instead of the id, but the shape of the data is the same. Both are useable. The API takes some effort to learn, but the exact “steps” to get the data are recorded in your code. 11.4.3 The tidycensus package Kyle Walker is a professor at TCU who developed the tidycensus package to return census data in tidyverse-ready data frames. He also includes an option to pull the geometry, which I’ll show later. With tidycensus we don’t have to specify to get the MOE with his get_acs() function, it just comes. We only supply the two fields we want, the population and the median income. tx_income &lt;- get_acs( geography = &quot;county&quot;, variables = c(&quot;B01003_001&quot;,&quot;B19013_001&quot;), state = &quot;TX&quot; ) GEOID NAME variable estimate moe 48001 Anderson County, Texas B01003_001 57747 NA 48001 Anderson County, Texas B19013_001 42313 2337 48003 Andrews County, Texas B01003_001 17577 NA 48003 Andrews County, Texas B19013_001 70753 6115 48005 Angelina County, Texas B01003_001 87700 NA 48005 Angelina County, Texas B19013_001 46472 1452 Note how the resulting data is different shape here. Instead of the table getting wider for each variable added, it gets longer. This is a more “tidy” shape that can potentially be easier to plot or map. 11.4.4 Adding geometry to tidycensus The tidycensus package also allows you to download the geometry or shapes of your data at the same time by adding geometry = TRUE to your tidycensus call. This allows you to quickly make static maps of your data. I’m removing the populaton variable because we don’t need it for the map. tx_income_map &lt;- get_acs( geography = &quot;county&quot;, variables = c(&quot;B19013_001&quot;), state = &quot;TX&quot;, geometry = TRUE # gets shapes ) Now we can use geom_sf() to plot the estimate value to each county in shapefile in ggplot. ggplot(tx_income_map) + geom_sf(aes(fill=estimate), color=&quot;white&quot;) + theme_void() + theme(panel.grid.major = element_line(colour = &#39;transparent&#39;)) + scale_fill_distiller(palette=&quot;Oranges&quot;, direction=1, name=&quot;Median income&quot;) + labs(title=&quot;2017 median income in Texas counties&quot;, caption=&quot;Source: Census Bureau/ACS5 2017&quot;) Which yields this: Map from tidy census If you already have the Census data, or perhaps data that is not from the census but has a county name or one of the other geographic code values, then you can use Walker’s tigris package to get just the shapefiles. 11.5 Interactive maps with leaflet Here is a tutorial that walks through creating an interactive map of median income by census tract using the leaflet, mapview, tigris and acs packages. It’s a pretty basic map best used for exploration, but it’s pretty neat and not too hard to make. 11.6 Resources Some othere resources not already mentioned: R Census guide Sharon Machlis guide Thematic maps tutorial. Baltimore Sun example story and code. Christine Zhang says “Sometimes I prefer the output of one over the other ( censusapi vs tidycensus) which is why I alternate. Spatial Data Science with R Tutorial. Not a tutorial but but this post by Timo Grossenbacher is an explanation and inspiration on how far you can take R in mapping. "],
["how-to-tackle-a-new-dataset.html", "Chapter 12 How to tackle a new dataset 12.1 Start by listing questions 12.2 Understand your data 12.3 Counting and aggregation 12.4 Cleaning up categorical data 12.5 Time as a variable 12.6 Explore the distributions in your data", " Chapter 12 How to tackle a new dataset For those unfamiliar with exploring data, starting the process can be paralyzing. How do I explore when I don’t know what I’m looking for? Where do I start? Every situation is different, but there are some common techniques and some common sense that you can bring to every project. 12.1 Start by listing questions It’s likely you’ve acquired data because you needed it to add context to a story or situation. Spend a little time at the beginning brainstorming as list of questions you want to answer. (You might ask a colleague to participate: the act of describing the data set will reveal questions for both of you.) I like to start my RNotebook with this list. 12.2 Understand your data Before you start working on your data, make sure you understand what all the columns and values mean. Look at your data dictionary, or talk to the data owner to make sure you understand what you are working with. To get a quick summary of all the values, you can use a function called summary() to give you some basic stats for all your data. Here is an example from the Top 100 Billboard data we used in a class assignment. Summary of billboard data A summary() will show you the data type for each column, and then for number values it will show you the min, max, median, mean and other stats. 12.3 Counting and aggregation A large part of data analysis is counting and sorting, or filtering and then counting and sorting. It’s possible you may need to reshape your data using gather() or spread() before you can do the mutating or grouping and summarizing you need. Review the Tidy data chapter for more on that. 12.3.1 Counting rows based on a column If you are just counting the number of rows based on the values within a column (or columns), then count() is the key. When you use count() like this, a new column called n is created to hold the count of the rows. You can then use arrange() to sort the n column. (I’ll often rename n to something more useful. If you do, make sure you arrange() by the new name.) In this example, we are counting the number of rows for each princess in our survey data, the arranging by 'n then in decending order. survey %&gt;% count(princess) %&gt;% arrange(n %&gt;% desc()) princess n Mulan 14 Rapunzel (Tangled) 7 Jasmine (Aladdin) 6 Ariel (Little Mermaid) 5 Tiana (Princess and the Frog) 2 Aurora (Sleeping Beauty) 1 Belle (Beauty and the Beast) 1 Merida (Brave) 1 Snow White 1 12.3.2 Sum, mean and other aggregations If you want to aggregate values in a column, like adding together values, or to find a mean or median, then you will want to use group_by() on your columns of interest, then use summarize() to aggregate the data in the manner you choose, like sum(), mean() or the number of rows n(). Here is an example where we use group_by and summarize() to add together values in our mixed beverage data. In this case, we had multiple rows for each name/address group, but we wanted to add together total_receipts() for each group. receipts %&gt;% group_by(location_name, location_address) %&gt;% summarize( total_sales = sum(total_receipts) ) %&gt;% arrange(desc(total_sales)) location_name location_address total_sales WLS BEVERAGE CO 110 E 2ND ST 35878211 RYAN SANDERS SPORTS 9201 CIRCUIT OF THE AMERICAS BLVD 20714630 W HOTEL AUSTIN 200 LAVACA ST 15435458 ROSE ROOM/ 77 DEGREE 11500 ROCK ROSE AVE 14726420 THE DOGWOOD DOMAIN 11420 ROCK ROSE AVE STE 700 14231072 The result will have all the columns you included in the group, plus the columns you create in your summarize statement. You can summarize more than one thing at a time, like the number of rows numb_rows = n() and average of the values average = mean(column_name). 12.3.3 Creating columns to show difference Sometimes you need to perform math on two columns to show the difference between them. Use mutate() to create the column and do the math. Here’s a pseudo-code example: new_or_reassigned_df &lt;- df %&gt;% mutate( new_col_name = (part_col / total_col) * 100 ) 12.4 Cleaning up categorical data If you are going to count our summarize rows based on categorical data, you might want to make sure the values in that column are clean and free of typos and values that might be better combined. In class we did this with the proposed_use column in our wells data in the Cleaning chapter. Review that chapter for examples. Some strategies you might use: Create a count() of the column to show all the different values and how often they show up. You might want to use mutate() to create a new column and then update the values there. Again, see the Cleaning chapter for examples. If you find you have hundreds of values to clean, then come see me. There are some other tools like OpenRefine that you can learn farily quickly to help. 12.5 Time as a variable If you have dates in your data, then you almost always want to see change over time for different variables. Summarize records by year or month as appropriate and create a Bar or Column chart to show how the number of records for each time period. Do you need to see how different categories of data have changed over time? Consider a line chart that shows those categories in different colors. If you have the same value for different time periods, do might want to see the change or percent change in those values. You can create a new column using mutate() to do the math and show the difference. Do you need the mean (average), median or sum of a column, or certain values within columns? The the group_by() and summarize() functions are likely your tool to discover those values. 12.6 Explore the distributions in your data We didn’t talk about histograms in class, but sometimes you might want see the “distributon” of values in your data, i.e. how the values vary within the column. Are many of the values similar? A histogram can show this. Here is an example of a histogram from our wells data exploring the borehole_depth. Each bar represents the number of wells broken down in 100ft depth increments (set with binwidth=100). So the first bar shows that most of the wells (more than 7000) are less than 100 feet deep. wells %&gt;% ggplot(aes(x = borehole_depth)) + geom_histogram(binwidth = 100) Borehole depth histogram While there are wells deeper than 1000 feet, they are so few they don’t even show on the graphic. You’ll rarely use a histogram as a graphic with a story because they are more difficult to explain to readers. But they do help you to understand how much values differ within a column. 12.6.1 More on histograms If you google around, you might see other ways to create a histogram, including hist() and qplot(). You might stick with the ggplot’s geom_histogram() since you already are familiar with the syntax. Tutorial on histograms using ggplot from DataCamp. R Cookbook on histograms. "],
["publishing-your-findings.html", "Chapter 13 Publishing your findings 13.1 Examples 13.2 How to publish your projects 13.3 Making branded graphics", " Chapter 13 Publishing your findings A work in progress Exploring a data set in R is different than publishing your findings. When you are exploring, you will ask a lot of questions and create a lot of plots that will lead to nothing. That is OK. Keep them, and write notes to your future self why you think each plot is NOT interesting, so when you return to it later you’ll see you already studied that path. This is a document for your current and future self. But once you’ve done all your exploring and FOUND things – findings that become sentences in your story – you may want to (you should) create a new notebook that focuses on these findings and how they relate to the story. You can use this more formal R notebook as a way to explain to readers and others the specific ways you came to your conclusions. This is a document for the public, and should be written as such. Get editing help to make sure you are clear and concise in your writing. Some things to consider: Include links to the original data. Explain what it is and how it applies to your story. Include a link to the published story if you have it. Include data dictionaries or similar as files or links so others can see and use the same materials you used to understand the data. If that understanding came through interviews, explain that and include the sources when possible. If you are pulling from a dataset that will change over time, include the dates you pulled the data. Save a copy of the raw data you used in your final version of the story, if possible, and explain that the data may change as records are added to the original. If you cleaned or modified to the data, include those steps so they can be repeated. It is sometimes useful to split those steps into a separate notebook and export/import (as an .rds file) into subsequent notebooks. For each finding, use a headline and text to explain the code block that follows. After the code block, write out your finding and how it relates to your story. Unless there is a good reason not to, stick with the findings you actually used in your story. Don’t waste readers’ time going down paths that were not reported. 13.1 Examples Not all of these examples are in R. Some are done in Python, but the theory is the same. Buzzfeed lists all their data analysis in a special Github repo. Their computational journalists use both R and Python, depending on the author or project. The L.A. Times also publishes a list of their data analysis projects in their Github account. Most of their analysis is done in Python (usually in files that end with .ipynb) and the depth of the annotations vary. The Washington Post Investigative team has begun hosting analysis from their stories in their their Github repo. Statesman A Question of Restraint Demolition permits in Austin Baltimore Sun Data Desk Trend CT SRF Data, the data investigative unit of a Swiss TV/Radio network, has robust documentation of their R analysis for stories. 13.2 How to publish your projects Needs expounding: what is the HTML file Explain about the HTML file and how it differs from the .Rmd file. Explain Advantages to separate Github repos Use chunk options to hide output or code that is not relevant to the reader. 13.2.1 Using Github Pages Needs research and expounding If you are saving your projec to Github, you can set your RMarkdown documents to knit the HTML versions of your documents into a /docs/ folder. As such, you can use Github Pages to publish your docs folder. In the metadata for your RMarkdown document, include the output lines outlined below. --- title: &quot;R Notebook&quot; output: html_document: df_print: paged knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = &quot;docs&quot;) }) --- 13.3 Making branded graphics We haven’t explored many ways to change the theme or looks of our ggplot graphics, but here are some notes worth exploring. ggplot themes can be added with one line. Branding ggplot graphs Note this article about BBC using R, ggplot. BBC created the bblot package to set BBC default styles, and BBC R cookook as a collection of tips and tricks to build their styled graphics. It’s just an example of you can customize R graphics. "],
["chart-examples.html", "Chapter 14 Chart examples 14.1 Resources 14.2 Bar charts 14.3 Column chart 14.4 A line chart 14.5 Scatterplot 14.6 Histogram 14.7 Titles, labels and other cleanup 14.8 Interactivity with plotly", " Chapter 14 Chart examples Under construction This will be a annotated list of chart code and examples from this book. It does not include the construction of the data frame used in the plot. (I’ll try to go back and add links.) 14.1 Resources In addition to the examples below, you might also look at these resources: The R Graph Gallery R Graphics Cookbook Plotly R, especially the Plotly ggplot2 Library ggplot themeshttps://ggplot2.tidyverse.org/reference/ggtheme.html and R Graph theme list 14.2 Bar charts ggplot(wells_by_county, aes(x = county, y = wells_count)) + geom_bar(stat = &quot;identity&quot;) + geom_text(aes(label=wells_count), vjust=-0.45) + labs(title = &quot;Wells by county&quot;, x = &quot;&quot;, y = &quot;Number of wells drilled&quot;) Wells by county with title 14.2.1 Explanaton This is a geom_bar() using stat=&quot;identity&quot;. See below for a similar one using geom_col() which assumes the identity of the category. The geom_text() line adds the numbers to the top of the bars. vjust moves those number up vertically. The labs() add the title and modifies the x and y labels. The x value is set to blank because the county labels on the bar is enough. 14.3 Column chart This is the same as above, but using geom_col which inheriently understands the stat=&quot;identity&quot; problem. ggplot(wells_by_county, aes(x = county, y = wells_count)) + geom_col() + geom_text(aes(label=wells_count), vjust=-0.45) + labs(title = &quot;Wells by county&quot;, x = &quot;&quot;, y = &quot;Number of wells drilled&quot;) It looks the same as above, and is probably the better choice. 14.4 A line chart ggplot(wells_county_year, aes(x=year_drilled, y=wells_drilled)) + geom_line(aes(color=county)) + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) Wells drilled by county by year 14.4.1 Explanation A geom_line() needs either a group= or a color= to “split” the lines on a category across the graphic. This example does this by setting a specific aes() color value in the geom_line() call. In this example, the title and pretty x and y labels are added with labs() 14.5 Scatterplot ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + # added color aesthetic geom_smooth() MPG: disply vs mpg with class 14.5.1 Explanation This plot uses the mpg data ggplot. It is comparing displ (the size of an engine) to mpg, the miles per gallon of the car. The goem_point() adds a new aesthetic to color the dots based on another value in the data, the class. The geom_smooth() plot adds a line showing the average of the points at each position. This helps you determine if there is a relationshiop between the two variables. 14.6 Histogram To come 14.7 Titles, labels and other cleanup Titles and labels are added as new layers on a graphic. These examples just show the line that adds the new layer. 14.7.1 Title and axis labels yourchartsofar + labs(title = &quot;Wells by county and year&quot;, x = &quot;Year&quot;, y = &quot;Number of wells&quot;) 14.7.2 Flip the graphic Sometime you want to turn a graphic 90 degrees to you can read long values, or make a bar chart horizontal instead of vertical. yourchartsofar + coord_flip() 14.7.3 Fix exponential numbers on an axis yourchartsofar + scale_y_continuous(labels=comma) You can do the same for scale_x_continuous() if needed. 14.8 Interactivity with plotly This requires a library in addition to the tidyverse called library(plotly). The idea is that you: create a ggplot graphic. save that entire graphic to a new R object. Call the ggplotly() function with that new variable. saved_plot &lt;- wells_county_year %&gt;% ggplot(aes(x = year_drilled, y = wells_drilled, color = county)) + geom_line() + geom_point() + labs(title = &quot;Wells drilled per county&quot; ) # shove into ggploty ggplotly(saved_plot) This screen shot below is not interactive, but you can see the hover tool tip displayed. Wells by county with ggplotly "],
["verbs.html", "Chapter 15 Verbs 15.1 Import/Export 15.2 Data manipulation 15.3 Aggregation 15.4 Math", " Chapter 15 Verbs An opinionated list of the most common Tidyverse and other R verbs used with data storytelling. 15.1 Import/Export read_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(&quot;path/to/file.csv&quot;). write_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(&quot;path/to/file.rds&quot;). readxl is a package we didn’t talk about, but it has read_excel() that allows you to import from an Excel file, including specified sheets. clean_names() from the library(janitor) package standardizes column names. 15.2 Data manipulation select() to select columns. Example: select(col01, col02) or select(-excluded_col). rename() to rename a column. Example: rename(new_name = old_name). filter() to filter rows of data. Example: filter(column_name == &quot;value&quot;). See Relational Operators like ==, &gt;, &gt;= etc. See Logical operators like &amp;, | etc. See is.na tests if a value is missing. distinct() will filter rows down to the unique values of the columns given. arrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc()) mutate() changes and existing column or creates a new one. Example: mutate(new_col = (col01 / col02)). gather() collapses columns into two, one a key and the other a value. Turns wide data into long. Example: gather(key = &quot;new_key_col_name&quot;, value = &quot;new_val_col_name&quot;, 3:5) will gather columns 3 through 5. Can also name columns to gather. spread() turns long data into wide by spreading into multiple columns based on as key. 15.3 Aggregation count() will count the number rows based on columns you feed it. group_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. Example: group_by(song, artist) %&gt;% summarize(weeks = n(), top_chart_position = min(peak_position)) 15.4 Math These are the function often used within summarize(): n() to count the number of rows. n_distinct() counts the unique values. sum() to add things together. mean() to get an average. median() to get the median. min() to get the smallest value. max() for the largest. +, -, *, / are math operators similar to a calculator. "],
["notes-about-future-sessions.html", "Notes about future sessions Maps Joins Data packages Tools for PDFs More graphics ideas Matt Waite ideas yet to incorporate", " Notes about future sessions Maps Andrew Tran’s mapping session from NICAR Joins Add this as a short lesson on joining and merging files. Data packages Add a chapter that lists various data packages and such. googledrive to get data from Google Drive. Could do the perhaps do the ice cream and princess graphics. Twitter Slides from a rtweets package workshop. Tools for PDFs @abtran says: In R, there are a handful of packages I cycle through depending on the quality of the PDFs: tabulizer will detect tables, will let you set parameters for where to find the tables, or lets you interactively set the parameters. Another example. sometimes the PDFs need to be OCR’d first, then I use Tesseract’s R bindings. and finally there’s pdftools, which also extracts tables. here’s a video of Hadley Wickham scraping pdfs and turning the data into dataframes More graphics ideas RStudio add in esquisse with UI for easy ggplot graphics. Plotly for interactive charts with hovers. There is also a ggplot port that makes ggplot graphics interactive. It appears use of the open source library is free. When you get into publishing embeddable graphics, there is cost. ggirafe is yet another port to make ggplot interactive. Matt Waite ideas yet to incorporate Uses sports data to make a waffle chart, which is pretty cool. A better way than a pie chart to compare two values, or even multiples of a whole. formattable could be useful. More features than tabyl. "],
["references.html", "References", " References Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 15.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 15.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 15.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 15.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). # download: [&quot;pdf&quot;, &quot;epub&quot;] # bookdown::pdf_book: # includes: # in_header: preamble.tex # latex_engine: xelatex # citation_package: natbib # keep_tex: yes # bookdown::epub_book: default "]
]
