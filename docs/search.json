[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting with Data in R",
    "section": "",
    "text": "About this book\nReporting with Data in R is a series of lessons and instructions used for courses in the School of Journalism and Media, Moody College of Communication at the University of Texas at Austin. The first two versions of the book were written by Associate Professor of Practice Christian McDonald. Assistant Professor Jo Lukito began collaborating and teaching sections of the Reporting with Data in spring 2022 and Anastasia Goodwin is teaching from this material as an adjunct in fall 2023.\nThis means the writing voice or point of view may change or be confusing at times. For instance, Prof. McDonald uses a Mac and Prof. Lukito uses a PC, so keyboard commands and screenshot examples may flip back and forth. We’ll try to note the author and platform at the beginning of each chapter.\nThis version is being revised for fall 2023 version. Earlier versions are linked below."
  },
  {
    "objectID": "index.html#some-words-from-prof.-mcdonald",
    "href": "index.html#some-words-from-prof.-mcdonald",
    "title": "Reporting with Data in R",
    "section": "Some words from Prof. McDonald",
    "text": "Some words from Prof. McDonald\nI’m a strong proponent of what I call Scripted Journalism, a method of committing data journalism that is programmatic, repeatable, transparent and annotated. There are a myriad of programming languages that further this, including Python (pandas using Jupyter) and JavaScript (Observable), but we’ll be using R, Quarto and RStudio.\nR is a super powerful, open-source programming language for data that is deep with features and an awesome community of users who build upon it. No matter the challenge before you in your data storytelling, there is probably a package available to help you solve that challenge. Probably more than one.\nThere is always more than one way to do things in R. This book is a Tidyverse-oriented, opinionated collection of lessons intended to teach students new to programming and R for the expressed act of committing journalism. As a beginner course, we strive to make it as simple as possible, which means we may not go into detail about alternative (and possibly better) ways to accomplish tasks in favor of staying in the Tidyverse and reducing options to simplify understanding. We rarely discuss differences from base R; Tidyverse is our default.\nProgramming languages evolve constantly, and R has seen some significant changes in the past few years, many of them driven by Posit, the company that makes Rstudio and maintains the Tidyverse packages.\n\nThe introduction of Quarto in mid-2022. This modern implementation of RMarkdown is a major shift for this edition of the book. Every lesson and video will need to be updated, and we may not get to all of them. The good news is RMarkdown still works inside Quarto documents.\nThe introduction of the base R pipe |&gt; in 2021. Posit developers began using the |&gt; in favor of the magrittr pipe %&gt;% in 2022, and this book follows their lead. The two implementations work interchangeably and you’ll see plenty of %&gt;% in the wild.\nThe use of YAML code chunk options. I first noticed this style when I started using Quarto in 2023. It might not matter much for this book as we don’t use that many code chunk options in our assignments, but we’ll see."
  },
  {
    "objectID": "index.html#conventions-and-styles-in-this-book",
    "href": "index.html#conventions-and-styles-in-this-book",
    "title": "Reporting with Data in R",
    "section": "Conventions and styles in this book",
    "text": "Conventions and styles in this book\nWe will try to be consistent in the way we write documentation and lessons. But we are human, so sometimes we break our own rules, but in general keep the following in mind.\n\nThings to do\nThings to DO are in ordered lists:\n\nDo this thing.\nThen do this thing.\n\nExplanations are usually in text, like this very paragraph.\nSometimes details will be explained in lists:\n\nThis is the first thing I want you to know.\nThis is the second. You don’t have to DO these things, just know about them.\n\n\n\nCode blocks\nThis book often runs the code that is shown, so you ’ll see the code and the result of that code below it.\n\n1 + 1\n\n[1] 2\n\n\n\nCopying code blocks\nWhen you see R code in the instructions, you can roll your cursor over the right-corner and click on the copy icon to copy the code clock content to your clipboard.\n\nYou can then paste the code inside your R chunk.\nThat said, typing code yourself has many, many benefits. You learn better when you type yourself, make mistakes and have to fix them. We encourage you to always type short code snippets. Leave the copying to long ones.\n\n\nFenced code\nSometimes we need to show code chunk options that are added, like when explaining how to name chunks. In those cases, you may see the code chunk with all the tick marks, etc. like this:\n\n```{r block-named}\n1 + 1\n```\n\n[1] 2\n\n\nor\n\n```{r}\n#| label: block-named-yaml\n\n1 + 1\n```\n\n[1] 2\n\n\nYou can still copy/paste these blocks, but you’ll get the entire code block, not just the contents.\n\n\nHidden code\nSometimes we want to include code in the book but not display it so you can try the to write the code yourself first. When we do this, it will look like this:\n\n\nClick here to show the code\n1 + 1\n\n\n[1] 2\n\n\nIf you click on the triangle or the words that follow, you’ll reveal the code. Click again to hide it.\n\n\n\nNotes, some important\nWe will use callouts to set off a less important aside:\n\n\n\n\n\n\nMarkdown was developed by JOHN GRUBER, as outlined on his Daring Fireball blog.\n\n\n\nBut sometimes those asides are important. We usually indicate that:\n\n\n\n\n\n\nImportant\n\n\n\nYou really should learn how to use Markdown as you will use it the whole semester, and hopefully for the rest of your life."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Reporting with Data in R",
    "section": "About the authors",
    "text": "About the authors\n\nChristian McDonald\nI’m a career journalist who most recently served as data and projects editor at the Austin American-Statesman before joining the University of Texas at Austin faculty full-time in fall 2018. I’ve taught data-related course at UT since 2013. I also serve as the innovation director of the Dallas Morning News Journalism Innovation Endowment.\n\nThe UT Data Github: utdata\nTwitter: crit | Mastodon crit | Bluesky: @crit\nEmail: christian.mcdonald@utexas.edu\n\n\n\nJo Lukito\nI’m an aspiring-journalist-turned-academic who studies journalism and digital media. To make a long story short (tl;dr): I trained to be a journalist as an undergraduate student, but just fell in love with researching and supporting journalism. I completed my Ph.D in 2020, and my dissertation focused on international trade reporting (which relies on plenty o’ data). I also do a ton of social media research (especially in politics and disinformation), so if you’re interested in the social media beat, I’m your gal!\n\nProf. Jo Lukito’s git: jlukito\nTwitter: JosephineLukito\nEmail: jlukito@utexas.edu\nWebsite: https://www.jlukito.com/"
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Reporting with Data in R",
    "section": "Versions",
    "text": "Versions\nThe first version of this book from spring 2019 is buried somewhere in the commit history, but we’ve archived other recent editions:\n\nFall 2021\nSpring 2022\nFall 2022\nSpring 2023"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reporting with Data in R",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nLet’s just say this is free on the internet. We don’t make any money from it and you shouldn’t either."
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Reporting with Data in R",
    "section": "Other resources",
    "text": "Other resources\nThis text stands upon the shoulders of giants and by design does not cover all aspects of using R. Here are some other useful books, tutorials and sites dedicated to R. There are other task-specific tutorials and articles sprinkled throughout the book in the Resources section of select chapters.\n\nR for Data Science\nThe Tidyverse site, which has tons of documentation and help.\nThe RStudio Cheatsheets.\nggplot2: Elegant Graphics for Data Analysis\nR Graphics Cookbook\nThe R Graph Gallery another place to see examples.\nPractical R for Journalism by Sharon Machlis, an editor with PC World and related publications. Sharon is a longtime proponent of using R in journalism.\nSports Data Analysis and Visualization and Data Journalism with R and the Tidyverse by Matt Waite, a professor at the University of Nebraska-Lincoln.\nR for Journalists site by Andrew Tran, a reporter at the Washington Post and University of Texas alum. A series of videos and tutorials on using R in a journalism setting."
  },
  {
    "objectID": "install.html#mac-vs-pc",
    "href": "install.html#mac-vs-pc",
    "title": "1  Install Party",
    "section": "1.1 Mac vs PC",
    "text": "1.1 Mac vs PC\nWe’re a big fan of using keyboard commands to do operations in any program, but Prof. McDonald references this from a Mac perspective and Prof. Lukito references this from a PC perspective. So if we say use Cmd+S or Command+S to save, that translates to Cntl+S or Control+S on a PC. You can typically just switch Cmd (for Mac) and Cntl (for PC), but occasionally there are other differences. You can usually check menu items in RStudio to figure out the command your your computer.\nWe will note the author and operating system at the top chapters so you have a frame of reference."
  },
  {
    "objectID": "install.html#rstudio-vs-posit.cloud",
    "href": "install.html#rstudio-vs-posit.cloud",
    "title": "1  Install Party",
    "section": "1.2 RStudio vs posit.cloud",
    "text": "1.2 RStudio vs posit.cloud\nThis book is written assuming the use of the RStudio IDE application, which is free and available for Macs, PC’s and Linux. In cases where computers have trouble running R and Rstudio, it is possible to to use the online posit.cloud version of RStudio. If you are using posit.cloud for the Reporting with Data class, you’ll likely hit a pay tier at some point. The Cloud Plus plan at $5/mo is typically sufficient. You may find some specific posit.cloud instruction in this book, but we don’t outline every difference.\nWe will install R and RStudio. It might take some time depending on your Internet connection. If you are doing this on your own you might follow this tutorial. But below you’ll find the basic steps."
  },
  {
    "objectID": "install.html#installing-r",
    "href": "install.html#installing-r",
    "title": "1  Install Party",
    "section": "1.3 Installing R",
    "text": "1.3 Installing R\nOur first task is to install the R programming language onto your computer.\n\nGo to the https://cloud.r-project.org/.\nClick on the link for your operating system.\nThe following steps will differ slightly based on your operating system.\n\nFor Macs, you’ll need to know if you have an Apple or Intel chip. Go under the Apple menu to About this Mac and you should be able to see if you have Apple or Intel. You’ll choose which download based on that.\nFor Windows, you want the “base” package. You’ll need to decide whether you want the 32- or 64-bit version. (Unless you’ve got a pretty old system, chances are you’ll want 64-bit.)\n\n\nThis should be pretty self explanatory: once you download the installation file, you should be able to run it on your respective computers to install R.\nYou’ll never “launch” R as a program in a traditional sense, but you need it on your computers (it’s mostly so that the computer can recognize R as a “language”). In all situations (in this class, and beyond), we’ll use RStudio, which is next."
  },
  {
    "objectID": "install.html#install-quarto",
    "href": "install.html#install-quarto",
    "title": "1  Install Party",
    "section": "1.4 Install Quarto",
    "text": "1.4 Install Quarto\nGo to the Quarto Getting Started page and there should be a big blue button that links to the software for your computer. Follow the prompts to install."
  },
  {
    "objectID": "install.html#installing-rstudio",
    "href": "install.html#installing-rstudio",
    "title": "1  Install Party",
    "section": "1.5 Installing RStudio",
    "text": "1.5 Installing RStudio\nRStudio is an “integrated development environment” – or IDE – for programming in R. Basically, it’s the program you will use when doing work for this class.\n\nGo to https://posit.co/downloads/.\nYou’ve already done step 1 to install R. Find step 2.\nThere should be a big blue button to download for your computer.\nInstall it. This should be like installing any other program on your computer.\n\n\n1.5.1 Getting “Git” errors on Macs\nIf later during package installation you get errors that mention “git” or “xcode-select” then say yes! and do it. It might take some time. If it doesn’t finish, then try again the next time it comes up."
  },
  {
    "objectID": "install.html#class-project-folder",
    "href": "install.html#class-project-folder",
    "title": "1  Install Party",
    "section": "1.6 Class project folder",
    "text": "1.6 Class project folder\nTo keep things consistent and help with troubleshooting, we recommend that you save your work in the same location all the time.\n\nOn both Mac and Windows, every user has a “Documents” folder. Open that folder. (If you don’t know where it is, ask us to help you find it.)\nCreate a new folder called “rwd”. Use all lowercase letters.\n\nWhen we create new “Projects”, I want you to always save them in the Documents/rwd folder. This just keeps us all on the same page."
  },
  {
    "objectID": "intro.html#about-quarto-r-and-scripted-journalism",
    "href": "intro.html#about-quarto-r-and-scripted-journalism",
    "title": "2  Introduction to R",
    "section": "2.1 About Quarto, R and scripted journalism",
    "text": "2.1 About Quarto, R and scripted journalism\nBefore we dive into RStudio and programming and all that, I want to show you where we are heading, so you can “visualize success”. As I wrote in the book intro, I’m a believer in Scripted Journalism … data journalism that is repeatable, transparent and annotated. As such, the whole purpose of this book is to train you to create documents to share your work.\nThe best way to explain this is to show you an example.\n\nGo to this link in a new browser window: Major League Soccer salaries.\n\nThis is a website with all the code from a data journalism project. If you click on the navigation link for Cleaning you can read where the data come from and see all the steps I went through – with code and explantion – to process the data so I could work with it. And in the Analysis 2023 notebook you’ll see I set out with some questions for the data, and then I wrote the code to find my answers. Along with the way I wrote explanations of how and why I did what I did.\n\n\n\nQuarto Pub page\n\n\nThis website was created using Quarto and R, and the tool I used to write it was the RStudio IDE. Here’s the crazy part: I didn’t have to write any HTML code, I just wrote my thoughts in text and my code in R. With a few short lines of configuration and a two-word command quarto publish I was able to publish my work on the Internet for free.\nKeep this in mind:\n\nWe can also easily publish the same work in other formats, like PDF, Word or even a slide show.\nWe can also choose NOT to “publish” our work. We don’t have to share our work on the internet, we are just ready if we want to.\n\nCreating shareable work is our goal for every project. Let’s get started."
  },
  {
    "objectID": "intro.html#rstudio-tour",
    "href": "intro.html#rstudio-tour",
    "title": "2  Introduction to R",
    "section": "2.2 RStudio tour",
    "text": "2.2 RStudio tour\n\n\n\n\n\n\nWarning\n\n\n\nIf you are using the online posit.cloud version of RStudio, then some steps have to be done differently, mainly in dealing with project creation and exporting. I’ll try to note when I’m aware of differences, but may not go into great detail in the book. Happy to do so in class.\nIn this case, you need go to your posit.cloud account and then use the blue New Project button to launch a new RStudio project, and then continue below.\n\n\nWhen you launch RStudio, you’ll get a screen that looks like this:\n\n\n\nRStudio launch screen"
  },
  {
    "objectID": "intro.html#updating-preferences",
    "href": "intro.html#updating-preferences",
    "title": "2  Introduction to R",
    "section": "2.3 Updating preferences",
    "text": "2.3 Updating preferences\nThere are some preferences in RStudio that I would like you to change. By default, the program wants to save the state of your work (all the variables and such) when you close a project, but that is typically not good practice. We’ll change that.\n\nGo to the Tools menu and choose Global Options.\nUnder the General tab, uncheck the first four boxes.\nOn the option “Save Workspace to .Rdata on exit”, change that to Never.\nClick Apply to save the change (but don’t close the box yet).\n\n\n\n\nRStudio preferences\n\n\nNext we will set some value is the Code pane.\n\nOn the left options, click on the Code pane.\nCheck the box for Use native pipe operator, |&gt;.\nClick OK to save and close the box.\n\n\n\n\nNative pipe preference\n\n\nWe’ll get into why we did this part later."
  },
  {
    "objectID": "intro.html#the-r-package-environment",
    "href": "intro.html#the-r-package-environment",
    "title": "2  Introduction to R",
    "section": "2.4 The R Package environment",
    "text": "2.4 The R Package environment\nR is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful.\nWhat happens is developers will find it difficult to do a certain task, so they will write code that solves that problem and save it into an R “package” so they can use it later. They share that code with the community, and suddenly the R garage has an “ultimate set of tools” that would make Spicoli’s dad proud.\nOne set of these tools is the tidyverse developed by Hadley Wickham and his team at Posit. It’s a set of R packages for data science that are designed to work together in similar ways. Prof. Lukito and I are worshipers of the tidyverse worldview and we’ll use these tools extensively. While not required reading, I highly recommend Wickham’s book R for data science, which is free.\nThere are also a series of useful tidyverse cheatsheets that can help you as you use the packages and functions from the tidyverse. We’ll refer to these throughout the course.\nWe will use these tidyverse packages extensively throughout the course. We’ll install some other packages later when we need them.\n\n2.4.1 How we use packages\nThere are two steps to using an R package:\n\nInstall the package onto your computer by using install.packages(\"package_name\"). You only have to do this once for each computer, so I usually do it using the R Console instead of in a notebook.\nInclude the library using library(package_name). This has to be done for each notebook or script that uses it, so it is usually one of the first things you’ll see in a notebook.\n\n\n\n\n\n\n\nNote\n\n\n\nYou use “quotes” around the package name when you are installing, but you DON’T need quotes when you load the library.\n\n\n\n\n2.4.2 Install some packages\nWe need to install some packages before we can go further. To do this, we will use the Console, which we haven’t talked about much yet.\n\n\n\nThe Console and Terminal\n\n\n\nUse the image above to orient yourself to the R Console and Terminal.\nCopy the code below and paste it into the Console, then hit Return.\n\ninstall.packages(c(\"quarto\", \"rmarkdown\", \"tidyverse\", \"janitor\"))\nYou’ll see a bunch of response fly by in the Console. It’s probably all fine unless it ends the last response with an error.\nIf you are using the RStudio IDE app on your computer, you only have to do this install.packages() move once. However, if you are using the online posit.cloud version of RStudio, you’ll have to do this for each new project because each project is a new virtual computer. I’ve included a posit.cloud cheetsheet here\n\nOK, we’re done with all the computer setup. Let’s get to work."
  },
  {
    "objectID": "intro.html#starting-a-new-quarto-website",
    "href": "intro.html#starting-a-new-quarto-website",
    "title": "2  Introduction to R",
    "section": "2.5 Starting a new Quarto Website",
    "text": "2.5 Starting a new Quarto Website\n\n\n\n\n\n\nImportant\n\n\n\nThis is a case where posit.cloud differs greatly. See the posit.cloud Appendix to see how to build a new Quarto Website project.\n\n\nWhen we work in RStudio, we will create “Projects” to hold all the files related to one another. This sets the “working directory”, which is a sort of home base for the project.\n\nClick on the second button that has a green +R hexagon sign.\nThat brings up a box to create the project with several options. You want New Directory in this case.\nFor Project Type, choose Quarto Website.\nNext, for the Directory name, choose a new name for your project folder. For this project, use “firstname-first-project” but use YOUR firstname.\nFor the subdirectory, you want to use the Browse button to find your rwd folder we created earlier.\n\nI want you to be anal retentive about naming your folders. It’s a good programming habit.\n\nUse lowercase characters.\nDon’t use spaces. Use dashes.\nFor this class, start with your first name.\n\n\n\n\nQuarto project, directory\n\n\nWhen you hit Create Project, your RStudio window will refresh and two things will happen:\n\nYou’ll see the three files listed in your Files window.\nA new document window will open on the left.\n\n\n2.5.1 The files pane\n\n\n\nNew Quarto Project\n\n\nLet’s walk through the files created and explain what they are in order of importance.\n\nThe _quarto.yml file is a YAML configuration file for your project. This allows us to set publishing rules for our Quarto project. We might not get too much into all the options, but you can read more about it in the Quarto Guide if you like.\nThe index.qmd file is a Quarto document that makes the “home page” of your website. In this book you’ll use the index to describe your project and record other important information related to it. Each Quarto file you create can become a new page in your website, and they all end in .qmd, which stands for Quarto Markdown. (We might also encounter and edit .Rmd files, which are very similar, but just a little less awesome.) The about.qmd file is another Quarto document created as a placeholder. We’ll usually rename/reuse that or delete it.\nThe christian-first-project.Rproj file is your Project file. It sets the working directory or what is essentially “home base” for your project. When you go to open your project again, this is the file you will open.\nstyles.css is file where you can assert extra control on your website. We won’t use it.\n\nThe big thing to remember is this: Most of our work is done in files with the .qmd suffix."
  },
  {
    "objectID": "intro.html#the-quarto-document",
    "href": "intro.html#the-quarto-document",
    "title": "2  Introduction to R",
    "section": "2.6 The Quarto document",
    "text": "2.6 The Quarto document\nThe document that opened on the left is our Quarto document where we will do our work. The Quarto document is a cutting-edge way of authoring programming documents where the result can be output in a myriad of formats: As HTML, PDFs, slideshows, books, etc. In fact, this very book you are reading is written in R using Quarto.\n\n\n\nNew Quarto document\n\n\nI could write reams of text about the birth of Quarto and the evolution of what came before it, but you won’t really care. Let’s break this down to what you need to know:\n\nThese documents allow us to weave together our thoughts and our code. We’ll use Markdown to record our thoughts, and R to write our code.\nWe write the document and we run the code to see the results, but all the while we are creating documents that we will be rendered to share with others. Our goal with these documents is to explain to an “audience” what are doing with our data analysis. Often our most important audience is our future self.\n\nThink of Render like exporting or publishing a pretty version of your work.\n\n2.6.1 Render the document\nLet’s see what this basic document looks like when when we Render it.\n\nIn the toolbar of the document you’ll see a blue arrow with Render after it.\nClick on that button or word.\n\nWhat this should do is open up the Console below the document and you’ll see a bunch of feedback, but on the right side of RStudio. You should end up with the Viewer pane that shows your document. Here is a tour of sorts:\n\n\n\nRender Quarto\n\n\nFor the posit.cloud users, this will launch in a new web browser window. RStudio users can do that if you want though a button in the Viewer toolbar.\n\n\n2.6.2 The metadata\nThe top of our document has what we call the metadata of our document written in YAML. These are commands to control the output of our document. Right now it only has the title of our document.\n---\ntitle: \"christian-first-project\"\n---\nWhen we created our project, RStudio added the name of our folder here, which kinda sucks because that is not what the title is for. It should describe a title for your project like the top of a Microsoft Word or Google Docs document.\n\nEdit the text inside the quotes to be more like a title you would find in a Word or Google Doc document, like this, but your your name:\n\n---\ntitle: \"Christian's First Quarto project\"\n---\n\nRe-render the document so you can see the updates. (You can click on the Render button or do Cmd-Shift-K to update it.)\n\n\n\n2.6.3 R code chunks\nThe next bit of our document shows an R chunk:\n\n\n\nR chunk\n\n\nThis is where we write and execute our programming code. Let’s break down parts of this:\n\nThe three tick marks ` at the beginning and end indicate where the code chunk starts and ends.\nThe {r} part notes that this is R code. (Quarto supports other languages like {python}, but we’ll stick with R.)\nThe 1 + 1 part is the code. In this case, it is some basic math. We do cooler stuff later.\nThe green triangle that points to the right will run all the code in this chunk.\nThe gray triangle that points down with the green bar will run all the code above this chunk.\n\nLet’s run this chunk of code to see what happens.\n\nClick on the green triangle.\n\n\n\n\nCode chunk has run\n\n\nDoing this executes the code that is in the R block and it will print the result to your document below the chunk. You might have noticed something similar when you rendered your document earlier.\n\n\n\n\n\n\nImportant\n\n\n\nThere are about five keyboard commands that I will implore you to learn. Here are the first three. Remember if you are on a PC use Cntl instead of Cmd.\n\nCmd+option+i will insert a code block.\nCmd+Return will run a single line (or selection) of code within a code block.\nCmd+Shift+Return will run a whole code chunk."
  },
  {
    "objectID": "intro.html#lets-do-some-data-analysis",
    "href": "intro.html#lets-do-some-data-analysis",
    "title": "2  Introduction to R",
    "section": "2.7 Let’s do some data analysis",
    "text": "2.7 Let’s do some data analysis\nRemember the goal of all our work here in this class is to explain to use data analysis to make sense of the world and then explain to others what we are doing. There are multiple parts to that:\n\nWhat are we thinking?\nOur code.\nOur thoughts about the results, if any.\n\nOur goal with this notebook is to discover what is the socially-acceptable age to date someone older or younger than ourselves.\n\nCopy the text below. Note you can use the copy-clipboard button in the book if you roll your cursor over the code and click on the clipboard icon at the top right.\nPaste the code at the bottom of your notebook.\nRe-render your document to see what it looks like.\n\n## My upper dating age\n\nThe following section details the [socially-acceptable maximum age of anyone you should date](https://www.psychologytoday.com/us/blog/meet-catch-and-keep/201405/who-is-too-young-or-too-old-you-date).\n\nThe math works like this:\n\n- Take your age\n- subtract 7\n- Double the result\nLet’s walk through this Markdown code. You might bookmark the Markdown basics so you can refer back to it as you learn.\n\nThe ## line is a “Header 2” headline, meaning it is the second biggest. (The title is an H1.) Add more hashmarks ### and you get a smaller headline, like subheads, etc.\nThere is a full blank return between each element, including paragraphs of text. The exception is the bullet list.\nIn the first paragraph we have embedded a hyperlink. We put the words we want to show inside square brackets and the URL in parenthesis DIRECTLY after the closing square bracket: [words to link](https://the_url.org).\nThe - at the beginning of a line creates a bullet list. (You can also use *). Those lines need to be one after another without blank lines.\n\n\n\n\n\n\n\nNote\n\n\n\nI should note at this point there is a “Visual” editor where RStudio gives you more formatted look as your editing as it writes Markdown underneath the hood. I want you to use the “Source” editor so you can see and learn the underlying Markdown syntax. All my examples will be written in “Source” mode. After you pass this class, you can use “Visual” mode.\n\n\n\n2.7.1 Adding new code chunks\nLet’s add a new R chunk to add the code that will calculate the maximum age of someone we should date.\n\nAdd a few blank lines after your Markdown text.\nUse the keyboard command Cmd+Option+i to add an R chunk.\nYour cursor will be inserted into the middle of the chunk. Type or copy this code in the space provided:\n\n\n# update 57 to your actual age\nage &lt;- 57\n(age - 7) * 2\n\n[1] 100\n\n\n\nChange the number to your real age.\nWith your cursor somewhere in the code block, use the key command Cmd+Shift+Return, which is the key command to RUN ALL LINES of code chunk.\n\nCongratulations! The answer given at the bottom of that code chunk is the upper end age of someone socially acceptable for you to date.\nThrowing aside whether the formula is sound, let’s break down the code.\n\n# update 57 to your age is a comment. It’s a way to explain what is happening in the code without being considered part of the code. We create comments inside code chunks by starting with #. You can also add a comment at the end of a line.\nage &lt;- 57 is assigning a number (57) to an R object/variable called (age). A variable is a placeholder. It can hold numbers, text or even groups of numbers. Variables are key to programming because they allow you to change a value as you go along.\nThe next part is simple math: (age - 7) * 2 takes the value of age and subtracts 7, then multiplies by 2.\nWhen you run it, you get below it the result of the math equation, [1] 100 in my case. That means there was one observation [1], and the value was “100”. For the record, my wife is much younger than that. Perhaps this formula breaks down when you get older ¯\\_(ツ)_/¯.\n\nNow you can play with the number assigned to the age variable to test out different ages. Do that.\n\n\n2.7.2 Practice adding code chunks\nNow, I want you to add a similar section that calculates the minimum age of someone you should date, but using the formula (age / 2) + 7.\n\nAdd a Markdown headline and text describing what you are doing.\nUse the keyboard command to create a new code chunk.\nInclude a comment within the code block that explains you are using the age variable already established.\nAdd the math statement (age / 2) + 7 to the chunk.\nRun the chunk and re-render your document.\n\nNow you know the youngest a person should be that you date. FWIW, we don’t recreate the assignment of the age variable since we already have one. A Quarto document is designed to run from top to bottom so that all the pieces work together.\n\n\n2.7.3 The toolbar\nOne last thing to point out in the document window: The toolbar that runs across the top of the document window. The image below explains some of the more useful tools, but you REALLY should learn and use the keyboard commands instead.\n\n\n\nR Notebook toolbar"
  },
  {
    "objectID": "intro.html#a-quick-look-back-at-files",
    "href": "intro.html#a-quick-look-back-at-files",
    "title": "2  Introduction to R",
    "section": "2.8 A quick look back at files",
    "text": "2.8 A quick look back at files\nWe’ve been concentrating on editing the Quarto document, but let’s peek back at the Files pane to note some new files that were created.\n\nIn the window with the viewer, you’ll notice several other tabs: Files, Plots, Packages, etc. Click on the Files tab.\n\n\n\n\nQuarto files list\n\n\nNote there is a new folder there, _site. All your rendered versions to into the folder to make it easy to share or publish them.\nWith our next project, we’ll create some other folders to store our data and such."
  },
  {
    "objectID": "intro.html#publish-to-quarto.pub",
    "href": "intro.html#publish-to-quarto.pub",
    "title": "2  Introduction to R",
    "section": "2.9 Publish to quarto.pub",
    "text": "2.9 Publish to quarto.pub\nLet’s publish our work to the Internet!!!!\n\nGo to quartopub.com and sign up for a free account.\nCome back to RStudio and down by the Console, click on the Terminal 1 pane.\nType in quarto publish and hit Return on your keyboard.\n\nA bunch of stuff will happen in your Terminal pane. Here is a video showing this:\n\n\nVideo\nQuarto pub movie\n\n\nA couple of things about this the video clip above:\n\nThe first time you run this, you’ll be asked to authenticate to quarto.pub, so it will be a little different.\nIn the last bit of that video, a browser window opens with your website. That’s cool!\n\nYou can learn more about Quarto pub here, including how to configure your profile page."
  },
  {
    "objectID": "intro.html#on-your-own-update-the-about-page",
    "href": "intro.html#on-your-own-update-the-about-page",
    "title": "2  Introduction to R",
    "section": "2.10 On your own: Update the About page",
    "text": "2.10 On your own: Update the About page\nTry some things on your own! Go into the about.qmd page and write some things about yourself, like your favorite hobbies. Use the Markdown guide to write headlines, lists and maybe even try an image? (You can use a URL from an image on the web.)\n\nRe-render you pages\nRe-publish your pages by going back to Terminal and using the quarto publish command."
  },
  {
    "objectID": "intro.html#turning-in-our-projects",
    "href": "intro.html#turning-in-our-projects",
    "title": "2  Introduction to R",
    "section": "2.11 Turning in our projects",
    "text": "2.11 Turning in our projects\n\n\n\n\n\n\nNote\n\n\n\nThis is a bit different for posit.cloud users. See below.\n\n\nThe best way to turn in all of those files into Canvas is to compress the project folder into a single .zip file that you can upload to the assignment.\n\nIn your computer’s Finder, open the Documents/rwd folder.\nFollow the directions for your operating system linked below to create a compressed version of your yourname-final-project folder.\n\nCompress files on a Mac.\nCompress flies on Windows.\n\nUpload the resulting .zip file to the assignment for this week in Canvas.\n\nIf you find you make changes to your R files after you’ve zipped your folder, you’ll need to to zip it again. Make sure you get the new version (or delete the old one first).\nBecause we are building “repeatable” code, I’ll be able to download your .zip files, uncompress them, and the re-run them to get the same results.\nWell done!\n\n2.11.1 Exporting for posit.cloud\nYou can export your project as a zip file from posit.cloud pretty easily. Just follow the directions on this screenshot:\n\n\n\nExport posit\n\n\nThen submit the downloaded .zip file to Canvas."
  },
  {
    "objectID": "intro.html#review-of-what-weve-learned-so-far",
    "href": "intro.html#review-of-what-weve-learned-so-far",
    "title": "2  Introduction to R",
    "section": "2.12 Review of what we’ve learned so far",
    "text": "2.12 Review of what we’ve learned so far\n\nYou’ve learned about the RStudio IDE and how you write and render R code with it.\nYou used install.packages() to download R packages to your computer. Typically executed from within the Console and only once per computer. We installed a lot of packages including the tidyverse.\nYou created a Quarto Website and related Quarto documents with code on them.\nYou published your work on Quarto Pub to share over the internet."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "2  Introduction to R",
    "section": "",
    "text": "The Terminal is where you can send commands to your computer using text vs. pointing and clicking through “normal” actions. It’s super powerful and useful, but this is probably the only terminal command we’ll use.↩︎"
  },
  {
    "objectID": "billboard-cleaning.html#learning-goals-of-this-lesson",
    "href": "billboard-cleaning.html#learning-goals-of-this-lesson",
    "title": "3  Billboard Cleaning",
    "section": "3.1 Learning goals of this lesson",
    "text": "3.1 Learning goals of this lesson\n\nPractice organized project setup.\nLearn a little about data types available to R.\nLearn how to download and import CSV files using the readr package.\nIntroduce the tibble/data Frame.\nString functions together with the pipe: |&gt; (or %&gt;%).\nLearn how to modify data types (date) and select() columns.\n\nEvery project in this book is built around a data source that can birth acts of journalism. For this first one we’ll be exploring the Billboard Hot 100 music charts. You’ll use R skills to find the answer to a bunch of questions in the data and then write about it."
  },
  {
    "objectID": "billboard-cleaning.html#basic-steps-of-this-lesson",
    "href": "billboard-cleaning.html#basic-steps-of-this-lesson",
    "title": "3  Billboard Cleaning",
    "section": "3.2 Basic steps of this lesson",
    "text": "3.2 Basic steps of this lesson\nBefore we get into our storytelling, we have to set up our project, get our data and make sure it is in good shape for analysis. This is pretty standard for any new project. Here are the major steps we’ll cover in detail for this lesson (and many more to come):\n\nCreate your project structure\nFind the data and (usually) get it onto on your computer\nImport the data into your project\nClean up column names and data types\nExport cleaned data for later analysis\n\nSo this chapter is about collecting and cleaning data. We’ll handle the analysis separately in another chapter. I usually break up projects this way (cleaning vs analyzing) to stay organized and to avoid repeating the same cleaning steps."
  },
  {
    "objectID": "billboard-cleaning.html#create-a-new-project",
    "href": "billboard-cleaning.html#create-a-new-project",
    "title": "3  Billboard Cleaning",
    "section": "3.3 Create a new project",
    "text": "3.3 Create a new project\nWe started a new Quarto website Chapter 2 so you got this! Here are the basic steps for you to follow:\n\nLaunch RStudio if you haven’t already.\nMake sure you don’t have an existing project open. Use File &gt; Close project if you do.\nUse the +R button to create a project in a New Directory. For project type, choose Quarto Website.\nName the project yourfirstname-billboard and put it in your ~/Documents/rwd folder.\n\n\n3.3.1 Update index with project description\nFor our projects, we want to use the index.qmd file (and our resulting website home page) as a description of project. This is so you, your editor and anyone you share this with can see what this is about.\nTo save time and such, I’ll supply you with a short project description.\n---\ntitle: \"Billboard Hot 100 project\"\n---\n\nThis data project is an assignment in Reporting with Data, and it uses the [Billboard Hot 100](https://www.billboard.com/charts/hot-100/) chart data as collected by Prof. McDonald in a [Github repo](https://github.com/utdata/rwd-billboard-data).\n\nNotebooks created for this project are available through the site navigation.\n\nCopy the text above and replace the contents of index.qmd.\nUse the Render button or Cmd-Shift-K to render the page.\n\nProject descriptions like this can be longer and more involved, describing details about the source data, the goals of the project, the findings and perhaps links to the resulting stories and graphics published by the news organization.\nThat said, we’re done with this file for now so you can close it.\n\n\n3.3.2 Create directories for your data\nNext we are going to create some folders in our project. We’ll use some standard names for consistency and we’ll do this for every project we build.\nThe first folder is called data-raw. We are creating this folder because we want to keep a pristine version of our original data that we never change or overwrite. This is a core data journalism commandment: Thou shalt not change original data.\nIn your Files pane at the bottom-right of RStudio, there is a New Folder icon.\n\nClick on the New Folder icon.\nName your new folder data-raw. This is where we’ll put raw data that we don’t want to overwrite.\nCreate an additional folder called data-processed. This is were we will write any data we create.\n\nOnce you’ve done that, they should show up in the file explorer in the Files pane. Click the refresh button if you don’t see them. (The circlish thing at top right of the screenshot below. You might have to widen the pane to see it.)\n\n\n\nDirectory made\n\n\nAgain, we’ll do this with every new project we create:\n\nCreate a Quarto website\nUpdate the index with information about the project\nCreate data-raw and data-processed folders."
  },
  {
    "objectID": "billboard-cleaning.html#create-our-cleaning-notebook",
    "href": "billboard-cleaning.html#create-our-cleaning-notebook",
    "title": "3  Billboard Cleaning",
    "section": "3.4 Create our cleaning notebook",
    "text": "3.4 Create our cleaning notebook\nWe’ll typically use at least three Quarto files in our projects:\n\nThe index.qmd that describes the project.\nA “Cleaning” notebook where we prepare our data.\nAn “Analysis” notebook where we pose and answer our data.\n\nSo let’s create our “Cleaning” notebook.\n\nUse top left new document button ( a white box with a green + sign) and choose the Quarto Document to start a new notebook.\nSet the title to “Cleaning”.\nMake sure the Editor choice to use the visual editor is NOT checked. Everything else should be fine.\nClick Create. This creates a Untitled document that we still need to save.\nSave the new file (Do Cmd-S or look under the File menu or use the floppy disc icon in the tool bar.)\nName the file 01-cleaning.qmd. It should already be set to save in the project folder.\n\n\n\n\n\n\n\nTip\n\n\n\nWe named this notebook starting with 01- because we will eventually end up with multiple notebooks that depend on each other and we will need to know the order to run them in the future."
  },
  {
    "objectID": "billboard-cleaning.html#update-our-_quarto.yml",
    "href": "billboard-cleaning.html#update-our-_quarto.yml",
    "title": "3  Billboard Cleaning",
    "section": "3.5 Update our _quarto.yml",
    "text": "3.5 Update our _quarto.yml\nWe’ve created our cleaning notebook and it will Render, but it won’t show up in our website navigation yet until we specifically add it. The _quarto.yml config file controls various output options for the “website” that you are creating.\n\nOpen the _quarto.yml file.\nOn the line that now says - about.qmd, replace that text with - 01-cleaning.qmd.\n\nLet’s talk a little about the part of the _quarto.yml file we edited:\nwebsite:\n  title: \"christian-billboard\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - 01-cleaning.qmd\nNote the indents and dashes and such are important in YAML files. Under the “parent” line website we have two children, title and navbar. The navbar line has a child of left and that has two children - href and - 01-cleaning.qmd.\nWhen we added the 01-cleaning.qmd file it automatically added that to the nav, and then used the title of the page as word linked in the navigation.\nBut for the index, we don’t want to use the title of the page. We have to define what we are doing with more detail, hence the href (meaning which page will be linked to) and the text to show in the navigation, Home.\nWhile we are here, we should adjust some other configurations:\n\nFor the title: change the value to \"Billboard Hot 100\". This is the main website name that displays in the navigation bar.\nAt the bottom of the file on a new line, by itself and starting flush left, add df-print: paged. This makes data we output in our notebooks look more pretty.\n\nYour file should look like this now:\nproject:\n  type: website\n\nwebsite:\n  title: \"Billboard Hot 100\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - 01-cleaning.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\ndf-print: paged\n\nMake sure you _quarto.yml file is saved.\nGo back to your Cleaning notebook and Render it to make there are no errors and that you can see the page in the website navigation.\n\n\n3.5.1 Describe the goals of the notebook\nAt the top of this file after the YAML metadata we’ll want to explain the goals of the notebook and what we are doing.\n\nAdd this text to your notebook AFTER the metadata:\n\n## Goals of this notebook\n\nThe steps we'll take to prepare our data:\n\n- Download the data\n- Import it into our notebook\n- Clean up data types and columns\n- Export the data for next notebook\nWe want to start each notebook with a list like this so our future selves and others know what the heck we are trying to accomplish. It’s not unusual to update this list as we work through the notebook.\nWe will also write Markdown like this to explain each new “section” or goal as we tackle them."
  },
  {
    "objectID": "billboard-cleaning.html#the-setup-chunk",
    "href": "billboard-cleaning.html#the-setup-chunk",
    "title": "3  Billboard Cleaning",
    "section": "3.6 The setup chunk",
    "text": "3.6 The setup chunk\nIn our previous chapter we installed several R packages onto our computer that we’ll use in almost every lesson. There are others we’ll install and use later.\nBut to use these packages in our notebook (and the functions they provide us) we have to load them using the library() function. We always have to declare these and convention dictates we put it near the top of a notebook so everyone understands what is needed.\n\n3.6.1 Load the libraries\n\nIn your notebook after the goals listing, create a new “section” by adding a headline called “Setup” using Markdown, like this: ## Setup.\nAfter the. headline use Cmd+option+i to insert an R code chunk.\nInside that chunk, I want you to type  the code shown below.\n\nI want you to type the code so you can see how RStudio helps you complete the code. It’s something you have to see or do yourself to understand how RStudio helps you type commands, but as you type RStudio will give you valid options you can scroll through and hit tab key to choose.\nHere is a gif of me typing in the commands. I’m using keyboard commands like the up and down arrow to make selections, and the tab key to select them. This concept is called code completion.\n\n\n\nSetup chunk\n\n\nHere is the code:\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)\nlibrary(janitor)\n```\n\n\n\n3.6.2 About the libraries\nA little more about the two packages we are loading here. We use them a lot.\n\nThe tidyverse package is actually a collection of packages for data science that are designed to work together. You can see the first time I run the chunk in the gif above that a bunch of libraries were loaded. By loading the whole tidyverse library we get readr functions for importing data, dplyr to manipulate data, lubridate to help work with dates, and ggplot to visualize data. There are more.\nThe janitor package is not maintained by the same folks at Posit, but it has a couple of useful tools to clean and view data I use a lot, including the clean_names() that we’ll use here.\n\n\n\n3.6.3 About the options\nI go then go back and add some lines at the top of the chunk called execution options. Let’s talk about each line:\n\nWe start with #| label: setup which is an execution option to “name” the code chunk. Labeling chunks is optional but useful.\n\nThe chunk label setup is special as it will be run first if it hasn’t already been run.\nWhen we name a code chunk it creates a bookmark of sorts, which I’ll show you later.\n\nThe #| message: false option suppress the message about all the packages loaded since we don’t need to display that in our notebook. I usually only add that for my setup chunk."
  },
  {
    "objectID": "billboard-cleaning.html#about-the-billboard-hot-100",
    "href": "billboard-cleaning.html#about-the-billboard-hot-100",
    "title": "3  Billboard Cleaning",
    "section": "3.7 About the Billboard Hot 100",
    "text": "3.7 About the Billboard Hot 100\nThe Billboard Hot 100 singles charts has been the music industry’s standard record chart since its inception on Aug. 4th, 1958. The rankings, published by Billboard Media, are currently based on sales (physical and digital), radio play, and online streaming. The methods and policies of the chart have changed over time.\nThe data we will use was compiled by Prof. McDonald from a number of sources. When you write about this data (and you will), you should source it as the Billboard Hot 100 from Billboard Media, since that is where it originally came from and they are the “owner” of the data.\n\n3.7.1 Data dictionary\nTake a look at the current chart. Our data contains many (but not quite all) of the elements you see there. Each row of data (or observation as they are known in R) represents a song and the corresponding position on that week’s chart. Included in each row are the following columns (a.k.a. variables):\n\nCHART DATE: The release date of the chart\nTHIS WEEK: The current ranking as of the chart date\nTITLE: The song title\nPERFORMER: The performer of the song\nLAST WEEK: The ranking on the previous week’s chart\nPEAK POS.: The peak rank the song has reached as of the chart date\nWKS ON CHART: The number of weeks the song has appeared as of the chart date\n\n\n\n3.7.2 Let’s download our data\nOur data is stored on a code sharing website called Github, and it is formatted as a “comma-separated value” file, or .csv. That means it basically a text file where every line is a new row of data, and each field is separated by a comma.\nBecause it is on the internet and has a URL, we could import it directly into our project from there, but there are benefits to getting the file onto your computer first. The hosted file could change later in ways you can’t control, and you would need Internet access to the file the next time you ran your notebook. If you download the file to keep your own copy, you have more control of your future.\nSince this is a new “section” of our cleaning notebook, we’ll note what we are doing and why in Markdown.\n\nAdd a Markdown headline ## Downloading data and some text explaining you are downloading data. Add a note about where the data comes from and include a link to the original source. (This way others and future self will know where the data came from.)\nCreate an R chunk and copy/paste the following inside it. (Given the long URL, go ahead and use the copy icon at the top right of the chunk):\n\ndownload.file(\n  \"https://github.com/utdata/rwd-billboard-data/blob/main/data-out/hot100_assignment.csv?raw=true\",\n  \"data-raw/hot100_assignment.csv\",\n  mode = \"wb\"\n)\nLet’s explain about this:\nThis download.file() code is function, or a bit of code that has been written to do a specific task. This one (surprise!) downloads files from the internet. As a function, it has some “arguments”, which are expected pieces of information that the function needs to do its job. Sometimes we just enter the arguments in the order that the function expects them, like we do here. Other times we well it the kind of argument, like mode = \"w\". You can search for documenation about functions in the Help tab in the bottom-right pane of RStudio.\nIn this case, we are supplying three arguments:\n\nThe first is the URL of the file you are downloading. Note that this is in quotes.\nThe second is the location where we want to save the file, and what want to name it. We call this a “path” and you can see it looks a lot like a URL. This path is in relation to the document we are in, so we are saying we want to put this file in the data-raw/ folder with a name of hot100_assignment.csv, but it is really one path: data-raw/hot100_assignment.csv.\nThe third argument mode = \"w\" gets into the weeds a little, but it helps Windows computers understand the file.\n\ntrying URL 'https://github.com/utdata/rwd-billboard-data/blob/main/data-out/hot100_assignment.csv?raw=true'\nContent type 'text/plain; charset=utf-8' length 18364249 bytes (17.5 MB)\n==================================================\ndownloaded 17.5 MB\nThat’s not a small file at 17 MB and 300,000 rows, but it’s not a huge one, either.\nYou can check that this worked by going to your Files tab and clicking on the data-raw folder to go inside it and see if the file is there. To return out of the folder, click on the two dots to the right of the green up arrow.\n\n\n\nCheck download\n\n\n\n\n3.7.3 Comment the download code\n\n\n\n\n\n\nImportant\n\n\n\nThe Hot 100 data updates each week. We’ll not download it again so our results don’t change mid-analysis!\n\n\nNow that we’ve downloaded the data to our computer, we don’t need to run this line of code again unless we want to update our data from the source. We can “comment” the code to preserve it but keep it from running again if we re-run our notebook (and we will).\n\nAbove your code chunk, write in Markdown a note to your future self that you commented the download for now.\nInside your code chunk, highlight the lines of code that include the download.file() function, then go under the Code menu to Comment/Uncomment Lines. (Note the keyboard command there Cmd-Shift-C, as it is another useful one!)\n\nAdding comments in programming is a common thing and every programming language has a way to do it. It is a way for the programmer to write notes to their future self, colleagues or — like in this case — comment out some code that you want to keep, but don’t want to execute when the program is run.\nWe write most of our “explaining” text outside of code chunks using a syntax called Markdown. Markdown is designed to be readable as written, but it is given pretty formatting when “printed” as a PDF or HTML file.\nBut, sometimes it makes more sense to explain something right where the code is being executed. If you are inside a code chunk, you start the comment with one or more hashes #. Any text on that line that follows won’t be executed as code.\n\n\n\n\n\n\nWarning\n\n\n\nYes, it is confusing that in Markdown you use hashes # to make headlines, but in code chunks you use # to make comments. We’re essentially writing in two languages in the same document."
  },
  {
    "objectID": "billboard-cleaning.html#import-the-data",
    "href": "billboard-cleaning.html#import-the-data",
    "title": "3  Billboard Cleaning",
    "section": "3.8 Import the data",
    "text": "3.8 Import the data\nNow that we have the data on our computer, let’s import it into our notebook so we can see it.\nSince we are doing a new thing, we should again note what we are doing.\n\nAdd a Markdown headline: ## Import data\nAdd some text to explain that we are importing the Billboard Hot 100 data.\nAfter your description, add a new code chunk (Cmd+Option+i).\n\nWe’ll be using the read_csv() function from the tidyverse readr package, which is different from read.csv that comes with base R. read_csv() is mo betta.\nInside the function we put in the path to our data, inside quotes. If you start typing in that path and hit tab, it will complete the path. (Easier to show than explain).\n\nAdd the following code into your chunk and run it.\n\nread_csv(\"data-raw/hot100_assignment.csv\")\n\n\n\n\n\n\nTip\n\n\n\nNote the path to the file is in quotes!\n\n\nYou get two results printed to your notebook.\nThe first result called “R Console” shows what columns were imported and the data types. It’s important to review these to make sure things happened the way that expected. In this case it noted which columns came in as text (chr), or numbers (dbl). The red colored text in this output is NOT an indication of a problem.\n\n\n\nRConsole output\n\n\nThe second result spec_tbl_df prints out the data like a table. The data object is a tibble, which is a fancy tidyverse version of a “data frame”.\n\n\n\n\n\n\nNote\n\n\n\nI will use the term tibble and data frame interchangably. Think of tibbles and data frames like a well-structured spreadsheet. They are organized rows of data (called observations) with columns (called variables) where every column is a specific data type.\n\n\n\n\n\nData output (IGNORE FILENAME)\n\n\nWhen we look at the data output in RStudio, there are several things to note:\n\nBelow each column name is an indication of the data type. This is important.\nYou can use the arrow icon on the right to page through the additional columns.\nYou can use the paging numbers and controls at the bottom to page through the rows of data.\nThe number of rows and columns is displayed.\n\nAt this point we have only printed this data to the screen. We have not saved it in any way, but that is next.\n\n3.8.1 Assign our import to an R object\nNow that we know how to find our data, we next need to assign it to an R object so it can be named thing we can reuse. We don’t want to re-import the data over and over each time we need it.\nThe syntax to create an object in R can seem weird at first, but the convention is to name the object first, then insert stuff into it. So, to create an object, the structure is this:\n# this is pseudo code. Don't put it in your notebook.\nnew_object &lt;- stuff_going_into_object\nThink of it like this: You must have a bucket before you can fill it with water. We “name” the bucket, then fill it with data. That bucket is then saved into our “environment”, meaning it is in memory where we can access it easily by calling its name.\nLet’s make a object called hot100 and fill it with our imported tibble.\n\nEdit your existing code chunk to look like this. You can add the &lt;- by using Option+- as in holding down the Option key and then pressing the hyphen:\n\n\nhot100 &lt;- read_csv(\"data-raw/hot100_assignment.csv\")\n\nRun that chunk and several things happen:\n\nWe no longer see the result of the data in the notebook because we created an object instead of printing it.\nIn the Environment tab at the top-right of RStudio, you’ll see the hot100 object listed.\n\nClick on the blue play button next to hot100 and it will expand to show you a summary of the columns.\nClick on the name hot100 and it will open a “View” of the data in another window, so you can look at it in spreadsheet form. You can even sort and filter it.\n\nOnce you’ve looked at the data, close the data view with the little x next to the tab name.\n\n\n\n3.8.2 Print a peek to your R Notebook\nSince we can’t see the data after we assign it, let’s print the object to our notebook so we can refer to it.\n\nEdit your import chunk to add the last two lines of this, including #:\n\n# create the object, then fill it with data from the csv\nhot100 &lt;- read_csv(\"data-raw/hot100_assignment.csv\")\n\n# peek at the data\nhot100\n\nYou can use the green play button at the right of the chunk, or preferrably have your cursor inside the chunk and do Cmd+Shift+Return to run all lines. (Cmd+Return runs only the current line.)\n\n\n\n3.8.3 Glimpse the data\nThere is another way to peek at the data that I use alot because it is more compact and shows you all the columns and data examples without scrolling: glimpse().\n\nIn your existing chunk, edit the last line to add the glimpse() function as noted below.\n\nI’m showing the return here as well. Afterward I’ll explain the pipe: |&gt;.\nhot100 &lt;- read_csv(\"data-raw/hot100_assignment.csv\")\n\n# peek at the data\nhot100 |&gt; glimpse()\n\n\nRows: 338,500\nColumns: 7\n$ `CHART WEEK`   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/20…\n$ `THIS WEEK`    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ TITLE          &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The …\n$ PERFORMER      &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives…\n$ `LAST WEEK`    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 1…\n$ `PEAK POS.`    &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12…\n$ `WKS ON CHART` &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, …\n\n\nThe glimpse shows there are 300,000+ rows and 7 columns in our data. Each column is then listed out with its data type and the first several values in that column.\n\n\n3.8.4 About the pipe |&gt;\nWe need to break down this code a little: hot100 |&gt; glimpse().\nWe are starting with the object hot100, but then we follow it with |&gt;, which is called a pipe. The pipe is a construct that takes the result of an object or function and passes it into another function. Think of it like a sentence that says “AND THEN” the next thing.\nLike this:\nI woke up |&gt; \n  got out of bed |&gt;\n  dragged a comb across my head\nYou can’t start a new line with a pipe. If you are breaking your code into multiple lines, then the |&gt; needs to be at the end of a line and the next line should be indented so there is a visual clue it is related to line above it, like this:\nhot100 |&gt; \n  glimpse()\nIt might look like there are no arguments inside glimpse(), but what we are actually doing is passing the hot100 tibble into it like this: glimpse(hot100). For almost every function in R the first argument is “what data are you taking about?” The pipe allows us to say “hey, take the data we just mucked with (i.e., the code before the pipe) and use that in this function.”\n\n\n\n\n\n\nTip\n\n\n\nThere is a keyboard command for the pipe |&gt;: Cmd+Shift+m. Learn that one!\n\n\n\n\nA rabbit dives into a pipe\nThe concept of the pipe was first introduced by tidyverse developers in 2014 in a package called magrittr. They used the symbol %&gt;% as the pipe. It was so well received the concept was written directly into base R in 2021, but using the symbol |&gt;. Hadley Wickham’s 2022 rewriting of R for Data Science uses the base R pipe |&gt; by default. You can configure which to use in RStudio.\n(This switch to |&gt; is quite recent so you might see %&gt;% used in this book. Assume |&gt; and %&gt;% are interchangeable. There is A LOT of code in the wild using the magrittr pipe %&gt;%, so you’ll find many references on Stack Overflow and elsewhere.)"
  },
  {
    "objectID": "billboard-cleaning.html#cleaning-data",
    "href": "billboard-cleaning.html#cleaning-data",
    "title": "3  Billboard Cleaning",
    "section": "3.9 Cleaning data",
    "text": "3.9 Cleaning data\nData is dirty. Usually because a human was involved at some point, and we humans are fallible.\nData problems are often revealed when importing so it is good practice to check for problems and fix them right away. We’ll face some of those challenges in this project, but we should talk about what is good vs dirty data.\nGood data should:\n\nHave a single header row with well-formed column names.\n\nDescriptive names are better than not descriptive.\nShort names are better than long ones.\nSpaces in names make them harder to work with. Use and _ or . between words. I prefer _ and all lowercase text.\n\nRemove notes or comments from the files.\nEach column should have the same kind of data: numbers vs words, etc.\n\n\n3.9.1 Cleaning column names\nSo, given those notes above, we should clean up our column names. This is why we have included the janitor package, which includes a neat function called clean_names()\n\nEdit the first line of your chunk to add a pipe and the clean_names function: %&gt;% clean_names()\n\n\nhot100 &lt;- read_csv(\"data-raw/hot100_assignment.csv\") %&gt;% clean_names()\n\n# peek at the data\nhot100 |&gt; glimpse()\n\nRows: 338,500\nColumns: 7\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch…\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",…\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, …\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49…\n\n\nThis function has cleaned up your names, making them all lowercase and using _ instead of periods between words. Believe me when I say this is helpful when you are writing code. It makes type-assist work better and you can now double-click on a column name to select all of it and copy and paste somewhere else. When you have spaces or dashes in an object you can’t double-click on it to select all of it.\n\n\n3.9.2 Fixing the date\nDates in programming are a tricky data type because they are represented essentially as the number of seconds before/after January 1, 1970. Yes, that’s crazy, but it is also cool because that allows us to do math on them. So, to use our chart_date properly in R we need to convert it from the text into a real date datatype. (If you wish, you can read more about why dates are tough in programming | PDF version.)\nConverting text into dates can be challenging, but the tidyverse universe has a package called lubridate to ease the friction. (Get it?).\nSince we are doing something new, we want to start a new section in our notebook and explain what we are doing.\n\nIn Markdown add a headline: ## Fix our dates.\nAdd some text that you are using lubridate to create a new column with a real date.\nAdd a new code chunk. Remember Cmd+Option+i will do that.\n\nWe will be changing or creating our data, so we will create a new object to store it in. We do this so we can go back and reference the unchanged data if we need to. Because of this, we’ll set up a chunk of code that allows us to peek at what is happening while we write our code. We’ll do this kind of setup often when we are working out how to do something in code.\n\nAdd the following inside your code chunk.\n\n\n# part we will build upon\nhot100_date &lt;- hot100\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 338,500\nColumns: 7\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch…\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",…\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, …\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49…\n\n\nLet’s break this down:\n\nI have a comment starting with # to explain the first part of the code.\nWe created a new object (or “bucket”) called hot100_date and we fill it with our hot100 data. Yes, as of now they are exactly the same.\nI leave a blank line for clarity.\nThen another comment.\nThen we glimpse the new hot100_date object so we can see changes as we work on it.\n\nTo be clear, we haven’t changed any data yet. We just created a new object like the old object.\n\n3.9.2.1 Working with mutate()\nWe are going to use the text of our date field chart_date to create a new converted date. We will use the dplyr function mutate() to do this, with some help from lubridate.\n\n\n\n\n\n\nNote\n\n\n\ndplyr is the tidyverse package of functions to manipulate data. We’ll use functions from it a lot. Dplyr is loaded with the library(tidyverse) so you don’t have to load it separately.\n\n\nLet’s explain how mutate works first: Mutate changes every value in a column. You can either create a new column or overwrite an existing one.\nWithin the mutate function, we name the new thing first (the bucket!) and then fill it with the new value.\n# This is just explanatory psuedo code\n# You don't need this in your notebook\ndata |&gt; \n  mutate(\n    newcol = new_stuff_from_math_or_whatever\n  )\nThat new value could be arrived at through math or any combination of other functions. In our case, we want to convert our old text-based date to a real date, and then assign it back to the “new” column.\n\nEdit your chunk to add the changes below and run it. I implore you to type the changes so you see how RStudio helps you write it. Use tab completion, etc.\n\n\n# part we will build upon\nhot100_date &lt;- hot100 |&gt; \n  mutate(\n    chart_date = mdy(chart_week)\n  )\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 338,500\nColumns: 8\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch…\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",…\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, …\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49…\n$ chart_date   &lt;date&gt; 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-…\n\n\nAgain, let’s break down what we’ve done:\n\nAt the end of the first line, we added the pipe |&gt; because we are taking our hot100 data AND THEN we will mutate it.\nNext, we start the mutate() function. If you use type assist and tab completion to type this in, your cursor will end up in the middle of the parenthesis. This allows you to then hit your Return key to split it into multiple lines with proper indenting. We do this so we can more clearly see what inside the mutate … where the real action is going on here. It’s also possible to make multiple changes within the same mutate, and putting each one on their own line makes that more clear.\nInside the mutate, we first name our new column chart_date and then we set that equal to mdy(chart_week), which is explained next.\n\nThe mdy() function is part of the lubridate package. Lubridate allows us to parse text and then turn it into a real date if we tell it the order of the date values in the original data.\n\nOur original date was something like “7/17/1965”. That is month, followed by day, followed by year.\nWe use the lubridate function mdy() to say “that’s the order this text is in, now please convert this into a real date”, which properly shows as YYYY-MM-DD. Lubridate is smart enough to figure out if you have / or - between your values in the original text.\n\nIf your original text is in a different date order, then you look up what lubridate function you need to convert it. I typically use the cheatsheet in the lubridate documentation. You’ll find them in the PARSE DATE-TIMES section.\n\n\n\n\n\n\nTip\n\n\n\nA note about the spacing and indenting of my code above: I strategically used returns to make the code more readable. This code would work the same if it were all on the same line, but writing it this way helps me understand it. RStudio will help you indent properly as you type. (Easier to show than explain.)\n\n\n\n\n3.9.2.2 Check the result!\nThis new chart_date column is added as the LAST column of our data. After doing any kind of mutate you want to check the result to make sure you got the results you expected, which is why we didn’t just overwrite the original chart_week column. That’s also why we built our code this way with glimpse() so we can see example of our data from both the first and the last column. (We’ll rearrange all the columns in a bit once we are done cleaning everything.)\nCheck your glimpse returns … did your dates convert correctly?\n\n\n\n3.9.3 Arrange the data\nJust to be tidy, we want ensure our data is arranged to start with the oldest week and “top” of the chart and then work forward through time and rank.\ni.e., let’s arrange this data so that the oldest data is at the top.\nSorting data is not a particularly difficult concept to grasp, but it is one of the Basic Data Journalism Functions, so watch this video:\n\nIn R, the sorting function we use is called arrange().\nWe’ll build upon our existing code and use the pipe |&gt; to push it into an arrange() function. Inside arrange we’ll feed it the columns we wish to sort by.\n\nEdit your chunk to the following to add the arrange() function:\n\n\n# part we will build upon\nhot100_date &lt;- hot100 |&gt; \n  mutate(\n    chart_date = mdy(chart_week)\n  ) |&gt; \n  arrange(chart_date, this_week)\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 338,500\nColumns: 8\n$ chart_week   &lt;chr&gt; \"8/4/1958\", \"8/4/1958\", \"8/4/1958\", \"8/4/1958\", \"8/4/1958…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ title        &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard He…\n$ performer    &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby D…\n$ last_week    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ wks_on_chart &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ chart_date   &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-…\n\n\nNow when you look at the glimpse, the first record in the chart_date column is from “1958-08-04” and the first in the this_week is “1”, which is the top of the chart.\nJust to see this all clearly in table form, we’ll print the top of the table to our screen so we can see it.\n\nAdd a line of Markdown text in your notebook explaining your are looking at the table.\nAdd a new code chunk and add the following.\n\n\nhot100_date |&gt; head(10)\n\n\n\n  \n\n\n\nThis just prints the first 10 lines of the data.\n\nUse the arrows to look at the other columns of the data (which you can’t see in the book).\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s OK that the last_week columns has “NA” for those first rows because this is the first week ever for the chart. There was no last_week.\n\n\n\n3.9.3.1 Getting summary stats\nPrinting your data to the notebook can only tell you so much. Yes, you can arrange by different columns to see the maximum and minimum values, but it’s hard to get an overall sense of your data that way when there is 300,000 rows like we have here. Luckily there is a nice function called summary() that gives you some summary statistics for each column.\n\nAdd some Markdown text that you’ll print summary stats of your data.\nAdd a new R chunk and put the following in and run it\n\n\nhot100_date |&gt; summary()\n\n  chart_week          this_week        title            performer        \n Length:338500      Min.   :  1.0   Length:338500      Length:338500     \n Class :character   1st Qu.: 26.0   Class :character   Class :character  \n Mode  :character   Median : 51.0   Mode  :character   Mode  :character  \n                    Mean   : 50.5                                        \n                    3rd Qu.: 75.0                                        \n                    Max.   :100.0                                        \n                                                                         \n   last_week         peak_pos       wks_on_chart      chart_date        \n Min.   :  0.00   Min.   :  1.00   Min.   : 1.000   Min.   :1958-08-04  \n 1st Qu.: 23.00   1st Qu.: 13.00   1st Qu.: 4.000   1st Qu.:1974-10-26  \n Median : 47.00   Median : 38.00   Median : 7.000   Median :1991-01-12  \n Mean   : 47.38   Mean   : 40.78   Mean   : 9.261   Mean   :1991-01-11  \n 3rd Qu.: 71.00   3rd Qu.: 65.00   3rd Qu.:13.000   3rd Qu.:2007-03-31  \n Max.   :100.00   Max.   :100.00   Max.   :91.000   Max.   :2023-06-17  \n NA's   :32460                                                          \n\n\nThese summary statistics can be informative for us. It is probably the easiest way to check what the newest and oldest dates are in your data (see the Min. and Max. returns for chart_date). You get an average (mean) and median for each number, too. You might notice potential problems in your data, like if we had a this_week number higher than “100” (we don’t).\n\n\n3.9.3.2 Note the max date\nSince this data updates each week, it is a good idea to note in your notebook what is the most recent chart date. You can find that as the Max. value for chart_date in your summary. You’ll need this value when you write your story.\n\n\n\n3.9.4 Selecting columns\nNow that we have the fixed date column, we don’t need the old chart_week version that is text. We’ll use this opportunity to discuss select(), which is another concept in our Basic Data Journalism Functions series, so watch this:\n\n\nIn R, the workhorse of the select concept is the function called — you guessed it — select(). In short, the function allows us to choose a subset of our columns. We can either list the ones we want to keep or the ones we don’t want.\nLike a lot of things in R, select() is really easy at its most basic level: List the columns you want to keep. But select() can also be very powerful as you learn more options. There are a bunch of selection helpers like starts_with() and ends_with() and everything(). You can choose ranges with numbers 1:3 or column names col_name1:col_name9, or a combination c(col_nam1, col_name3). You can specify what NOT to keep with the negate symbol ! like this !c(col_nam1, col_name3). You can even rename columns as you select them, which we’ll do below.\nIn our case we will select the columns want to keep, and we’ll rename some as we do.\n\nAdd a Markdown headline: ## Selecting columns.\nExplain in text we are dropping the text date column and renaming others.\nAdd the code below and then I’ll explain it. We again are setting this up to create a new object and view the changes.\n\n\nhot100_clean &lt;- hot100_date |&gt; \n  select(\n    chart_date,\n    # this renames a column with new name first\n    current_rank = this_week,\n    title,\n    performer,\n    previous_rank = last_week,\n    peak_rank = peak_pos,\n    wks_on_chart\n  )\n\nhot100_clean |&gt; glimpse()\n\nRows: 338,500\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08…\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H…\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby …\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\nLine by line, it goes like this:\n\nName our new object hot100_clean and fill it with the result of hot100_date and then the modifications that follow.\nWe use the select() function to choose our columns to keep, renaming some of them along the way:\n\nWe start with chart_date which is our real date. We just won’t ever name the text version so we won’t keep it.\nWhen we get to our this_week column, we rename it to current_rank. We’re doing this because we’ll rename all the “ranking” columns with something that includes _rank at the end. We’re being (too) clever here so later we can use select(ends_with(\"_rank\")) to get those ranking columns together. Note the R concept of naming the new thing first before filling it.\nFor title and performer we just list them because we want them to come next but we don’t need to change the names at all.\nFor the other columns we rename or not as needed.\n\nLastly, we glimpse the new object to check it.\n\nThere are other ways to accomplish the same thing, but this works for us."
  },
  {
    "objectID": "billboard-cleaning.html#exporting-data",
    "href": "billboard-cleaning.html#exporting-data",
    "title": "3  Billboard Cleaning",
    "section": "3.10 Exporting data",
    "text": "3.10 Exporting data\n\n3.10.1 Using multiple notebooks\nIt is good practice to separate the cleaning of your data from your analysis. By doing all our cleaning at once and exporting it, we can use that cleaned data over and over in future notebooks.\nBecause each notebook needs to be self-contained to render, we will export our cleaned data in a native R format called .rds (maybe stands for R data storage?). This format preserves all our data types so we won’t have to reset or clean them.\n\n\n\n\n\n\nNote\n\n\n\nThis is one of the reasons I had you name this notebook 01-cleaning.Rmd with a 01 at the beginning, so we know to run this one before we can use the notebook 02-analysis (next lesson!). I use 01- instead of just 1- in case there are more than nine notebooks. I want them to appear in order in my files directory. I’m anal retentive like that, which is a good trait for a data journalist.\n\n\nOne last thought to belabor the point: Separating your cleaning can save time. I’ve had cleaning notebooks that took 20 minutes to process. Imagine if I had to run that every time I wanted to rebuild my analysis notebook. Instead, the import notebook spits out a clean file that can be imported in a fraction of that time.\nThis was all a long-winded way of saying we are going to export our data now.\nWe will use another readr function called write_rds() to create our file to pass along to the next notebook, saving the data into the data-processed folder we created earlier. We are avoiding our data-raw folder because “Thou shalt not change original data” even by accident.\n\nCreate a Markdown headline ## Exports and write a description that you are exporting files to .rds.\nAdd a new code chunk and add the following code:\n\n\n\nhot100_clean |&gt; \n   write_rds(\"data-processed/01-hot100.rds\")\n\nSo, here we are starting with the hot100_clean tibble that we saved earlier. We then pipe |&gt; the result of that into a new function write_rds(). In addition to the data, the function needs to know where to save the file, so in quotes we give the path to where and what we want to call the file: \"data-processed/01-hot100.rds\".\nRemember, we are saving in data-processed because we never export into data-raw. We are naming the file starting with 01- to indicate to our future selves that this output came from our first notebook. We then name it, and use the .rds extension."
  },
  {
    "objectID": "billboard-cleaning.html#bookmarks",
    "href": "billboard-cleaning.html#bookmarks",
    "title": "3  Billboard Cleaning",
    "section": "3.11 Bookmarks",
    "text": "3.11 Bookmarks\nI didn’t want to break our flow of work to explain this earlier, but I want you show you a nice feature in RStudio to jump up and down your notebook.\n\nLook at the bottom of your window above the console and you’ll see a dropdown window. Click on that.\n\nHere is mine, but yours will be different:\n\n\n\nRStudio bookmarks\n\n\nYou’ll notice that each Markdown headline you have is listed. My chunks also have names, but yours probably don’t. It’s optional to name chunks with a label but it helps you find them through the bookmarks. In addition, plots produced by the chunks will have useful names that make them easier to reference elsewhere, but that’s later lesson in the semester.\nHere is how you can name a chunk by using “execution options”, which can control your code output. (This example shows all of the R code chunk.)\n\n```{r}\n#| label: select-rows\n\nhot100_clean |&gt; select(title, performer) |&gt; head()\n```\n\n\n\n  \n\n\n\nYour chunk labels should be short but evocative and should not contain spaces. Hadley Wickham recommends using dashes - to separate words (instead of underscores _) and avoiding other special characters in chunk labels.\nYou can read more execution options and chunks in R for Data Science."
  },
  {
    "objectID": "billboard-cleaning.html#render-and-publish",
    "href": "billboard-cleaning.html#render-and-publish",
    "title": "3  Billboard Cleaning",
    "section": "3.12 Render and publish",
    "text": "3.12 Render and publish\nLastly, I want you to render your notebook so you can see the pretty HTML version.\n\nClick the Render button in the toolbar (or use Cmd-Shift-k).\n\nThis should open the HTML version of your page in the Viewer pane. Note there is also a button in that View toolbar that will instead open your site in your web browser.\n\n\n\nRender site\n\n\nBut note this version is only available on your computer. In a minute we’ll publish this to Quarto Pub so you have an online version that we’ll continue to update.\n\n3.12.1 Publish to Quarto pub\nNow the we have that in order, let’s publish this to the internet. If you published your last project as instructed, it should remember your credentials. (I’m not sure about that if you are using posit.cloud, though.)\n\nIn the bottom-left pane of RStudio, click on the pane Terminal.\nType in quarto publish.\nThe first option available (and perhaps the only) should be Quarto Pub. Hit Return to choose that.\nIt might know who you are already, but continue through the prompts.\n\nYou should end up with your browser open to your new Quarto Pub website.\n\nWell done."
  },
  {
    "objectID": "billboard-cleaning.html#review-of-what-weve-learned-so-far",
    "href": "billboard-cleaning.html#review-of-what-weve-learned-so-far",
    "title": "3  Billboard Cleaning",
    "section": "3.13 Review of what we’ve learned so far",
    "text": "3.13 Review of what we’ve learned so far\nMost of this lesson has been about importing and cleaning data, which can sometimes be the most challenging part of a project. Here we were working with well-formed data, but we still used quite a few tools from the tidyverse like readr (read_csv, write_rds) and dplyr (select, mutate).\nHere are the functions we used and what they do. Most are linked to documentation sites:\n\nlibrary() loads a package so we can use functions within it. We will use at least library(tidyverse) in every project.\nread_csv() imports a csv file. You want that function with the underscore, not read.csv.\nclean_names() is a function in the janitor package that standardizes column names.\nglimpse() is a view of your data where you can see all of the column names, their data type and a few examples of the data.\nhead() prints the first 6 rows of your data unless you specify differently within the function.\nmutate() changes data. You can create new columns or overwrite existing ones.\nmdy() is a lubridate function to convert text into a date. There are other functions depending on the way your text is ordered.\nselect() selects columns from your tibble. You can list all the columns to keep, or use ! to remove columns. There are many variations.\nsummary() gives you some quick summary statistics about your data like min, max, mean, median.\nwrite_rds() writes data out to a file in a format that preserves data types."
  },
  {
    "objectID": "billboard-cleaning.html#whats-next",
    "href": "billboard-cleaning.html#whats-next",
    "title": "3  Billboard Cleaning",
    "section": "3.14 What’s next",
    "text": "3.14 What’s next\nThis is part one of a two-chapter project. You might be asked to turn in your progress, or we might wait until both parts are done. Check your class assignments in Canvas.\nPlease reach out to me if you have questions on what you’ve done so far. These are important skills you’ll use on future projects."
  },
  {
    "objectID": "billboard-analysis.html#goals-of-this-lesson",
    "href": "billboard-analysis.html#goals-of-this-lesson",
    "title": "4  Billboard Analysis",
    "section": "4.1 Goals of this lesson",
    "text": "4.1 Goals of this lesson\n\nTo use the group_by/summarize/arrange combination to count rows of data\nTo filter our data in two ways: to focus data before summarizing, and to logically cut summarized lists.\nWe’ll also cover some more complex filters using and/or logic\nIntroduce the shortcut count() function"
  },
  {
    "objectID": "billboard-analysis.html#the-questions-well-answer",
    "href": "billboard-analysis.html#the-questions-well-answer",
    "title": "4  Billboard Analysis",
    "section": "4.2 The questions we’ll answer",
    "text": "4.2 The questions we’ll answer\nNow that we have the Billboard Hot 100 charts data in our project it’s time to find the answers to the following questions:\n\nWhich performer had the most appearances on the Hot 100 chart at any position?\nWhich song (title & performer) has been on the charts the most?\nWhich song (title & performer) was No. 1 for the most number of weeks?\nWhich performer had the most songs reach No. 1?\nWhich performer had the most songs reach No. 1 in the most recent five years?\nWhich performer has had the most Top 10 hits overall?\n\nWhat are your guesses for the questions above? NO PEEKING!\nIn each case we’ll talk over the logic of finding the answer and the code to accomplish it.\nBefore we can get into the analysis, we want to set up a new notebook to separate our cleaning from our analysis.\n\n\n\n\n\n\nImportant\n\n\n\nThe data outputs in this book will differ from what you get since the data source is updated every week. This is especially true in videos and gifs."
  },
  {
    "objectID": "billboard-analysis.html#setting-up-an-analysis-notebook",
    "href": "billboard-analysis.html#setting-up-an-analysis-notebook",
    "title": "4  Billboard Analysis",
    "section": "4.3 Setting up an analysis notebook",
    "text": "4.3 Setting up an analysis notebook\nAt the end of the last notebook we exported our clean data as an .rds file. We’ll now create a new Quarto notebook and import that data. It will be much easier this time.\n\nIf you don’t already have it open, go ahead and open your Billboard project.\nIf your Cleaning notebook is still open, go ahead and close it.\nUse the + menu to start a new Quarto Document.\nSet the title as “Analysis”.\nSave the file as 02-analysis.qmd in your project folder.\nCheck your Environment tab (top right) and make sure the environment is empty. We don’t want to have any leftover data. If there is, then go under the Run menu and choose Restart R and Clear Output.\nAlso go into your _quarto.yml file and add 02-analysis.qmd on the line after 01-cleaning.qmd so that this notebook will show up in your website navigation.\n\n\n4.3.1 Add your goals, setup\nSince we are starting a new notebook, we need to set up a few things. First up we want to list our goals.\n\nAdd a headline and text describing the goals of this notebook. You are exploring the Billboard Hot 100 charts data.\nGo ahead and copy all the questions we outlined above into your notebook.\nFormat those questions as a nice list. Start each line with a - or * followed by a space. There should be a blank line above and below the entire LIST but not between the items. List items should be on sequential lines … and it is the only markdown item like that.\nNow add a another headline (two hashes) called Setup.\nAdd a chunk, add the option #| label: setup and add in the tidyverse library.\nRun the chunk to load the libraries.\n\n\n\nTry to write the code on your own first\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDo you see the grey triangle above with the term Try to write the code on your own first next to it? That is code that I’ve written but hidden to give you a chance to write it yourself before you see it. If you click on the little triangle, it will turn and reveal the code!\n\n\n\n\n4.3.2 Import the cleaned data\nWe need to import the data that we cleaned and exported in the last notebook. It’s just a couple of lines of code and you could write them all out and run it, but here is where I tell you for the first of a 1,000 times:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\nYes, sometimes for the sake of brevity I will give you multiple lines of code at once, but to understand what is actually going on you should add and run that code one line at a time.\nWe’ll painstakingly walk through that process here to belabor the point. Here are our goals for this bit:\n\nDocument that we are importing clean data\nImport our cleaned data\nFill an R object with that data\nGlimpse the R Object so we can check it\n\nI want you to:\n\nStart a Markdown section with a headline noting you are importing the cleaned data. Add any other text notes you might need for yourself.\nAdd a new code chunk.\nInside your code chunk, add this line of code:\n\nread_rds(\"data-processed/01-hot100.rds\")\nThis read_rds() function is just like the read_csv() function we used in our last notebook to read in data from our computer, except we are reading a different kind of file format: RDS. The argument (in quotes) is the path where our file is on our computer.\nWhat did you expect will happen if you run the code?\nNow run that line of code.\nIt should print the results of the data in your notebook. Did it?\nIf you got an error, you should read that error output for some clues about what happened. When using a read_ function problems are usually about one of two things: It can’t find the function (load the library tidyverse!) or it doesn’t understand the path to the file (that you have the folder structure wrong or something misspelled).\n\n\n\n\n\n\nNote\n\n\n\nCATCHING COMMON MISTAKES: This guide about common R mistakes is worth bookmarking and reading.\n\n\nOur next goal is to take all that data and put it into a new R object.\n\nEdit your one line of code to assign the data to an object hot100. Remember we will add the new object FIRST (our bucket) and then use the &lt;- operator to put the data inside it (the water).\nRun it!\n\nIt should look like this:\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\nWhat happened when you ran the code? Did you get an error? Did you get a result printed to the screen?\nOK, that last one was a trick question. When you save data into an object it does NOT print it to your notebook, but it does save that object into your “Environment”, which means it is in memory and can be called and run at any time. You should see hot100 listed in your Environment tab at the top right of RStudio.\nNow, before I started this quest, I knew I wanted that data inside the hot100 object and could’ve written it like that from the beginning. But I didn’t becuase most “read” problems are in the path, so I wanted to make sure that function was written correctly first. Also, when I put the data into the object I can’t see it until I print it out again, so I do it one piece at at time. You should, too.\nSo next we’ll print the data out to our screen. We do that by just by calling the object.\n\nEdit your code chunk to add a blank line then the object and run it, like this:\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100\nThis should print out the data to your screen so you can see the first couple of columns and the first 10 rows of data.\nBut what we really need is to see all the column names, the data types and an example of the data. We can get that with our glimpse() function.\n\nEdit the last line of the chunk to add the pipe and the glimpse() function\nRun it.\n\nYour code (and result) should look like this now:\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100 |&gt; glimpse()\n\nRows: 338,500\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08…\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H…\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby …\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\nAgain, I could’ve written all that code at once before running it, as I’ve written code like that for many years. I still write code one line at a time, and you should, too. It is the only way to make sure your code is correct. It’s easier to find problems. So, for the second of a 1,000 times:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are looking at my code and writing it into your notebook, you have to run each line of code BEFORE you add the pipe |&gt; at the end of that line. A pipe |&gt; must have a function directly following it on the same or next (or same) line to work."
  },
  {
    "objectID": "billboard-analysis.html#introducing-dplyr",
    "href": "billboard-analysis.html#introducing-dplyr",
    "title": "4  Billboard Analysis",
    "section": "4.4 Introducing dplyr",
    "text": "4.4 Introducing dplyr\nOne of the packages within the tidyverse is dplyr. Dplyr allows us to transform our data frames in ways that let us explore the data and prepare it for visualizing. It’s the R equivalent of common Excel functions like sort, filter and pivoting. There is a cheatsheet on the dplyr that you might find useful.\n\n\n\nImages courtesy Hadley. Yes, I personally asked him. OK, it was email, but still.\n\n\nWe’ve used select(), mutate() and arrange() already, but we’ll introduce more dplyr functions in this chapter.\nWe will use these dplyr functions to find the answers to our questions."
  },
  {
    "objectID": "billboard-analysis.html#most-appearances",
    "href": "billboard-analysis.html#most-appearances",
    "title": "4  Billboard Analysis",
    "section": "4.5 Most appearances",
    "text": "4.5 Most appearances\nOur first question: Which performer had the most appearances on the Hot 100 chart at any position?\nLet’s work through the logic of what we need to do before I explain exactly how to do it.\nEach row in the data is one song ranked on the chart. It includes a field with the name of the “performer” so we know who recorded it.\nSo, to figure out how many times a performer is in the data, we need to count the number of rows with the same performer.\nWe’ll use the tidyverse’s version of Group and Aggregate to get this answer. It is actually two different functions within dplyr that often work together: group_by() and summarize()\n\n4.5.1 Group & Aggregate\nBefore we dive into the code, let’s review this video about “Group and Aggregate” to get a handle on the concept.\n\nWe’ll dive deep into this next.\n\n\n4.5.2 Summarize\nWe’ll start with summarize() first because it can stand alone.\nThe summarize() function computes summary tables describing your data. We’re usually finding a single number to describe a column of data, like the “average” of numbers in column.\nIn our case we want a “summary” about the number of times a specific performer appears in data, hence we use summarize().\n\n\n\n\n\n\nTip\n\n\n\nTHEY BE THE ZAME: summarize() and summarise() are the same function, as R supports both the American and UK spelling of summarize. They work the same and I don’t care which you use.\n\n\nHere is an example of summarize() in a different context:\n\n\n\nLearn about your data with summarize()\n\n\nThe example above is giving us two summaries: It is applying a function mean() (or average) on all the values in the lifeExp column, and then again with min(), the lowest life expectancy in the data.\nMuch like the mutate() function we used earlier, within summarize() we list the name of the new column first, then assign to it the function and column we want to accomplish using =.\nAgain, in our case (as we work toward finding the performer with most appearances) we want to summarize the number of rows, and there is a function for that: n(). (Think “number of observations”.) Every row in the data is an appearance … we just need to count how many rows have each performer.\nBut first, to show how this works, we’ll count all the rows in our data. Let’s write the code and run it, then I’ll explain:\n\nSet up a new section with a Markdown headline, text and explain you are looking for most appearances.\nAdd a named code chunk and add the following:\n\n\nhot100 |&gt; \n  summarize(appearances = n())\n\n\n\n  \n\n\n\n\nWe start with the tibble first and then pipe into summarize().\nWithin the function, we define our summary:\n\nWe name the new column “appearances” because that is a descriptive column name for our result.\nWe set that new column to count the number of rows.\n\n\nBasically we are summarizing the total number of rows in the data. Through 2021 there were 330,800 rows.\nBut I bet you’re asking: Professor, we want to count the number of times a performer has appeared, right?\nThis is where we bring in a close friend to summarize() … the group_by() function.\n\n\n4.5.3 Group by\nThe group_by() function pre-sorts data into groups so that whatever function follows is applied within each of the groups. That means group_by() always has something following it, and that something is usually summarize().\nIf we group_by() performer before we summarize/count our rows, it will put all of the rows with “Aerosmith” together, then all the “Bad Company” rows together, etc. and then it will count the rows within those groups … first those for Aerosmith and then those for Bad Company.\n\nModify your code block to add the group_by line before summarize, then run it:\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n())\n\n\n\n  \n\n\n\nWhat we get in return is a summarized table that shows all 10,000+ different performers that have been on the charts, and the number of rows in which they appear in the data.\nThat’s great, but who had the most?\n\n\n4.5.4 Arrange the results\nRemember in our import notebook when we sorted all the rows by the oldest chart date? We’ll use the same arrange() function here, but we’ll change the result to descending order, because journalists almost always want to know the most of something.\n\nEdit your chunk to add the pipe and arrange function below and run it, then I’ll explain.\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n\n\n  \n\n\n\n\nWe added the arrange() function and fed it the column of “appearances”. If we left it with just that, then it would list the smallest values first.\nWithin the arrange function we wrapped our column in another function: desc() to change the order to “descending”, or the most on the top.\n\n\n\n\n\n\n\nNote\n\n\n\nI sometimes pipe the column name into the desc() function like this: arrange(appearances |&gt; desc()).\n\n\n\n\n4.5.5 Get the top of the list\nWe’ve printed 10,000+ rows of data into our notebook when we really only wanted the Top 10 or so. You might think it doesn’t matter, but your knitted HTML file will store all that data and can make it a big file (as in hard drive space), so I try to avoid that when I can.\nWe can use the head() command again to get our Top 10. It will give us a specified number of rows at the top of the table (with a default of six if we don’t specify.) There is a corresponding tail() function to get the last rows.\n\nEdit your code to pipe the result into head() function set to 10 rows.\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(appearances |&gt; desc()) |&gt; \n  head(10)\n\n\n\n  \n\n\n\nIf I was to explain all of the code above in English, I would describe it as this:\n\nWe start with the hot100 data AND THEN\nwe group by the data by performer AND THEN\nwe summarize it by counting the number of rows in each group, calling the new column “appearances” AND THEN\nwe arrange the result by appearances in descending order AND THEN\nwe display just the first 10 rows\n\nSince we have our answer here and we’re not using the result later, we don’t need to create a new object or anything. Printing it to our notebook is sufficient.\nSo, Taylor Swift … is that who you guessed? A little history here, Swift past Elton John in the summer of 2019. Elton John has been around a long time, but Swift’s popularity at a young age, plus changes in how Billboard counts plays in the modern era (like streaming) has rocketed her to the top. (Sorry, Rocket Man). And it doesn’t hurt that she is re-releasing her entire catalog (Taylor’s version)!\n\n\n\n\n\n\nImportant\n\n\n\nThe list we’ve created here is based on unique performer names, and as such considers collaborations separately. For instance, Drake is near the top of the list but those are only songs he performed alone and not the many, many collaborations he has done with other performers. So, songs by “Drake” are counted separately than “Drake featuring Future” and even “Future featuring Drake”. You’ll need to make this clear when you write your data drop in a later assignment.\n\n\n\n\n4.5.6 Render your notebook\nNow that you have your first quest answers, let’s celebrate by rendering your notebook so you can see all your pretty work.\n\nIn the toolbar of your notebook, click the Render button or use the Cmd-Shift-k keyboard command.\n\nThis will give you the website view of your page, but we need to add it to the navigation.\n\nOpen your quarto.yml file\nAfter the line that you added for the cleaning notebook, add another new line (with the same indenting) and put in the name of your analysis file. It should be - 02-analysis.qmd.\nNow re-Render your notebook and check the navigation on the page."
  },
  {
    "objectID": "billboard-analysis.html#song-with-most-appearances",
    "href": "billboard-analysis.html#song-with-most-appearances",
    "title": "4  Billboard Analysis",
    "section": "4.6 Song with most appearances",
    "text": "4.6 Song with most appearances\nOur quest here is this: Which song (title & performer) has been on the charts the most?\nThis is very similar to our quest to find the performer with the most appearances, but we have to consider both title and performer together because different artists can perform songs of the same name. For example, Adele’s song “Hold On” entered the Hot 100 at 49 in December 2021, but at least 18 other performers have also had a song titled “Hold On” on the Hot 100.\nLet’s first talk through the logic of what we want to do and how it differed from our first quest:\n\nWe want to count rows where BOTH the title and performer are the same. We can do this by putting both values in our group_by() function, instead of just one thing. This will put all rows with both “Rush” as a performer AND Tom Sawyer as a title into the same group. Rows with “Rush” and Red Barchetta will be considered in a different group.\nThen we want to summarize() to count the number of rows in each group.\nOnce we have a summary table, we’ll sort it by appearances in descending order to put the highest value on the top.\n\nLet’s write the code.\n\nStart a new section (headline, text describing goal and a new code chunk.)\nAdd the code below ONE LINE AT A TIME and run it and then I’ll outline it below.\n\nWRITE ONE LINE OF CODE. RUN IT. CHECK IT. REPEAT\nRemember, you write and run the line BEFORE you add the pipe |&gt;!\n\nhot100 |&gt;\n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(desc(appearances))\n\n\n\n  \n\n\n\nIf we were describing each line in English, it would be:\n\nStart with the hot100 data AND THEN …\nGroup the data by both performer and title AND THEN ..\nSummarize the data by first naming our new column “appearances” and use n() to count the number of rows AND THEN …\nArrange the data in descending order by the appearances column.\n\nWe will often use group_by(), summarize() and arrange() together, which is why I’ll refer to this as the GSA trio. They are like three close friends that always want to hang out together.\nSo, what was your guess or this one? A little bit of history in that answer … Glass Animals’ Heat Waves in 2022 overcame The Weeknd’s Blinding Lights, which had overcome Imagine Dragon’s Radioactive some time in 2021.\n\n\n\n\n\n\nNote\n\n\n\nWhen you run the code above you might see a warning “summarise() has grouped output by ‘performer’. You can override using the.groups argument.” THIS IS NOT A PROBLEM. It is just R letting you know that the output of this remains grouped by the first item. Explaining it would break your brain right now. Don’t worry about it. This is not the droid you are looking for.\n\n\n\n4.6.1 Introducing filter()\nI showed you head() in the previous quest and that was useful to show just a few records at the top of a list, but it does so indiscriminately. Note there are some songs here with the same number of weeks on the charts. If we used head() we might split that tie, leaving us with an incomplete answer. A better strategy is to cut off the list at a logical place using filter(). Let’s dive into this new function:\nFiltering is one of those Basic Data Journalism Functions:\n\nThe dplyr function filter() reduces the number of rows in our data based on one or more criteria within the data.\nThe syntax works like this:\n# this is psuedo code. don't run it\ndata |&gt; \n  filter(variable comparison value)\n\n# example\nhot100 |&gt; \n  filter(performer == \"Taylor Swift\")\nThe filter() function typically works in this order:\n\nWhat is the variable (or column) you are searching in.\nWhat is the comparison you want to do. Equal to? Greater than?\nWhat is the observation (or value in the data) you are looking for?\n\nNote the two equals signs == in our Taylor Swift example above. It is important to use two of them when you are asking if a value is “true” or “equal to”, as a single = typically means you are assigning a value to something.\n\n4.6.1.1 Comparisons: Logical tests\nThere are a number of these logical tests for the comparison:\n\n\n\nOperator\nDefinition\n\n\n\n\nx &lt; y\nLess than\n\n\nx &gt; y\nGreater than\n\n\nx == y\nEqual to\n\n\nx &lt;= y\nLess than or equal to\n\n\nx &gt;= y\nGreater than or equal to\n\n\nx != y\nNot equal to\n\n\nx %in% c(y,z)\nIn a group\n\n\nis.na(x)\nIs NA (missing values)\n\n\n!is.na(x)\nIs not NA\n\n\n\nWhere you apply a filter matters. If we want to consider only certain data when we are grouping/summarizing then we filter BEFORE the GSA. If we filter after the GSA, we affect only the results of the summarize function, which is what we want to do here.\n\n\n4.6.1.2 Filter to a logical cutoff\nIn this case, I want you to use filter after the GSA actions to include only results with 65 or more appearances.\n\nEdit your current chunk to add a filter as noted in the example below. I’ll explain it after.\n\n\nhot100 |&gt;\n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc()) |&gt; \n  filter(appearances &gt;= 65) # this is the new line\n\n\n\n  \n\n\n\nLet’s break down that last line:\n\nfilter() is the function.\nThe first argument in the function is the column we are looking in, in our case the appearances column which was created in the summarize line.\nWe then provide a comparison operator &gt;= to get “greater than or equal to”.\nWe then give the value to compare, 65 in our case.\n\n\n\n\n4.6.2 Render your second quest\nThis would be a good time to again Render your notebook so you can see what the output will look like. Look through that output, checking your headlines and such. Clean up any problems you might see."
  },
  {
    "objectID": "billboard-analysis.html#song-the-longest-at-no.-1",
    "href": "billboard-analysis.html#song-the-longest-at-no.-1",
    "title": "4  Billboard Analysis",
    "section": "4.7 Song the longest at No. 1",
    "text": "4.7 Song the longest at No. 1\nWe introduced filter() in the last quest to limit the summary. For this quest you’ll need to filter the data before the GSA trio.\nLet’s review the quest: Which song (title & performer) was No. 1 for the most number of weeks?\nWhile this quest is very similar to the one above, it really helps to think about the logic of what you need and then build the query one line at a time to make each line works.\nLet’s talk through the logic:\n\nWe are starting with our hot100 data.\nDo we want to consider all the data? In this case, no: We only want titles that have a current_rank of 1. This means we will filter before any summarizing.\nThen we want to count the number of rows with the same performer and title combinations. This means we need to group_by both performer and title.\nSince we are counting rows, we need use n() as our summarize function, which counts the number or rows in each group.\n\nSo let’s step through this with code:\n\nCreate a section with a headline, text and code chunk\nStart with the hot100 data and then pipe into filter().\nWithin the filter, set the current_rank to be == to 1.\nRun the result and check it\n\n\nhot100 |&gt; \n  filter(current_rank == 1)\n\n\n\n  \n\n\n\nThe result should show only titles with a 1 for current_rank.\nThe rest of our logic is just like our last quest. We need to group by the title and performer and then summarize using n() to count the rows.\n\nEdit your existing chunk to add the group_by and summarize functions. Name your new column appearances and set it to count the rows with n().\n\n\n\nTry this on your own before you peek for the answer\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n())\n\n\n\n\n  \n\n\n\nLook at your results to make sure you have the three columns you expect: performer, title and appearances.\n\n\n\n\n\n\nNote\n\n\n\nWhile WRITING AND RUNNING ONE LINE AT A TIME is still “the Way”, when you run a line with group_by() it won’t usually show different results without its buddy summarize(). So I usually write those two together, or I write the summarize() line first to make sure it works, then edit in the group_by() line to split the data before the summary.\n\n\nOur result above doesn’t quite get us where we want because it lists the results alphabetically by the performer. You need to arrange the data to show us the most appearances at the top.\n\nEdit your chunk to add the arrange() function to sort by appearances in desc() order. This is just like our last quest.\n\n\n\nMaybe check your last chunk on how you used arrange, then try it before checking the answer here\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc())\n\n\n\n\n  \n\n\n\nYou have your answer now (you go, Lil Nas) but we are listing more than 1,000 rows. Let’s cut this off at a logical place like we did in our last quest.\n\nUse filter() to cut your summary off at appearances of 14 or greater.\n\n\n\nYou’ve done this before … try it on your own!\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc()) |&gt; \n  filter(appearances &gt;= 14)\n\n\n\n\n  \n\n\n\nNow you have the answers to the song with the most weeks at No. 1 with a logical cutoff. If you add to the data later, that logic will still hold and not cut off arbitrarily at a certain number of records."
  },
  {
    "objectID": "billboard-analysis.html#performer-with-most-no.-1-singles",
    "href": "billboard-analysis.html#performer-with-most-no.-1-singles",
    "title": "4  Billboard Analysis",
    "section": "4.8 Performer with most No. 1 singles",
    "text": "4.8 Performer with most No. 1 singles\nOur new quest is this: Which performer had the most titles reach No. 1?\nThis sounds similar to our last quest, but there is a distinct difference. (That’s a bad joke that will reveal itself here in a bit.)\nAgain, let’s think through the logic of what we have to do to get our answer:\n\nWe need to consider only No. 1 songs (filter!)\nBecause a song could be No. 1 for more than one week, we need to consider the same title/performer combination only once. Another way to say this is we need unique or distinct combinations of title/performer. (We’ll introduce a new function to find this.)\nOnce we have all the unique No. 1 songs in a list, then we can group by performer and count the number of times they are on the list.\n\nLet’s start by getting the No. 1 songs. You’ve did this in the last quest.\n\nCreate a new section with a headline, text and code chunk.\nStart with the hot100 data and filter it so you only have current_rank of 1.\n\n\nhot100 |&gt; \n  filter(current_rank == 1)\n\n\n\n  \n\n\n\nNow look at the result. Note how “Poor Little Fool” shows up more than once? Other songs do as well. If we counted rows by performer now, that would tell us the number of weeks they’ve had No. 1 songs, not how many different songs have made No. 1.\n\n4.8.1 Using distinct()\nThe next challenge in our logic is to show only unique performer/title combinations. We do this with distinct().\nWe feed the distinct() function with the variables we want to consider together, in our case the perfomer and title. All other columns are dropped since including them would mess up their distinctness.\n\nEdit your chunk to add the distinct() function to your code chunk.\n\n\nhot100 |&gt; \n  filter(current_rank == 1) |&gt; \n  distinct(title, performer)\n\n\n\n  \n\n\n\nNow we have a list of just No. 1 songs!\n\n\n4.8.2 Summarize the performers\nNow that we have our list of No. 1 songs, we can summarize the “number” of times a performer is in the list to know how many No. 1 songs they have.\nWe’ll again use the group_by/summarize/arrange combination for this, but we are only grouping by performer since that is the values we are counting.\n\nEdit your chunk to add a group_by on performer and then a summarize() to count the rows. Name the new column no_hits. Run it.\nAfter you are sure the group_by/summarize runs, add an arrange() to show the no1_hits in descending order.\n\n\n\nYou’ve done this before! Give it a go before checking the code here.\nhot100 |&gt; \n  filter(current_rank == 1) |&gt;\n  distinct(title, performer) |&gt;\n  group_by(performer) |&gt;\n  summarize(no1_hits = n()) |&gt;\n  arrange(no1_hits |&gt; desc())\n\n\n\n\n  \n\n\n\n\n\n4.8.3 Filter for a good cutoff\nLike we did earlier, use a filter() after your arrange to cut the list off at a logical place.\n\nEdit your chunk to filter the summary to show performers with 8 or more No. 1 hits.\n\n\n\nYou can do this. Really.\nhot100 |&gt; \n  filter(current_rank == 1) |&gt;\n  distinct(title, performer) |&gt;\n  group_by(performer) |&gt;\n  summarize(no1_hits = n()) |&gt;\n  arrange(no1_hits |&gt; desc()) |&gt; \n  filter(no1_hits &gt;= 8)\n\n\n\n\n  \n\n\n\nSo, The Beatles. Was that your guess? Look closely at that list … who has any chance of topping them?\n\n\n4.8.4 The perils of collaborations\nIf you are a student of music history, you might be scratching your head about now, thinking the Beatles have 20 No. 1 hits! You would be right, there is something up here.\nThe song Get Back by “The Beatles with Billy Preston” reached No. 1 on the Hot 100 on May 10, 1969. But that song does not show up with our other Beatles songs because those are listed as “The Beatles”. This is the same issue I noted earlier about “Drake” and “Drake featuring Future” or “Future featuring Drake” each being considered separate performers. They aren’t counted together.\nHow do we get around this? Well, we write accurately. We note our results consider collaborations separately. Yes, it’s true we could miss some things. It’s a good idea to double check your work, perhaps by searching the data for all distinct song titles that include “Beatles” in the performer field.\nIf the performer data were more consistent we might be able to extract each band/person, but it isn’t. Which of these would you consider separate? “Supremes & Temptations”, “Romeo & Juliet Soundtrack”, “Crosby, Stills & Nash”, “Delaney & Bonnie & Friends/Eric Clapton.” It’s a mess."
  },
  {
    "objectID": "billboard-analysis.html#most-no.-1-hits-in-last-five-years",
    "href": "billboard-analysis.html#most-no.-1-hits-in-last-five-years",
    "title": "4  Billboard Analysis",
    "section": "4.9 Most No. 1 hits in last five years",
    "text": "4.9 Most No. 1 hits in last five years\nWhich performer had the most songs reach No. 1 in the most recent five years?\nLet’s talk through the logic. This is very similar to the No. 1 hits above but with two differences:\n\nIn addition to filtering for No. 1 songs, we also want to filter for charts since 2018.\nWe might need to adjust our last filter for a better “break point”.\n\nThere are a number of ways we could write a filter for the date, but we’ll do so in a way that gets all the rows after the last day of 2017.\n\nhot100 |&gt; \n  filter(chart_date &gt; \"2017-12-31\") |&gt; \n  head() # added just to shorten our result\n\n\n\n  \n\n\n\nBut since we need this filter before our group, we can do this within the same filter function where we get the number one songs.\n\nCreate a new section (headline, text, chunk).\nBuild (from scratch, one line at a time) the same filter, group_by, summarize and arrange as above, but leave out the cut-off filter at the end. Make sure it runs.\nEdit your filter to put a comma after current_rank == 1 and then add this filter: chart_date &gt; \"2017-12-31\". Run the code.\nBuild a new cut-off filter at the end and keep only rows with more than 1 top_hits.\n\n\n\nNo, really. Try it on your own first\nhot100 |&gt; \n  filter(\n    current_rank == 1,\n    chart_date &gt; \"2017-12-31\"\n  ) |&gt; \n  distinct(title, performer) |&gt; \n  group_by(performer) |&gt; \n  summarize(top_hits = n()) |&gt; \n  arrange(top_hits |&gt; desc()) |&gt; \n  filter(top_hits &gt; 1)\n\n\n\n\n  \n\n\n\nNow you know who has the most No. 1 hits since 2018. I suspect that list will change."
  },
  {
    "objectID": "billboard-analysis.html#complex-filters",
    "href": "billboard-analysis.html#complex-filters",
    "title": "4  Billboard Analysis",
    "section": "4.10 Complex filters",
    "text": "4.10 Complex filters\nYou can combine filters in different ways to really target which rows to keep. For these I want you to play around a bit.\n\nCopy each of the examples below into your notebook, but change the title and/or performer to some that you like.\n\n\n4.10.1 Multiple truths\nIf you want filter data for when two or more things are true, you can write two equations and combine with &. Only rows where both sides prove true are returned.\n\n# When Poor Little Fool was No. 1, but not any other position\nhot100 |&gt; \n  filter(title == \"Poor Little Fool\" & current_rank == 1) |&gt; \n  select(chart_date:performer)\n\n\n\n  \n\n\n\n\n\n4.10.2 Either is true\nIf you want an or filter, then you write two equations with a | between them.\n\n# songs by Adam Sandler OR Alabama Shakes\nhot100 |&gt; \n  filter(performer == \"Adam Sandler\" | performer == \"Alabama Shakes\") |&gt; \n  select(chart_date:performer)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe | is found as the Shift of the \\ key above Return on your keyboard. That | character is also sometimes called a “pipe”, which gets confusing in R with |&gt;.)\n\n\n\n\n4.10.3 Mixing criteria\nIf you have multiple criteria, you separate them with a comma ,. Note I’ve also added returns to make the code more readable and added distinct to songs only once.\n\n# gives us rows with either Taylor Swift or Drake,\n# but only those that reached No. 1\nhot100 |&gt; \n  filter(\n    performer == \"Taylor Swift\" | performer == \"Drake\",\n    current_rank == 1\n) |&gt; \n  distinct(current_rank, title, performer)\n\n\n\n  \n\n\n\n\n\n4.10.4 Search within a string\nAnd if you want to search for text within the data, you can use str_detect() to look for specific characters within a value to filter rows. str_detect() needs two arguments: 1) What column to search in, and 2) what to search for “in quotes”. I also use distinct() here to show only unique title/performer combinations.\n\n# Songs where \"2 Chainz\" was among performers\nhot100 |&gt; \n  filter(str_detect(performer, \"2 Chainz\")) |&gt; \n  distinct(title, performer)\n\n\n\n  \n\n\n\nThere is a newer string search function called str_like() that has some interesting nuances derived from SQL. It must match the entire phrase so it will find only songs where “2 Chainz” is the only artist, but it case INSENSITIVE by default, unlike str_detect().\n\nhot100 |&gt; \n  filter(str_like(performer, \"2 chainz\")) |&gt;\n  distinct(title, performer)\n\n\n\n  \n\n\n\nOf course there is much, much more."
  },
  {
    "objectID": "billboard-analysis.html#on-your-own",
    "href": "billboard-analysis.html#on-your-own",
    "title": "4  Billboard Analysis",
    "section": "4.11 On your own",
    "text": "4.11 On your own\nI want you to do two things on your own using the skills we’ve covered above.\n\n4.11.1 Most Top 10 hits\nWhich performer had the most Top 10 hits overall?\nThis one I want you to do on your own.\nThe logic is very similar to the “Most No. 1 hits” quest you did before, but you need to adjust your filter to find songs within position 1 through 10. Don’t over think it, but do recognize that the “top” of the charts are smaller numbers, not larger ones.\n\nMake a new section\nDescribe what you are doing\nDo it using the group_by/summarize method\nFilter to cut off at a logical number or rows. (i.e., don’t stop at a tie)\n\n\n\n4.11.2 Find something you want to know\nNow I want you to find something else on your own. It doesn’t matter what it is. Just find something about a performer or song you like.\n\nStart a new section with a Markdown headline\nUse Markdown text to declare what you are looking for\nFind it!\nAfter your code, explain what functions you used and why (like what did they do for you)"
  },
  {
    "objectID": "billboard-analysis.html#summarizing-by-year",
    "href": "billboard-analysis.html#summarizing-by-year",
    "title": "4  Billboard Analysis",
    "section": "4.12 Summarizing by year",
    "text": "4.12 Summarizing by year\nThere is one more important concept I’d like you to learn and that you will need later, but this is already a long chapter and you have learned a lot of new stuff.\nIf you still have spare room in your brain now, go through the Grouping by dates chapter, which uses data from this project.\nIf your brain is mush, that’s cool. Just turn this project in. But know you’ll need to go through that chapter later when you are working on a mastery project."
  },
  {
    "objectID": "billboard-analysis.html#turn-in-your-project",
    "href": "billboard-analysis.html#turn-in-your-project",
    "title": "4  Billboard Analysis",
    "section": "4.13 Turn in your project",
    "text": "4.13 Turn in your project\n\nMake sure everything runs and Renders properly.\nPublish your changes to Quarto Pub and include the link to your project in your index notebook so I can bask in your glory.\nZip your project folder. (Or export to zip if you are using posit.cloud).\nUpload to the Canvas assignment.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo be clear, it is your zipped project I am grading. The Quarto Pub link is for convenience."
  },
  {
    "objectID": "billboard-analysis.html#review-of-what-weve-learned",
    "href": "billboard-analysis.html#review-of-what-weve-learned",
    "title": "4  Billboard Analysis",
    "section": "4.14 Review of what we’ve learned",
    "text": "4.14 Review of what we’ve learned\nWe introduced a number of new functions in this lesson, most of them from the dplyr package. Mostly we filtered and summarized our data. Here are the functions we introduced in this chapter, many with links to documentation:\n\nfilter() returns only rows that meet logical criteria you specify.\nsummarize() builds a summary table about your data. You can count rows n() or do math on numerical values, like mean().\ngroup_by() is often used with summarize() to put data into groups before building a summary table based on the groups.\ndistinct() returns rows based on unique values in columns you specify. i.e., it deduplicates data.\nstr_detect() and str_like() to search within strings.\ncount() is a shorthand for the group_by/summarize operation to count rows based on groups. You can name your summary columns and sort the data within the same function."
  },
  {
    "objectID": "billboard-analysis.html#soundtrack-for-this-assignment",
    "href": "billboard-analysis.html#soundtrack-for-this-assignment",
    "title": "4  Billboard Analysis",
    "section": "4.15 Soundtrack for this assignment",
    "text": "4.15 Soundtrack for this assignment\nThis lesson was constructed while listening to The Bright Light Social Hour. They’ve not had a song on the Hot 100, at least not yet.\n\nhot100 |&gt; \n  filter(str_detect(performer, \"Bright Light\"))"
  },
  {
    "objectID": "leso-cleaning.html#about-the-story-military-surplus-transfers",
    "href": "leso-cleaning.html#about-the-story-military-surplus-transfers",
    "title": "5  Military Surplus Cleaning",
    "section": "5.1 About the story: Military surplus transfers",
    "text": "5.1 About the story: Military surplus transfers\nAfter the 2014 death of Michael Brown and the unrest that followed, there was widespread criticism of Ferguson, Mo. police officers for their use of military-grade weapons and vehicles. There was also heightened scrutiny to a federal program that transfers unused military equipment to local law enforcement. Many news outlets, including NPR and Buzzfeed News, did stories based on data from the “1033 program” handled through the Law Enforcement Support Office. Buzzfeed News also did a followup report in 2020 where reporter John Templon published his data analysis, which he did using Python.\nYou will analyze the same dataset to see how Central Texas police agencies have used the program and write a short data drop about transfers to those agencies.\n\n5.1.1 The 1033 program\nTo work through this story we need to understand how this transfer program works. You can read all about it here, but here is the gist:\nIn 1997, Congress approved the “1033 program” that allows the U.S. military to give “surplus” equipment that they no longer need to law enforcement agencies like city police forces. The program is run by the Law Enforcement Support Office, which is part of the Defense Logistics Agency (which handles the global defense supply chain for all the branches of the military) within the Department of Defense. (The program is run by the office inside the agency that is part of the department.)\nAll kinds of equipment moves between the military and these agencies, from boots and water bottles to assault rifles and cargo planes. The local agency only pays for shipping the equipment, but that shipping cost isn’t listed in the data. What is in the data is the “original value” of the equipment in dollars, but we can’t say the agency paid for it, because they didn’t.\nProperty falls into two categories: controlled and non-controlled. Controlled property “consists of military items that are provided via a conditional transfer or ‘loan’ basis where title remains with DoD/DLA. This includes items such as small arms/personal weapons, demilitarized vehicles and aircraft and night vision equipment. This property always remains in the LESO property book because it still belongs to and is accountable to the Department of Defense. When a local law enforcement agency no longer wants the controlled property, it must be returned to Law Enforcement Support Office for proper disposition.” This is explained in the LESO FAQ.\nBut most of the transfers to local agencies are for non-controlled property that can be sold to the general public, like boots and blankets. Those items are removed from the data after one year, unless it is deemed a special circumstance.\nThe agency releases data quarterly, but it is really a “snapshot in time” and not a complete history. Those non-controlled items transferred more than a year prior are missing, as are any controlled items returned to the feds.\n\n\n5.1.2 About the data\n\n\n\nThe raw data\n\n\nThe data comes in a spreadsheet that has a different tab for each state and territory. The data we’ll use here was from June 31, 2022 and I’ve done some initial work on the original data that is beyond the scope of this class, so we’ll use my copy of the data. I will supply a link to the combined data below.\nThere is no data dictionary or record layout included with the data but I have corresponded with the Defense Logistics Agency to get a decent understanding of what is included.\n\nsheet: Which sheet the data came from. This is an artifact from the data merging script.\nstate: A two-letter designation for the state of the agency.\nagency_name: This is the agency that got the equipment.\nnsn: The National Stock Number, a special ID that identifies the item in a government supplies database.\nitem_name: The item transferred. Googling the names can sometimes yield more info on specific items, or you can search by nsn for more info.\nquantity: The number of the “units” the agency received.\nui: Unit of measurement (item, kit, etc.)\nacquisition_value: a cost per unit for the item.\ndemil_code: Categories (as single letter key values) that indicate how the item should be disposed of. Full details here.\ndemil_ic: Also part of the disposal categorization.\nship_date: The date the item(s) were sent to the agency.\nstation_type: What kind of law enforcement agency made the request.\n\nHere is a glimpse of a sample of the data:\n\n\nRows: 10\nColumns: 12\n$ sheet             &lt;dbl&gt; 17, 42, 5, 45, 36, 33, 5, 22, 3, 22\n$ state             &lt;chr&gt; \"KY\", \"SC\", \"CA\", \"TX\", \"OH\", \"NC\", \"CA\", \"MI\", \"AZ\"…\n$ agency_name       &lt;chr&gt; \"MEADE COUNTY SHERIFF DEPT\", \"PROSPERITY POLICE DEPT…\n$ nsn               &lt;chr&gt; \"6115-01-435-1567\", \"1005-00-589-1271\", \"7520-01-519…\n$ item_name         &lt;chr&gt; \"GENERATOR SET,DIESEL ENGINE\", \"RIFLE,7.62 MILLIMETE…\n$ quantity          &lt;dbl&gt; 5, 1, 32, 1, 1, 1, 1, 1, 1, 1\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Dozen\", \"Each\", \"Each\", \"Each\", \"Ea…\n$ acquisition_value &lt;dbl&gt; 4623.09, 138.00, 16.91, 749.00, 749.00, 138.00, 499.…\n$ demil_code        &lt;chr&gt; \"A\", \"D\", \"A\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\"\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ ship_date         &lt;dttm&gt; 2021-03-22, 2007-08-07, 2020-10-29, 2014-11-18, 2014…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n\n\nAnd a look at just some of the more important columns:\n\nleso_sample |&gt; \n  select(agency_name, item_name, quantity, acquisition_value)\n\n\n\n  \n\n\n\nEach row of data is a transfer of a particular type of item from the U.S. Department of Defense to a local law enforcement agency. The row includes the name of the item, the quantity, and the value ($) of a single unit.\nWhat the data doesn’t have is the total value of the items in the shipment. If there are 5 generators as noted in the first row above and the cost of each one is $4623.09, we have to multiply the quantity times the acquisition_value to get the total value of that equipment.\nThe data also doesn’t have any variable indicating if an item is controlled or non-controlled, but I’ve corresponded with the Defense Logistics Agency to gain a pretty good understanding of how to isolate them based on the demilitarization codes.\nThese are things I learned about by talking to the agency and reading through documentation. This kind of reporting and understanding ABOUT your data is vital to avoid mistakes."
  },
  {
    "objectID": "leso-cleaning.html#the-questions-we-will-answer",
    "href": "leso-cleaning.html#the-questions-we-will-answer",
    "title": "5  Military Surplus Cleaning",
    "section": "5.2 The questions we will answer",
    "text": "5.2 The questions we will answer\nAll answers should be based on controlled items given to Texas agencies from Jan. 1, 2010 to present.\n\nHow many total “controlled” items were transferred to Texas agencies, and what are they all worth? We’ll summarize all the controlled items only to get the total quantity and total value of everything.\nHow many total “controlled” items did each agency get and how much was it all worth? Which agency got the most stuff?\n\nHow about local police agencies? I’ll give you a list.\n\nWhat specific “controlled” items did each agency get and how much were they worth? Now we’re looking at the kinds of items.\n\nWhat did local agencies get?\n\n\nYou’ll research some of the more interesting items the agencies received so you can include them in a short data drop."
  },
  {
    "objectID": "leso-cleaning.html#getting-started-create-your-project",
    "href": "leso-cleaning.html#getting-started-create-your-project",
    "title": "5  Military Surplus Cleaning",
    "section": "5.3 Getting started: Create your project",
    "text": "5.3 Getting started: Create your project\n\n\n\n\n\n\nNote\n\n\n\nIf you are using posit.cloud, you’ll want to refer to the Using posit.cloud chapter to create your project, using the material below as necessary.\n\n\nWe will build the same project structure that we did with the Billboard project. In fact, all our class projects will have this structure. Since we’ve done this before, some of the directions are less detailed.\n\nWith RStudio open, make sure you don’t have a project open. Go to File &gt; Close project.\nUse the create project button (or File &gt; New project) to create a new project in a New Directory.\nFor the project type, choose Quarto Website\nName the directory “yourname-military-surplus”, but with your name.\nIn the index.qmd file, replace the title value with “Military Surplus”, as that is our project title.\nRemove the boilerplate stuff after the YAML title and write in Markdown a little about what this is … a class project about surplus military equipment. You can come back to this later to flesh it out.\nCreate two folders: data-raw and data-processed."
  },
  {
    "objectID": "leso-cleaning.html#cleaning-notebook",
    "href": "leso-cleaning.html#cleaning-notebook",
    "title": "5  Military Surplus Cleaning",
    "section": "5.4 Cleaning notebook",
    "text": "5.4 Cleaning notebook\nAgain, like Billboard, we’ll create a notebook specifically for downloading, cleaning and prepping our data.\n\nCreate a new Quarto Document.\nFor the title use “Cleaning”.\nRemove the rest of the boilerplate template.\nSave the file and name it 01-cleaning.qmd.\nOpen _quarto.yml and change the navigation link from about.qmd to 01-cleaning.qmd.\n\n\n5.4.1 The goals of the notebook\nAs noted before, I separate cleaning into a separate notebook so that each subsequent analysis notebook can take advantage of that work. It’s the DRY principal in programming: Don’t Repeat Yourself. Often I will be in an analysis notebook realizing that I have more cleaning to do, so I’ll go back to the cleaning notebook, add it and rerun everything. Because I’ve worked with and researched this data, I’m aware of cleaning steps that a newcomer to the data might not be aware of at this point. To save you that time and heartache, we will take advantage of my experience and do all this cleaning work up front even though you haven’t seen the “need” for it yet.\nThat’s a long-winded opening to say let’s add our notebook goals so you know what’s going on here.\n\nIn Markdown, add a headline noting these are notebook goals.\nAdd the goals below:\n\n- Download the data\n- Import the data\n- Check datatypes\n- Create a total_value variable\n- Create a control_type variable\n- Filter the date range (since Jan. 1 2010)\n- Export the cleaned data\n\n\n5.4.2 Add a setup section\nThis is the section in the notebook where we add our libraries and such. Again, every notebook has this section, though the packages used may vary on need. This time (and each time hense) we will add some “execution options” to our setup chunk.\n\nAdd a headline called “Setup” and text about what we are doing, adding our libraries.\nAdd your setup code chunk with the code below.\n\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)\n```\n\nSome reminders:\n\nThe #| label: setup is special in that it will be run first no matter where we are in the notebook. Helpful to make sure your libraries are loaded.\nThe #| message: false option silences the message about all the different packages in the tidyverse when the library is loaded. It just cleans up the notebook.\n\nOne other note about this … we should only need the tidyverse library for this notebook because the data already has clean names (no need for janitor.)"
  },
  {
    "objectID": "leso-cleaning.html#download-the-data",
    "href": "leso-cleaning.html#download-the-data",
    "title": "5  Military Surplus Cleaning",
    "section": "5.5 Download the data",
    "text": "5.5 Download the data\n\nA new section means a new headline and description. Add it. It is good practice to describe and link to the data you will be using. You can use the text below:\n\nWhile the data we will use here if from Prof. McDonald, it is from the [Law Enforcement Support Office](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/). Find more information [about the program here](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs/).\n\n\n\n\n\n\nNote\n\n\n\nNote the text above scrolls off the screen. Use the copy icon to select all the text.\n\n\n\nUse the download.file() function to download the date into your data-raw folder. Remember you need two arguments:\n\ndownload.file(\n  \"url_to_data\",\n  \"path_to_folder/filename.csv\",\n  mode = \"wb\"\n)\nWhat you need:\n\nThe data can be found at this url below\nIt should be saved into your data-raw folder with a name for the file.\n\nhttps://github.com/utdata/rwd-r-leso/blob/main/data-processed/leso.csv?raw=true\n\n\nTry writing this on your own\n# You can comment the lines below once you have the data\ndownload.file(\n  \"https://github.com/utdata/rwd-r-leso/blob/main/data-processed/leso.csv?raw=true\",\n  \"data-raw/leso.csv\",\n  mode = \"wb\"\n)\n\n\nOnce you’ve built your code chunk and run it, you should make sure the file downloaded into the correct place: in your data-raw folder.\n\n\n\n\n\n\nNote\n\n\n\nIf you get an error about the path, you might make sure you created the data-raw folder first."
  },
  {
    "objectID": "leso-cleaning.html#import-the-data",
    "href": "leso-cleaning.html#import-the-data",
    "title": "5  Military Surplus Cleaning",
    "section": "5.6 Import the data",
    "text": "5.6 Import the data\nIt’s time to import the data into the notebook. Like the Billboard project, we are again working with a CSV, or comma-separated-values text file.\n\nAdd a new Markdown section noting you are doing the import. Include a headline, text and a new code chunk.\n\nI suggest you build the code chunk a bit at a time in this order:\n\nUse read_csv() to read the file from our data-raw folder.\nEdit that line to put the result into a tibble object using &lt;-. Name your new tibble leso.\nPrint the tibble as a table to the screen again by putting the tibble object on a new line and running it. This allows you to see it in columnar form.\n\n\n\nTry real hard first before clicking here for the answer.\n# assigning the tibble\nleso &lt;- read_csv(\"data-raw/leso.csv\")\n# printing the tibble\nleso\n\n\nThe output you get will be the table, which looks sort of like this, but I’m just showing the first six lines:\n\n\n\n\n  \n\n\n\n\n5.6.1 Glimpse the data\n\nIn a new code block, print the tibble but pipe it into glimpse() so you can see all the column names.\n\n\nleso |&gt;  glimpse()\n\nRows: 99,152\nColumns: 12\n$ sheet             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…"
  },
  {
    "objectID": "leso-cleaning.html#checking-datatypes",
    "href": "leso-cleaning.html#checking-datatypes",
    "title": "5  Military Surplus Cleaning",
    "section": "5.7 Checking datatypes",
    "text": "5.7 Checking datatypes\nTake a look at your glimpse returns. These are the things to watch for:\n\nAre your variable names (column names) clean? All lowercase with _ separating words?\nAre dates saved in a date format? ship_date looks good at &lt;dttm&gt;, which means “datetime”.\nAre your numbers really numbers? acquisition_value is the column we are most concerned about here, and it looks good.\n\nThis data set looks good (because I pre-prepared it fo you), but you always want to check and make corrections, like we did to fix the date in the Billboard assignment."
  },
  {
    "objectID": "leso-cleaning.html#remove-unnecessary-columns",
    "href": "leso-cleaning.html#remove-unnecessary-columns",
    "title": "5  Military Surplus Cleaning",
    "section": "5.8 Remove unnecessary columns",
    "text": "5.8 Remove unnecessary columns\nSometimes at this point in a project, you might not know what columns you need to keep and which you could do without. The nice thing about doing this with code in a notebook is we can always go back, make corrections and run our notebook again. In this case, we will remove the sheet variable since we don’t need it. (It’s an artifact of the processing I’ve done on the file.)\n\nStart a new section with a headline and text to explain you are removing unneeded columns.\nAdd a code chunk and the following code. I’ll explain it below.\n\n\nleso_tight &lt;- leso |&gt; \n  select(!sheet)\n\nleso_tight |&gt; glimpse()\n\nRows: 99,152\nColumns: 11\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n\n\nIn English, we are:\n\ncreating a new tibble leso_tight and then ..\nfilling it with this: The leso data AND THEN we use select to remove the column called sheet. We then glimpse the new data.\n\nWithin the select() function, we use ! to “negate” the column called “sheet”. If we wanted to remove more than one column — say both sheet and station_type — then we would use ! negate in conjunction with the c() function to combine items into a new vector, like this: select(!c(sheet, station_type)).\nThe use of ! to remove things will come up in other places in R, as will using c() to combine things together.\nSo now we have a tibble called leso_tight that we will work with in the next section."
  },
  {
    "objectID": "leso-cleaning.html#create-a-total_value-column",
    "href": "leso-cleaning.html#create-a-total_value-column",
    "title": "5  Military Surplus Cleaning",
    "section": "5.9 Create a total_value column",
    "text": "5.9 Create a total_value column\nDuring my reporting about this data I learned that the acquisition_value noted in the data is for a single “unit” of each item. If the shipment item was a rifle with a quantity of “5” and acquisition_value of “200”, then each rifle is worth $200, but the total shipment would be 5 times $200, or $1,000. That $1000 total value is not listed in the data, so we need to add it.\nLet’s walk through how to do that with a different example.\nWhen we used mutate() to convert the date in the Billboard assignment, we were reassigning values in each row of a column back into the same column.\nIn this assignment, we will use mutate() to create a new column with new values based on a calculation. Let’s review the concept first.\nIf you started with data like this:\n\n\n\nitem\nitem_count\nitem_value\n\n\n\n\nBread\n2\n1.5\n\n\nMilk\n1\n2.75\n\n\nBeer\n3\n9\n\n\n\nAnd then wanted to create a total value of each item in the table, you would use mutate():\n# Don't put this in your notebook. It's just explanation.\ndata |&gt; \n  mutate(total_value = item_count * item_value)\nYou would get a return like this, with your new total_value column added at the end:\n\n\n\nitem\nitem_count\nitem_value\ntotal_value\n\n\n\n\nBread\n2\n1.5\n3\n\n\nMilk\n1\n2.75\n2.75\n\n\nBeer\n3\n9\n27\n\n\n\nOther math operators work as well: +, -, * and /.\nSo, now that we’ve talked about how it is done, I want you to:\n\nCreate a new section with headline, text and code chunk.\nUse mutate() to create a new total_value column that multiplies quantity times acquisition_value.\nAssign those results into a new tibble called leso_total so we can all be on the same page.\nGlimpse the new tibble so you can check the results.\n\n\n\nTry it on your own. You can figure it out!\nleso_total &lt;- leso_tight |&gt; \n  mutate(\n    total_value = quantity * acquisition_value\n  )\n\nleso_total |&gt; glimpse()\n\n\nRows: 99,152\nColumns: 12\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n$ total_value       &lt;dbl&gt; 658000.00, 16260.00, 3339.00, 158715.90, 10000.00, 6…\n\n\n\n5.9.1 Check that it worked!!\nNote that new columns are added at the end of the tibble. Sometimes you can look through the glimpsed data to see if your mutate worked correctly, but that depends on the data.\nIf there isn’t enough information visible, you might try printing out some rows of the data to peek through it.\n\nEdit your chunk on the last line to change glimpse() to head(20)\n\nI’m just showing that last line here:\n\nleso_total |&gt; head(20)\n\n\n\n  \n\n\n\nAt this point you can page through the columns to check the math. But even with this, there are a lot of columns to page through. You can use add on select() to pluck out the columns of interest to check.\n\nEdit your chunk’s last line like this:\n\n\nleso_total |&gt; \n  head(20) |&gt;\n  select(agency_name, quantity, acquisition_value, total_value)\n\n\n\n  \n\n\n\nThis should allow you to check if the math worked. As of this writing, there was a record for “ABBEVILLE POLICE DEPT” that has a quantity of “10” and acquisition_value of “15871.59” and I got a total_value of “158715.90”. So, 10 * 15871.59 = 158715.90, which is correct!\nSomething to note about this process: In the first part of this chunk we are taking leso_tight, making some changes to it and the saving it into leso_total. We have two objects now, one before the changes and one after.\nIn the last part of the chunk, we are taking leso_total object and we are peeking at some rows and some columns, but we are not saving those changes into a new object. There is no &lt;- operator here. We are just printing the results to the screen so we can see parts of the data more clearly."
  },
  {
    "objectID": "leso-cleaning.html#controlled-vs.-non-controlled",
    "href": "leso-cleaning.html#controlled-vs.-non-controlled",
    "title": "5  Military Surplus Cleaning",
    "section": "5.10 Controlled vs. non-controlled",
    "text": "5.10 Controlled vs. non-controlled\nAgain, by reading through the documentation about this data I learned about controlled vs non-controlled property. Basically non-controlled generic stuff like boots and blankets are removed from the data after one year, but controlled items like guns and airplanes remain on the list until they are returned to the military for disposal. We are only concerned with the controlled items.\nThere isn’t anything within the data that says it is “controlled” and really no clear indication in the documentation on how to tell what is what. So, I emailed the agency and asked them. Here is an edited version of their answer:\n\nProperty with the DEMIL codes A and Q6 are considered non-controlled general property and fall off the LESO property books after one year. All other Demil codes are considered controlled items and stay on the LESO property book until returned to DLA for disposition/disposal. However, there are some exceptions. For instance, aircraft are always controlled regardless of the demil code. Also, LESO has the discretion to keep items as controlled despite the demil code. This happens with some high value items. There isn’t a standard minimum value. It also may also depend on the type of property.\n\nThis actually took some back and forth to figure out, as I had noticed there were AIRPLANE, CARGO-TRANSPORT items in the data that were older than a year, along with some surveillance robots. That’s when they replied about the airplanes, but it turns out the robots were simply an error. Data is dirty when humans get involved; somebody coded it wrong.\nThese “DEMIL codes” they referenced are the demil_code and demil_ic columns in the data, so we can use those to mark which records are “non-controlled” (A and Q6) and then mark all the rest as “controlled”. We know of one exception – airplanes – which we need to mark controlled. We can’t do much with the “high value” items since there isn’t logic available to find them. We’ll just have to note that in our story, something like “the agency has discretion to designate as as controlled, such as high value items, but they are not categorized as such and may have been dropped from our analysis.”\nBut, we can catch most of them … we just need to work through the logic. This is not an uncommon challenge in data science, so we have some tools to do this. Our workhorse here is the case_when() function, where we can make decisions based on values in our data.\nI usually approach this by thinking of the logic first, then writing some code, then testing it. Sometimes my logic is faulty and I have to try again, which is why we test the results. Know this could go on for many cycles. In the interest of time and getting this done, I will just show the finished code and explain how it works.\nHere is the basic idea:\n\nWe want to create a new column to denote if the item is controlled. In that column we want it to be TRUE when an item is controlled, and FALSE when it is not.\nWe know that items with “AIRPLANE” are always controlled, no matter their demil designations.\nOtherwise we know that items that have a demil_code of “A”, OR a demil_code of “Q” AND a demil_id of “6”, are non-controlled.\nEverything else is controlled.\n\nI’ve noted this logic in a specific order for a reason: It’s the order that we write the logic in the code based on how the function case_when() works.\n\n5.10.1 Categorization logic with case_when()\nWe will use the mutate() function to create a new column called control_type. We’ve done that before, so no problem.\nBut this time we will fill in values in the new column based on other data inside each row. case_when() allows us to create a test (or number of tests) and then mark the new value based on the answer. Once new data has been written the function goes to the next row, so we write the most specific rules first.\nThis process is powerful and can get complicated depending on the logic needed. This example is perhaps more complicated than I like to use when explaining this concept, but this is real data and we need this, so here we go.\n\nStart a new section and explain that you are marking controlled items.\nCopy/paste the code chunk below and run it\nREAD the explanation that follows, so you understand what is going on!\n\n\nleso_control &lt;- leso_total |&gt; \n  mutate(\n    control_type = case_when(\n      str_detect(item_name, \"AIRPLANE\") ~ TRUE,\n      (demil_code == \"A\" | (demil_code == \"Q\" & demil_ic == 6)) ~ FALSE,\n      TRUE ~ TRUE\n    )\n  )\n\nleso_control |&gt; glimpse()\n\nRows: 99,152\nColumns: 13\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n$ total_value       &lt;dbl&gt; 658000.00, 16260.00, 3339.00, 158715.90, 10000.00, 6…\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\nOK, let’s go through the code line-by-line.\n\nOur first line creates a new tibble leso_control and fills it with the result of the rest of our expression. We start with the leso_total tibble.\nWe mutate the data and start with the name of new column: control_type. We are filling that column with the result of the case_when() function for each row.\nWithin the case_when() we are making the determination if the item is controlled or not. The left side of the ~ is the test, and the right side of ~ is what we enter into the column. But we have more than one test:\n\nThe first test is we use the str_detect() function to look inside the item_name field looking for the term “AIRPLANE”. If the test finds the term, then the control_type field gets a value of TRUE and we move to the next row. If not, it moves to the next rule to see if that is a match. (We could fill this column with any text or number we want, but we are using TRUE and FALSE because that is the most basic kind of data to keep. If the item is controlled, set the value is TRUE. If not, it should be set to FALSE.)\nOur second rule has two complex tests and we want to mark the row FALSE if either are true. (Remember, this is based on what the DLA told me: items with A or Q6 are non-controlled.) Our case_when() logic first looks for the value “A” in the demil_code field. If it is yes, then it marks the row FALSE. If no it goes to the next part: Is there a “Q” in the demil_code field AND a “6” in the demil_ic field? Both “Q” and “6” have to be there to get marked as FALSE. If both fail, then we move to the next test.\nThe last test is our catch-all. If none of the other rules apply, then mark this row as TRUE, which means it is controlled. So our default in the end is to mark everything TRUE if any of the other rules don’t mark it first.\n\nLastly we glimpse at the data just so we can see the column was created.\n\nAs I said, we skipped the process of figuring all that out line-by-line, but I’ll show some tests here to show that we did what we were intending.\nThis shows airplanes are marked as controlled with TRUE.\n\n# showing the results and some columns that determined them\nleso_control |&gt; \n  select(item_name, demil_code, demil_ic, control_type) |&gt; \n  filter(str_detect(item_name, \"AIRPLANE\"))\n\n\n\n  \n\n\n\nThis shows how many items are marked TRUE vs FALSE for each demil_code and demil_ic combination. Don’t sweat over this code as we cover it in later chapters, but know I used it to check that most A records were FALSE, along with Q6.\n\nleso_control |&gt; \n  count(demil_code, demil_ic, control_type, name = \"cnt\") |&gt; \n  pivot_wider(names_from = control_type, values_from = cnt)\n\n\n\n  \n\n\n\nOK, onto the next task to get data for specific dates."
  },
  {
    "objectID": "leso-cleaning.html#filtering-our-data",
    "href": "leso-cleaning.html#filtering-our-data",
    "title": "5  Military Surplus Cleaning",
    "section": "5.11 Filtering our data",
    "text": "5.11 Filtering our data\nIn the Billboard lesson you used filter() to get No. 1 songs and to get a date range of data. We need to do something similar here to get only data of a certain date range. We’ll build the filters one at a time so we can save the resulting objects separately.\n\nCreate a new section with headlines and text that denote you are filtering the data to records since Jan. 1, 2010 up to the most recent full year.\nCreate a chunk and filter your leso_control data to get rows with a ship_date later than 2010-01-01.\nSave the result into a new tibble called leso_dated.\nPrint out the new tibble leso_dated.\n\n\n\nWrite it yourself, but here is the code\nleso_dated &lt;- leso_control |&gt; \n  filter(\n    ship_date &gt;= \"2010-01-01\"\n  )\n\nleso_dated\n\n\n\n5.11.1 Checking the results with summary()\nHow do you know this date filter worked? Well, my data went from 99152 to 67603 rows, so we did something. You might look at the results table and click over to the ship_date columns so you can see some of the results, but you can’t be sure the top row is the oldest. We could use an arrange() to test that, but I have another suggestion: summary().\nNow, summary() is different than summarize(), which we’ll do plenty of in a minute. The summary() function will show you some results about each column in your data, and when it is a number or date, it will give you some basic stats like min, max and median values.\n\nEdit your chunk to not just print out the tibble, but pipe that into summary(), like this:\n\n\nleso_dated |&gt; summary()\n\nYou should be be able to look at the Min value of ship_date in the summary to make sure there are not dates before 2010. Here is what that summary looks like:\n\n\n    state           agency_name            nsn             item_name        \n Length:67603       Length:67603       Length:67603       Length:67603      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    quantity             ui            acquisition_value   demil_code       \n Min.   :    0.00   Length:67603       Min.   :       0   Length:67603      \n 1st Qu.:    1.00   Class :character   1st Qu.:     204   Class :character  \n Median :    1.00   Mode  :character   Median :     499   Mode  :character  \n Mean   :    5.49                      Mean   :   22162                     \n 3rd Qu.:    1.00                      3rd Qu.:    3578                     \n Max.   :44000.00                      Max.   :22000000                     \n                                                                            \n    demil_ic       ship_date                      station_type      \n Min.   :0.000   Min.   :2010-01-05 00:00:00.00   Length:67603      \n 1st Qu.:1.000   1st Qu.:2012-03-02 00:00:00.00   Class :character  \n Median :1.000   Median :2014-08-06 00:00:00.00   Mode  :character  \n Mean   :1.445   Mean   :2015-06-26 03:36:05.45                     \n 3rd Qu.:1.000   3rd Qu.:2018-02-21 00:00:00.00                     \n Max.   :7.000   Max.   :2023-06-30 06:06:28.00                     \n NA's   :5553                                                       \n  total_value       control_type   \n Min.   :       0   Mode :logical  \n 1st Qu.:     371   FALSE:7008     \n Median :     749   TRUE :60595    \n Mean   :   24179                  \n 3rd Qu.:    6276                  \n Max.   :22000000                  \n                                   \n\n\nWe now have an object, leso_dated that has all the data since 2010. We’ll export this for our next notebook."
  },
  {
    "objectID": "leso-cleaning.html#leso-export",
    "href": "leso-cleaning.html#leso-export",
    "title": "5  Military Surplus Cleaning",
    "section": "5.12 Export cleaned data",
    "text": "5.12 Export cleaned data\nNow that we have our data selected, mutated and filtered how we want it, we can export into .rds files to use in other notebooks. As you may recall from Billboard, we use the .rds format because it will remember data types and such.\n\nCreate a new section with headline and text explaining that you are exporting the data.\nDo it.\n\nThe function you need is called write_rds and you need to give it a path/name that saves the file in the data-processed folder. Name the file with 01- at the beginning so you know it came from the first notebook, and then use a short and descriptive name for the rest. No spaces. Well-formatted, descriptive file names are important to your future self and other colleagues.\nHere is my version:\n\n\nThe last chunk. You can do it.\nleso_dated |&gt; write_rds(\"data-processed/01-leso-all.rds\")"
  },
  {
    "objectID": "leso-cleaning.html#render-and-clean-up-your-notebook",
    "href": "leso-cleaning.html#render-and-clean-up-your-notebook",
    "title": "5  Military Surplus Cleaning",
    "section": "5.13 Render and clean up your notebook",
    "text": "5.13 Render and clean up your notebook\nYou should Render your notebook and read it over carefully. Some things to look for:\n\nMake sure you have nice headlines for each of your sections.\nMake sure your navigation works for the Home and Cleaning pages.\nFlesh out your index.qmd file to make sure the basic assignment information is there."
  },
  {
    "objectID": "leso-cleaning.html#things-we-learned-in-this-lesson",
    "href": "leso-cleaning.html#things-we-learned-in-this-lesson",
    "title": "5  Military Surplus Cleaning",
    "section": "5.14 Things we learned in this lesson",
    "text": "5.14 Things we learned in this lesson\nThis chapter was similar to when we imported data for Billboard, but we did introduce a couple of new concepts:\n\ncase_when() allows you to categorize a new column based on logic within your data.\nsummary() gives you descriptive statistics about your tibble. We used it to check the “min” date, but you can also see averages (mean), max and medians."
  },
  {
    "objectID": "leso-analysis.html#learning-goals-of-this-lesson",
    "href": "leso-analysis.html#learning-goals-of-this-lesson",
    "title": "6  Military Surplus Analysis",
    "section": "6.1 Learning goals of this lesson",
    "text": "6.1 Learning goals of this lesson\nIn this chapter we will start querying the data using summarize with math, basically adding values in a column instead of counting rows, which we did with the Billboard assignment.\nOur learning goals are:\n\nTo use the combination of group_by(), summarize() and arrange() to add columns of data using sum().\nTo use different group_by() groupings in specific ways to get desired results.\nTo practice using filter() on those summaries to better see certain results, including filtering within a vector (or list of strings).\nWe’ll research and write about some of the findings, practicing data-centric ledes and sentences describing data."
  },
  {
    "objectID": "leso-analysis.html#questions-to-answer",
    "href": "leso-analysis.html#questions-to-answer",
    "title": "6  Military Surplus Analysis",
    "section": "6.2 Questions to answer",
    "text": "6.2 Questions to answer\nAll answers should be based on controlled items given to Texas agencies from Jan. 1, 2010 to present.\n\nHow many total “controlled” items were transferred to Texas agencies, and what are they all worth? We’ll summarize all the controlled items only to get the total quantity and total value of everything.\nHow many total “controlled” items did each agency get and how much was it all worth? Which agency got the most stuff?\n\nHow about local police agencies? I’ll give you a list.\n\nWhat specific “controlled” items did each agency get and how much were they worth? Now we’re looking at the kinds of items.\n\nWhat did local agencies get?\n\n\nYou’ll research some of the more interesting items the agencies received so you can include them in your data drop."
  },
  {
    "objectID": "leso-analysis.html#set-up-the-analysis-notebook",
    "href": "leso-analysis.html#set-up-the-analysis-notebook",
    "title": "6  Military Surplus Analysis",
    "section": "6.3 Set up the analysis notebook",
    "text": "6.3 Set up the analysis notebook\nBefore we get into how to do this, let’s set up our analysis notebook.\n\nMake sure you have your military surplus project open in RStudio. If you have your import notebook open, close it and use Run &gt; Restart R and Clear Output.\nCreate a new Quarto Document with the title “Texas Analysis”.\nSave the notebook at 02-analysis.qmd.\nUpdate your _quarto.yml file to add the 02-analysis.qmd to your navigation.\nRemove the boilerplate text.\nCreate a setup section (headline, text and code chunk) that loads the tidyverse library. Include the same execution options we had in the cleaning notebook.\n\nWe’ve started each notebook like this, so you should be able to do this on your own now. That said, here is the setup chunk with options:\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)\nlibrary(janitor)\n```\n\n\n6.3.1 Load the data into a tibble\n\nNext create an import section (headline, text and chunk) that loads the data from the previous notebook and save it into a tibble called leso.\nAdd a glimpse() of the data for your reference.\n\nWe did this in Billboard and you should be able to do it. You’ll use read_rds() and find your data in your data-processed folder.\n\n\nImport code\nleso &lt;- read_rds(\"data-processed/01-leso-all.rds\")\n\nleso |&gt; glimpse()\n\n\nYou should see the leso object in your Environment tab. You also have the glimpse like above so you an idea of what each variable is in the data.\nAs a review, here is what the glimpse looks like:\n\n\nRows: 67,603\nColumns: 13\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n$ total_value       &lt;dbl&gt; 658000.00, 16260.00, 3339.00, 158715.90, 10000.00, 6…\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…"
  },
  {
    "objectID": "leso-analysis.html#filter-to-data-of-interest",
    "href": "leso-analysis.html#filter-to-data-of-interest",
    "title": "6  Military Surplus Analysis",
    "section": "6.4 Filter to data of interest",
    "text": "6.4 Filter to data of interest\nFor this analysis we only want to look at “controlled” items, and only those items given to Texas agencies. We’ll build these filters separately so we can save the data at each step.\n\n6.4.1 Get the controlled items\nFor this analysis we want to focus on “controlled” items vs the more generic non-controlled items we learned about in the documentation. Let’s filter to capture just the controlled data for our analysis.\nAs you might recall from the Billboard project, the filter() function is our workhorse for focusing our data. In our import notebook we created our control_type column so we could do exactly this: Find only the rows of “controlled” items.\n\nStart a new Markdown section and note you are getting controlled items.\n\nLet’s first count how many of these items are controlled vs. non-controlled. This is much like counting songs by performer: We can use GSA and n() to count the number of rows for control_type, the column we created in the cleaning notebook.\n\nTry to write a quick count of the leso data that uses group_by() on the control_type column and summarizes using n().\n\n\n\nTry yourself. The result is below.\nleso |&gt; \n  group_by(control_type) |&gt; \n  summarise(number_items = n())\n\n\n\n\n  \n\n\n\nWe can see from this result that there are 7008 items that are not controlled, and 60595 that are. When we write our filter, we should end up with 60595 observations, which is only the TRUE ones.\n\nStart with your leso data, but then filter it to control_type == TRUE.\nSave the result into a new tibble called leso_c.\n\n\n\nFiltering control type\nleso_c &lt;- leso |&gt; \n  filter(control_type == TRUE)\n\n\nTo test that this worked you could do the same GSA on control_type we used above, or even glimpse() which shows the number of rows.\nIf you really wanted to just count the number of rows, there is another function, nrow().\n\nleso_c |&gt; nrow()\n\n[1] 60595\n\n\nAt this point you have a new tibble called leso_c that has only the weapons and other controlled property, so now we can take a closer look at that data.\n\n\n6.4.2 Filter for Texas\nIn this analysis we only want to look at Texas data so we’ll make a new object with just that. Remember, you want to do each step and run it to make sure it is working.\n\nCreate a new section with headlines and text that denote you are filtering the data to Texas.\nCreate the code chunk and start your filter process using the leso_c tibble.\nUse filter() on the state column to keep all rows with “TX”.\nEdit the chunk to put all that into a new object called leso_c_tx.\nPrint out the tibble.\n\n\n\nYou know the drill by now\nleso_c_tx &lt;- leso_c |&gt; \n  filter(\n    state == \"TX\"\n  )\n\nleso_c_tx\n\n\n\n\n  \n\n\n\nHow do you know if it worked? Well the first column in the data is the state column, so they should all start with “TX”. We are also down to 4756 rows."
  },
  {
    "objectID": "leso-analysis.html#building-summaries-with-math",
    "href": "leso-analysis.html#building-summaries-with-math",
    "title": "6  Military Surplus Analysis",
    "section": "6.5 Building summaries with math",
    "text": "6.5 Building summaries with math\nAs we get into the first quest, let’s talk about “how” we go about these summaries.\nWhen I am querying my data, I start by envisioning what the result should look like. Let’s take the first question: How many total “controlled” items were transferred to Texas agencies, and what are they all worth?\nLet’s glimpse the data …\n\nleso_c_tx |&gt; glimpse()\n\nRows: 4,756\nColumns: 13\n$ state             &lt;chr&gt; \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\"…\n$ agency_name       &lt;chr&gt; \"ABERNATHY POLICE DEPT\", \"ABERNATHY POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"1240-01-540-3690\", \"2320-01-371-9584\", \"1005-00-856…\n$ item_name         &lt;chr&gt; \"SIGHT,REFLEX\", \"TRUCK,UTILITY\", \"RIFLE,5.56 MILLIME…\n$ quantity          &lt;dbl&gt; 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50, 4, 9, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Eac…\n$ acquisition_value &lt;dbl&gt; 371.00, 62627.00, 120.00, 120.00, 120.00, 96466.00, …\n$ demil_code        &lt;chr&gt; \"Q\", \"C\", \"D\", \"D\", \"D\", \"C\", \"D\", \"D\", \"Q\", \"C\", \"D…\n$ demil_ic          &lt;dbl&gt; 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, NA, 1, 3, …\n$ ship_date         &lt;dttm&gt; 2016-02-02, 2016-03-07, 2011-09-13, 2011-09-13, 201…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n$ total_value       &lt;dbl&gt; 1855.0, 62627.0, 120.0, 120.0, 120.0, 96466.0, 108.0…\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\n… and then talk through the logic:\n\n“How many total controlled items” is how many individual things were transferred. We have this quantity column that has the number of items in each row, so if we want the total for the data set, we can add together all the values in that column. We do this within a summarize() but instead of counting rows using n(), we’ll use the function sum(quantity) which will add all the values in quantity column together.\n“… what are they all worth” is very similar, but we want to add together all those values in our total_value column. (Remember, we don’t want to use acquisition_value because that is the value of only ONE item, not the total for the row.\n\n\n6.5.1 Summarize across the data\nSo, let’s put this together with code.\n\nStart a new Markdown section that you are getting total values.\nAdd a code chunk like below and run it.\n\n\nleso_c_tx |&gt; \n  summarise(\n    sum(quantity),\n    sum(total_value)\n  )\n\n\n\n  \n\n\n\nWalking through the code …\n\nWe start with the leso_c_tx tibble of the controlled items.\nThen we pipe into summarize(). Because we are going to add multiple things, I put them on separate lines just to make this more readable.\n\nYou’ll notice that the names of the columns are the function names. We can “name” our new columns just like we did in Billboard. We could call this whatever we want, but good practice is to name it what it is. We’ll use good naming techniques and split the words using _. I also use all lowercase characters.\n\nEdit your chunk to add in the new column names and run it.\n\n\nleso_c_tx |&gt; \n  summarise(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  )\n\n\n\n  \n\n\n\nOK, from this we have learned something: The summed_quantity value there is the number of items all Texas law enforcement agencies have received, and the summed_total_value is how much they were worth.\n\nWrite a sentence in your notebook summing up the totals in a way that you could drop into a story.\n\nWe’ll do this again below with grouping, but first we need to talk about NA values.\n\n\n6.5.2 NA values in a sum, mean and median\nWhen we do math like this within summarize we need to take special note if our column has any blank values, called NA, as in “Not Available”. If there are, then you will get NA for the result. R will NOT do the math on the remaining values unless you tell it so. This is true not only for sum(), but also for mean() which gets an average, and for median() which finds the middle number in a column.\nThere is a way to get around this by including an argument within the mathy function: sum(col_name, na.rm = TRUE).\nI can show this with the demil_ic column which is a number datatype with some NA values. To be clear, the demil_ic variable isn’t really designed to do math on it as it is really a category, but it will show what I’m talking about here.\n\n# You don't have to add this to your notebook.\n# I'm just explaining concepts\nleso_c_tx |&gt; \n  summarise(\n    dumb_sum = sum(demil_ic),\n    less_dumb_sum = sum(demil_ic, na.rm = TRUE),\n    dumb_avg = mean(demil_ic),\n    less_dumb_avg = mean(demil_ic, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\nSo there you have examples of using sum() and mean() with and without na.rm = TRUE. OK, you’ve been warned."
  },
  {
    "objectID": "leso-analysis.html#totals-by-agency",
    "href": "leso-analysis.html#totals-by-agency",
    "title": "6  Military Surplus Analysis",
    "section": "6.6 Totals by agency",
    "text": "6.6 Totals by agency\nOK, your next question is this: For each agency, how many things did they get and how much was it all worth?\nThe key part of thinking about this logically is For each agency. That “for each” is a clue that we need group_by() for something. We basically need what we did above, but we first need to group_by the agency_name.\nLet’s break this question down:\n\n“For each agency” tells me I need to group_by the agency_name so I can summarize totals within each agency.\n“how many total things” means how many items. Like before, we have the quantity variable, so we need to add all those together within summarize like we did above.\n“how much was it worth” is another sum, but this time we want to sum the total_value column\n\nSo I envision my result looking like this:\n\n\n\nagency_name\nsummed_quantity\nsummed_total_value\n\n\n\n\nAFAKE POLICE DEPT\n6419\n10825707.5\n\n\nBFAKE SHERIFF’S OFFICE\n381\n3776291.52\n\n\nCFAKE SHERIFF’S OFFICE\n270\n3464741.36\n\n\nDFAKE POLICE DEPT\n1082\n3100420.57\n\n\n\nThe first columns in that summary will be our grouped values. This example is only grouping by one thing, agency_name. The other two columns are the summed values I’m looking to generate.\n\n6.6.1 Group_by, then summary with math\nWe’ll start with the total_quantity.\n\nAdd a new section (headline, text and chunk) that describes the second quest: For each agency in Texas, find the summed quantity and summed total value of the equipment they received.\nAdd the code below into the chunk and run it.\n\n\nleso_c_tx |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity)\n  )\n\n\n\n  \n\n\n\nLet’s break this down a little.\n\nWe start with the leso_c_tx, which is the “controlled” data, and then …\nWe group by agency_name. This organizes our data (behind the scenes) so our summarize actions will happen within each agency. Now I normally say run your code one line at a time, but you would not be able to see the groupings at this point, so I usually write group_by() and summarize() together.\nIn summarize() we first name our new column: summed_quantity, then we set that column to equal = the sum of all values in the quantity column. sum() is the function, and we feed it the column we want to add together: quantity.\nI put this inside of the summarize function in its own line because we will add to it. I enhances readability. RStudio will help you with the indenting, etc.\n\nIf you look at the first line of the return, it is taking all the rows for the “ABERNATHY POLICE DEPT” and then adding together all the values in the quantity field.\n\n\n6.6.2 Add the total_value\nWe don’t have to stop at one summary. We can perform multiple summarize actions on the same or different columns within the same expression.\nEdit your summary chunk to:\n\nAdd add a comma after the first summarize action.\nAdd the new expression to give us the summed_total_value and run it.\n\n\nleso_c_tx |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  )\n\n\n\n  \n\n\n\n\n\n6.6.3 Check the math\nIf you wanted to test this (and that is a real good idea), you might look at the data from one of the values and check the math. Here are the Abernathy rows. I usually do these tests in a code chunk of their own, and sometimes I delete them after I’m sure my logic works.\n\nleso_c_tx |&gt; \n  filter(agency_name == \"ABERNATHY POLICE DEPT\") |&gt; \n  select(agency_name, quantity, total_value)\n\n\n\n  \n\n\n\nIf we look at the quantity column there we can eyeball all the rows and add them on a calculator. Our answer should match our summed_quantity in the summary table.\nIf we add up the total_value rows then we should end up with the same number as summed_total_value above.\n\n\n6.6.4 Arrange the results\nOK, this gives us our answers, but in alphabetical order. We want to arrange the data so it gives us the most summed_total_value in descending order.\n\nEDIT your block to add an arrange() function below\n\n\nleso_c_tx |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n\n\n  \n\n\n\nSo now we’ve sorted the results to put the highest summed_total_value at the top.\nRemember, there are two ways we can set up that arrange() function in descending order:\n\narrange(summed_total_value |&gt; desc())\narrange(desc(summed_total_value))\n\nBoth work and are correct. It really is your preference.\n\n\n6.6.5 Consider the results\nIs there anything that sticks out in that list? It helps if you know a little bit about Texas cities and counties, but here are some thoughts to ponder:\n\nHouston is the largest city in the state (4th largest in the country, in fact). It makes sense that it tops the list. Same for Harris County or even the state police force. Austin being up there is also not crazy, as we have almost a million people.\nBut what about San Marcos (pop. 63,220)? Or Milam County (pop. 24,770)? Those are way smaller cities and law enforcement agencies. They might be worth looking into.\n\nPerhaps we should look at some of the police agencies closest to us."
  },
  {
    "objectID": "leso-analysis.html#looking-a-local-agencies",
    "href": "leso-analysis.html#looking-a-local-agencies",
    "title": "6  Military Surplus Analysis",
    "section": "6.7 Looking a local agencies",
    "text": "6.7 Looking a local agencies\nOur second quest had a second part: How does this look for local police agencies?\nWe’ll take the summary above, but then filter it to show only local agencies of interest.\n\n6.7.1 Save our “by agency” list\nSince we want to take an existing summary and add more filtering to it, it makes sense to go back into that chunk and save it into a new object so we can reuse it.\n\nEDIT your existing summary chunk to save the result into a new tibble. Name it tx_agency_totals so we are all on the same page.\nAdd a new line that prints the result to the screen so you can still see it.\n\n\n# adding the new tibble object in next line\ntx_agency_totals &lt;- leso_c_tx |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n# peek at the result\ntx_agency_totals\n\n\n\n  \n\n\n\nThe result is the same, but we can reuse the tx_agency_totals tibble.\n\n\n6.7.2 Filtering within a vector\nLet’s talk through the filter concepts before you try it with this data.\nWhen we talked about filtering with the Billboard project, we discussed using the | operator as an “OR” function. If we were to apply that logic here, it would look like this:\n# Just for explanation. Don't add to notebook.\ndata |&gt; \n  filter(column_name == \"Text to find\" | column_name == \"More text to find\")\nThat can get pretty unwieldy if you have more than a couple of things to look for.\nThere is another operator %in% where we can search for multiple items from a list. (This list of terms is officially called a vector, but whatever.) Think of it like this in plain English: Filter the column for things in this list.\n# Just for explanation. Don't add to notebook.\ndata |&gt; \n  filter(col_name %in% c(\"This string\", \"That string\"))\nWe can take this a step further by saving the items in our list into an R object so we can reuse that list and not have to type out all the terms each time we use them.\n# Just for explanation. Don't add to notebook.\nlist_of_strings &lt;- c(\n  \"This string\",\n  \"That string\"\n)\n\ndata |&gt; \n  filter(col_name %in% list_of_strings)\n\n\n6.7.3 Create a vector to build this filter\nIn the interest of time, I’m going to give you the list of local police agencies to filter on. To be clear, I did considerable work to figure out the exact names of these agencies. I consulted a different data set that lists all law enforcement agencies in Texas and then I used some creative filtering to find their “official” names in the leso data. It also helps that I’m familiar with local cities and counties so I can recognize the names. I don’t want to get sidetracked on that process here so I’ll give you the list and show you how to use it.\n\nCreate a new section (headline, text and chunk) and describe you are filtering the summed quantity/values for local agencies.\nAdd the code below into that chunk.\n\n\nlocal_agencies &lt;- c(\n  \"AUSTIN PARKS POLICE DEPT\", #NI\n  \"AUSTIN POLICE DEPT\",\n  \"BASTROP COUNTY SHERIFF'S OFFICE\",\n  \"BASTROP POLICE DEPT\",\n  \"BEE CAVE POLICE DEPT\",\n  \"BUDA POLICE DEPT\",\n  \"CALDWELL COUNTY SHERIFFS OFFICE\",\n  \"CEDAR PARK POLICE DEPT\",\n  \"ELGIN POLICE DEPARTMENT\",\n  \"FLORENCE POLICE DEPT\", #NI\n  \"GEORGETOWN POLICE DEPT\",\n  \"GRANGER POLICE DEPT\", #NI\n  \"HAYS CO CONSTABLE PRECINCT 4\",\n  \"HAYS COUNTY SHERIFFS OFFICE\",\n  \"HUTTO POLICE DEPT\",\n  \"JARRELL POLICE DEPT\", #NI\n  \"JONESTOWN POLICE DEPT\", #NI\n  \"KYLE POLICE DEPT\",\n  \"LAGO VISTA POLICE DEPT\",\n  \"LAKEWAY POLICE DEPT\", #NI\n  \"LEANDER POLICE DEPT\",\n  \"LIBERTY HILL POLICE DEPT\", #NI\n  \"LOCKHART POLICE DEPT\",\n  \"LULING POLICE DEPT\",\n  \"MANOR POLICE DEPT\",\n  \"MARTINDALE POLICE DEPT\", #NI\n  \"PFLUGERVILLE POLICE DEPT\",\n  \"ROLLINGWOOD POLICE DEPT\", #NI\n  \"SAN MARCOS POLICE DEPT\",\n  \"SMITHVILLE POLICE DEPT\", #NI\n  \"SUNSET VALLEY POLICE DEPT\", #NI\n  \"TAYLOR POLICE DEPT\", #NI\n  \"THRALL POLICE DEPT\", #NI\n  # TEXAS STATE UNIVERSITY HI_ED\n  \"TRAVIS COUNTY SHERIFFS OFFICE\",\n  # TRAVIS CONSTABLE OFFICE,\n  # SOUTHWESTERN UNIVERSITY HI_ID\n  \"WESTLAKE HILLS POLICE DEPT\", #NI\n  \"UNIV OF TEXAS SYSTEM POLICE HI_ED\",\n  \"WILLIAMSON COUNTY SHERIFF'S OFFICE\"\n)\n\ntx_agency_totals |&gt; \n  filter(agency_name %in% local_agencies)\n\n\n\n  \n\n\n\nLet’s walk through that code and my notes there.\n\nWe start by giving our list of agencies a name: local_agencies. This creates an R object that we can reuse. We will need this list a number of times, and it makes sense to manage it in once place instead of each time we need it.\nNext we fill that object with a list of agency names. Again, I did some pre-work to figure out those names that we aren’t covering here, and in some cases there is a comment #NI next to them. That is a note to myself that the particular agency is NOT INCLUDED in our data, which means I haven’t confirmed the name spelling. It could be at a later date the Austin Parks department gets equipment, but they list their name as “CITY OF AUSTIN PARKS” instead of “AUSTIN PARKS POLICE DEPT” and it would not be filtered properly. This is another example that your most important audience for your code is your future self.\nNow that our list is created, we can use it to filter our tx_agency_totals data. So, we start with that data, and then …\nWe pipe into filter(), but inside our filter we don’t set it == to a single thing, instead we say: agency_name %in% local_agencies, which says look inside that agency_name column and keep any row that has a value that is also in our local_agencies vector.\n\nThis filters our list of agencies to just those in Central Texas. Do you see anything interesting there?"
  },
  {
    "objectID": "leso-analysis.html#types-of-items-shipped-to-each-agency",
    "href": "leso-analysis.html#types-of-items-shipped-to-each-agency",
    "title": "6  Military Surplus Analysis",
    "section": "6.8 Types of items shipped to each agency",
    "text": "6.8 Types of items shipped to each agency\nNow that we have an overall idea of what local agencies are doing, let’s dive a little deeper. It’s time to figure out the specific items that they received.\nOur question is this: What specific “controlled” items did each agency get and how much were they worth?\nIn some cases an agency might get the same item shipped to them at different times. For instance, “AUSTIN POLICE DEPT” has multiple rows where they get a single “ILLUMINATOR,INTEGRATED,SMALL ARM” shipped to them on the same date, but then on other dates they have the same item but the quantity is 30. We want all of these “ILLUMINATOR,INTEGRATED,SMALL ARM” for the Austin police added together into a single record.\nThe logic works like this:\n\nStart with the controlled data, and then …\nGroup by the agency_name and item_name, which will group all the rows where those values are the same. All “AUSTIN POLICE DEPT” rows with “ILLUMINATOR,INTEGRATED,SMALL ARM” will be considered together, and then …\nSummarize to sum the quantity, and then do the same for total_value.\n\nThe code for this is very similar to what we did above when we summaries agencies, except we are grouping by two things, the agency_name and the item_name. Let’s do it:\n\nCreate a new section (headline, text and code chunk) and describe that you are finding the sums for each item that each agency has received since 2010.\nConsult (or even copy) the code you wrote when you created the agency totals, but modify the group_by() to add the item_name, like this: group_by(agency_name, item_name).\nBe sure you rename your created R objects, too, perhaps to tx_agency_item_totals.\n\n\n\nGive it a go. Below is the result.\n# adding the new tibble object in next line\ntx_agency_item_totals &lt;- leso_c_tx |&gt; \n  group_by(agency_name, item_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n# peek at the result\ntx_agency_item_totals\n\n\n\n\n  \n\n\n\nThis reuse of code like this – copying the agency grouping code and editing it to add the item_name value – is very common in coding, and there is nothing wrong with doing so as long as you are careful.\nWhen you reuse code, review it carefully so you don’t override things by accident. In this instance, our original code created an R object: tx_agency_totals &lt;- leso_c_tx |&gt; ... that holds the result of our functions, and we call that later to view it. If we reuse this code and don’t update that object name, we’ll reset the values inside that already-existing object, which was not our intent. We want to create a NEW thing tx_agency_item_totals so we can use that later, too. And if we don’t update the “peek” at the object, we’ll be looking at the old one instead of the new one.\n\n\n\n\n\n\nNote\n\n\n\nWith that last code chunk you might see a warning in your R Console: summarise() has grouped output by ‘agency_name’. You can override using the .groups argument. This is not a problem, it’s just a reminder that when we group by more than one thing, the first grouping is retained when future functions are applied to this result. It’s more confusing than helpful, to be honest. Just know if we wanted to do further manipulation, we might need to use ungroup() first.\n\n\n\n6.8.1 Items for local agencies\nJust like we did for our agency totals, we want to filter this list of items to those sent to our local agencies. However, this time we’ve already created the list of local agencies so we don’t have to redo that part … we just need to filter by it.\n\nStart a new section (headline, text) and explain that you are looking at items sent to local agencies.\nUse filter() to focus the data just on or local_agencies.\n\n\ntx_agency_item_totals |&gt; \n  filter(agency_name %in% local_agencies)\n\n\n\n  \n\n\n\nBecause our original list arranged the data by the most expensive items, we can see that here. But it might be easier to rearrange the data by agency name first, then the most expensive items.\n\nEDIT your chunk to add an arrange function.\nWithin the arrange, set it to agency_name first, then summed_total_value in descending order.\n\n\ntx_agency_item_totals |&gt; \n  filter(agency_name %in% local_agencies) |&gt; \n  arrange(agency_name, desc(summed_total_value))\n\n\n\n  \n\n\n\n\n\n6.8.2 Research some interesting items\nYou’ll want a little more detail about some of these items for your data drop. I realize (and you should, too) that for a “real” story we would need to reach out to sources for more information, but you can search online to learn enough to at least describe some of these items. There are a couple of ways to go about researching items.\n\nSimply search for the items on Google, like this.\nEach item has a “National Stock Number,” which is an ID for a government database of supplies. You can search the data for the nsn value and then look up that value online.\n\nLet do an example, looking up “ILLUMINATOR,INTEGRATED,SMALL ARMS”. First we filter the data to find the item. (I’m also using select so we can control the output and see it in this book, but you might not want that in your notebook so you can also check ship_dates, etc.)\n\nleso_c_tx |&gt; \n  filter(\n    item_name == \"ILLUMINATOR,INTEGRATED,SMALL ARMS\",\n    agency_name == \"AUSTIN POLICE DEPT\"\n    ) |&gt; \n  select(item_name, nsn)\n\n\n\n  \n\n\n\nIt looks like most of these illuminators use this nsn: 5855-01-534-5931.\nNow we go to the website https://nationalstocknumber.info/ and plug that number into the search bar. We get a couple of returns, and if we click on one we get this page. It gives us a description of the item:\n\n“A device which is a combination of several lasers and white light illumination used to provide multiple capabilities for engaging targets and providing light. The device may contain a flashlight or other white light illumination source, an illuminator, infrared and stand alone aiming lasers/pointers. The device has the capability to mount on an individual weapon.”\n\nNot every item is in the database, but it is worth checking."
  },
  {
    "objectID": "leso-analysis.html#turn-in-your-project",
    "href": "leso-analysis.html#turn-in-your-project",
    "title": "6  Military Surplus Analysis",
    "section": "6.9 Turn in your project",
    "text": "6.9 Turn in your project\n\nMake sure everything runs and Renders properly.\nPublish your changes to Quarto Pub and include the link to your project in your index notebook so I can bask in your glory.\nZip your project folder. (Or export to zip if you are using posit.cloud).\nUpload to the Canvas assignment.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo be clear, it is your zipped project I am grading. The Quarto Pub link is for convenience."
  },
  {
    "objectID": "leso-analysis.html#what-we-learned-in-this-chapter",
    "href": "leso-analysis.html#what-we-learned-in-this-chapter",
    "title": "6  Military Surplus Analysis",
    "section": "6.10 What we learned in this chapter",
    "text": "6.10 What we learned in this chapter\n\nWe used sum() within a group_by()/summarize() function to add values within a column. Sometimes sum() needs the arguent na.rm = TRUE.\nWe used summary() to get descriptive statistics about our data, like the minimum and maximum values, or an average (mean).\nWe learned how to use c() to combine a list of like values into a vector, and then used that vector to filter a column for values %in% that vector.\nWe used nrow() to count the number of rows in an object."
  },
  {
    "objectID": "plots.html#goals-for-this-section",
    "href": "plots.html#goals-for-this-section",
    "title": "7  Intro to ggplot",
    "section": "7.1 Goals for this section",
    "text": "7.1 Goals for this section\nIn this chapter, we’ll learn the basics of data visualization using the Grammar of Graphics principles. We’ll start with some smaller datasets to give you a sense of how the code works. And, in the next chapter, you’ll apply this to a dataset we have already used in the class.\nOur learning goals are:\n\nTo learn about the Grammar of Graphics\nTo make scatterplots\nTo make bar charts"
  },
  {
    "objectID": "plots.html#introduction-to-ggplot",
    "href": "plots.html#introduction-to-ggplot",
    "title": "7  Intro to ggplot",
    "section": "7.2 Introduction to ggplot",
    "text": "7.2 Introduction to ggplot\nggplot2 is the data visualization library within Hadley Wickham’s tidyverse. It is a beast of a package because it supports a whole variety of different types of data visualizations, from bar charts and line charts to fancy choropleth maps and animated figures.\nEven though the package is called ggplot2, the function to make graphs is just ggplot(). So, for simplicity, we’ll just call everything ggplot.\nThe ggplot package relies on a concept called the Grammar of Graphics, hence the gg in ggplot. The basic logic of the Grammar of Graphics is that any graph you could ever want to build will need similar things: a data set, some information about the scales of your variables, and the type of figure or graph that you want to create. These various things can be “layered” on top of each other to create a visually pleasing plot.\nFolks who have used Adobe creative programs (e.g., Photoshop, Illustrator, etc.) can think about it like laying an image: each layer in your image should do something to change the image. Likewise, each layer in a ggplot figure will add to the overall graph.\n\n7.2.1 What I like/dislike about ggplot\nLet me just start by saying that I’m a total ggplot geek. I’ll talk about ggplot figures the way people talk about new TikTok trends. When producing figures and graphs in R, ggplot is the absolute best approach because you’ll see the results right in your notebook. And, basic data visualizations are an absolutely essential skill for any data journalist: it helps you find important things in your data that you may ultimately report on. So, ggplot is important for any R-based data journalism project.\nThat being said, there are less complicated ways of creating publishable graphics. Tools like Datawrapper and Flourish can produce equally beautiful graphics without the code. So why learn ggplot? Because, (1) ggplot is super useful when you’re just learning about the data and (2) to get good enough in ggplot to make publishable graphics, you have to practice, practice, practice. Yes, ggplot is a big package with lots of nuance. But the more you take the time to learn it, the more you will master it.\n\n\n7.2.2 The Grammar of Graphics\nThis section was inspired by Matt Waite and the BBC Visual Cookbook.\nAs I said above, the gg in ggplot stands for “Grammar of Graphics,” which is a fancy way of saying we’ll build our charts layer by layer. There are three main things you need to make a plot:\n\ndata: You have to tell ggplot the name of the object with your data.\nmapping: Defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot. i.e., which columns are on the x or y axis. We use the function aes() to describe these aesthetics.\ngeometries: This how you describe the shape of your visualization, whether it’s lines, bars, points, or something else. We do this through different goem_() functions.\n\nIn addition to these three things, there are lots of helper layers we’ll learn about along the way, including:\n\nthemes (“theme”): this is where you tell R the font you’d like to use, the background color, and other things you want to “pretty up” the data viz.\ncoord_flip: a special layer for flipping the chart\n\nscales: transforming the data to make the plot more read-able\n\nlabels (“labs”): for making titles and labels\n\nfacets: For graphing many elements of the same data set in the same space (one dataset, multiple figures)\n\nThis all may seem complicated now, but it’ll make sense once we start putting together these layers together, one at a time. After all, the best way to learn any R package is to do it."
  },
  {
    "objectID": "plots.html#start-a-new-project",
    "href": "plots.html#start-a-new-project",
    "title": "7  Intro to ggplot",
    "section": "7.3 Start a new project",
    "text": "7.3 Start a new project\n\nGet into RStudio and make sure you don’t have any other files or projects open.\nCreate a new Quarto Website project, name it yourname-ggplot and save it in your rwd folder.\n(No need for a folder structure, we’ll do this all in one file.)\nWe’ll use our index.qmd file for everything for this project, so update the title as “ggplot practice”, remove the boilerplate and create a setup section that loads library(tidyverse) and library(janitor), like we do with every notebook.\n\nThe ggplot package is a part of tidyverse, so we don’t need to do anything special there.\n\n7.3.1 Install palmerpenguins\nFor this lesson, we will install a new R package, which will give us access to some data to work through some visualization techniques. I’d like you to meet the Palmer Penguins …\n\n\n\nArtwork by @allison_horst\n\n\nWe don’t need this dataset to plot charts, but it is a great one for data exploration & visualization so we’ll use it to learn about ggplot.\nThe palmerpenguins package includes scientific measurements of penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica. The package and the awesome artwork is maintained by Allison Horst.\nLet’s install and load it.\n\nIn your Console, run install.packages(\"palmerpenguins\").\nIn your setup section, add the library and rerun the chunk: library(palmerpenguins)\n\nRemember you only have to install a package once on your computer, but you should load the library at the top of each notebook that uses it. (i.e., don’t leave the install.packages() part in your notebook.)"
  },
  {
    "objectID": "plots.html#the-layers-of-ggplot",
    "href": "plots.html#the-layers-of-ggplot",
    "title": "7  Intro to ggplot",
    "section": "7.4 The layers of ggplot",
    "text": "7.4 The layers of ggplot\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of this first plot explanation comes from Hadley Wickham’s R for Data Science, with edits to fit the lesson here.\n\n\nBefore we dive into ggplot, let’s peek at our data.\n\nStart a new section “First plot”\nAdd some text that you are studying pretty penguins.\nAdd a code chunk like below and run it to see what the penguins dataset looks like.\n\n\npenguins\n\n\n\n  \n\n\n\nAs noted above, the data are measurements from a number or penguins, things like species, sex, bill length, etc.\nAmong the variables in penguins are:\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\nbody_mass_g: body mass of a penguin, in grams.\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nWith these variables, we’ll explore the relationship between flipper lengths and body masses of these penguins, taking into consideration the species of the penguin.\n\n7.4.1 Our goal chart\nOur goal is to recreate this chart:\n\n\n\n\n\nFigure 7.1: Our goal graphic\n\n\n\n\n\n\n7.4.2 1st: the data\nWhen working with the ggplot2 package, you’ll start nearly every figure with the ggplot() function. In the ggplot() function, you’ll tell R what data you’re using, and the coordinate system you want to build based on the data.\nThe first thing you’ll want to do is tell ggplot the dataset you want to use (in this case, penguins). Let’s do that now.\n\nEdit your first plot section,\nMake a new code chunk and add the code below.\n\n\nggplot(penguins)\n\n\n\n\nThis tells us… absolutely nothing! But that’s not surprising: you haven’t even told ggplot what variables you want to focus on or the way you want to visualize the data. To do that, you’ll need a mapping argument.\n\n\n7.4.3 2nd: Map the data\nWe use the aes() (short for “aesthetic”) to describe which data to plot and where. This is considered a mapping argument, because you use this argument to tell ggplot how you want to map your data.\nThe workhors of aes() is to indicate the variable to plot horizontally (the x axis) and vertically (the y axis). So your aes() argument will look something like this: aes(x = some_variable, y = another_variable).\nIn our case, set our flipper_length_mm (the flipper length) for x and body_mass_g for y:\n\nEdit your code to add the following line of code. Note I’ve rearranged the indenting, too.\n\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\nWe are getting closer! The plot knows what data we are using and the “range” of that data, so it has added tick marks and values along our x and y axes.\n\n\n7.4.4 3rd: geometries\nNow that we know which data to apply, we have to make a choice on how we want to show the shape of that data on our plot, and we do that through geometries (or “geoms” as we call them.) We’ll talk about different ones in a minute, but we’ll start with plotting our data as points.\nThis is our first added layer on our chart, and therefore introduction of the + option within ggplot. We use + to add on layers of information in our grammar of graphics. It’s sort of like the pipe |&gt; in the way it works, but we use + when adding ggplot layers.\n\nEdit your chunk to add the + and the geom_point() functions as noted below.\n\n(I really recommend you type the additions so you can see how RStudio helps fill the code.)\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n1) +\n2  geom_point()\n\n\n1\n\nDon’t forget the + here\n\n2\n\nThe geom_point() plotted dots based on the values set in our aes() function.\n\n\n\n\n\n\n\nOK, now we are getting somewhere. We can see each penguin plotted on the chart, and we can generally see that as the birds get heavier (higher on vertical axis) their flippers are also longer (to the right on the horizontal axis.)\n\n\n\n\n\n\nNote\n\n\n\nWe do get this warning: Warning: Removed 2 rows containing missing values geom_point(). This is telling us there are two rows of data that don’t have one of the two values, so they were dropped. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. As journalists, we just need to make sure we know why something is missing and note if it is important.\n\n\n\n7.4.4.1 Other geometries\nThere are many geoms, but here are a few common ones:\n\ngeom_point() adds dots onto the grid based on the data. We used them above.\ngeom_line() adds lines between data points on the grid. Basically a line chart.\ngeom_col() and geom_bars() adds bars to the grid based on values in the data. A bar chart. We’ll use geom_col() later in this lesson but you can read about the difference between the two in a later chapter.\ngeom_text() adds labels based on values in the data.\n\n\n\n\n7.4.5 Chart code review\nLet’s review the different parts of this code. Note I’ve rewritten it a bit here to put some arguments on their own line so it is more readable.\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\nThink of this like painting a chart:\n\nggplot() is the function we use to make a chart.\nInside of ggplot() the first argument is the data, in this case the penguins data.\nThe second argument is the aes() function, where we apply our aesthetics. Aesthetics describe how we will paint our data onto the plot. Inside of this function, we set values for the x and y axis so we know WHERE to paint. There are some other aesthetic values we can paint with data, and we will.\nLastly we add on a layer (with +) to add geom_point() to paint “points” on top of the plot based on the set aesthetics. (It’s like pointillism.)\n\n\n\n\n\n\n\nImportant\n\n\n\nYou might see this same code written different ways. Let’s talk about why.\n\nVerbose arguments\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\nThe above version adds data = and mapping = to the first two arguments. It is the official descriptive way to do this, and how Hadley Wickham describes it in R for Data Science. But, since the first argument can assumed to be the data we often don’t include it unless it unless we are trying to differentiate with different data used later. Same goes for mapping = … we don’t specify it unless necessary.\n\n\nPiping into ggplot\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\nIn this case we start with the data AND THEN pipes it into ggplot() as the first argument. Prof. McDonald often does it this way. The scoundrel.\n\n\n\n\n\n7.4.6 Global vs local aesthetics\nOur initial plot appears to show that flipper length is related to the weight of the penguin, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship. For example, does this relationship exist for all three species?\nLet’s color our points based on the species to see.\n\nEdit your plot and add the color mapping to the aes() function, like below. NOTE I’ve also rearranged some of the code to make it more readable.\n\n\nggplot(\n  penguins,\n1  aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n1\n\nThis is the line were yo uare adding color.\n\n\n\n\n\n\n\nWhen a categorical variable like the species is mapped to an aesthetic, ggplot will assign a color to each unique value and add a legend so you can tell them apart.\nLet’s add another layer: a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\n\nEdit your code to add on the last layer shown here:\n\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nWe’ve added the lines, but it doesn’t look like our “goal” graphic we started with in Section 7.4.1, which only has one line instead of three separate lines for each of the penguin species.\nWhen we set the x, y and color aesthetics in our code, we set them at the global level. All those characteristics apply to every added layer. However, each geom function can have it’s own aesthetics, setting them at the local level, applying to only that layer.\nLet’s show this by moving our color aesthetic from the global level to apply just at the point level.\n\nEdit your code to move the color = species bit from the global aesthetics to a local aesthetic within geom_point(), as shown below.\n\n\nggplot(\n  penguins,\n1  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n2  geom_point(aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n1\n\nThis is the line where we remove the color = species function.\n\n2\n\nThis is where we add it back, creating an aes() function inside geom_point() so it only applies to that geom.\n\n\n\n\n\n\n\nOK, this is looking pretty good, but it is not a great idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. We can improve readability of this chart of we also map species based on the shape aesthetic.\n\nEdit your code to add shape = species aesthetic to the geom_point() function, as shown below.\n\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n1  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n1\n\nWe add the shape = species argument here inside the aes() function.\n\n\n\n\n\n\n\n\n\n7.4.7 Labels\nIn addition to geoms, you can adjust and add labels (text layers) to our plot.\nLabels (or labs, since we use the labs() function for them) are a series of text-based items we can layer onto our plots like titles, bylines and axis names.\nLike geom_point() above, we’ll use the + at the end of each line before we add another layer.\nLet’s add a labels layer so we can describe our chart to our readers. Note there are a number of arguments available in labs(), even more than those listed here.\n\nEdit your code to add the labs() layer indicated below.\n\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  )\n\n\n\n\nWalking through that a bit\n\nWe start with the labs() function and open it up to write many arguments inside.\ntitle gives us a headline.\nsubtitle is a readout under the headline. We sometimes need to wrap the text in a str_wrap() function to keep it from running out of the chart.\nThe x axis already had a label on our chart by default, but it was the non-very-print-friendly name of the variable. Fine for us because we know what it is, but here we are replacing that with text more appropriate for a reader. We do the same with the the y label.\nThe color and shape arguments are similar in that we are replacing the existing label above the legend, resetting those to capitalize “Species”. We need to change them both so they are the same. (You might experiment by removing the shape argument and re-running it to see what happens.)\n\n\n\n7.4.8 Themes\nYou can change just about anything on a ggplot chart if you know the function and arguments to describe it. The cheetsheet shows a lot of them, but it can get really dense.\nThemes are collections of these visual changes saved into a single function. There are several available within ggplot and many others from the R community.\nLet’s how that here.\n\nEdit your code chunk to add on the last + theme_minimal() function.\n\n\nggplot(\n  penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  theme_minimal()\n\n\n\n\nThis changes the background color and grid lines to make the data pop a little better. There are other themes that create more radical change, some in their own packages. We’ll play more later.\nOK, you’ve made your first ggplot chart! Are you ready to make another?"
  },
  {
    "objectID": "plots.html#lets-build-a-bar-chart",
    "href": "plots.html#lets-build-a-bar-chart",
    "title": "7  Intro to ggplot",
    "section": "7.5 Let’s build a bar chart",
    "text": "7.5 Let’s build a bar chart\nIn our first week of class, we sent out a survey where you told us your favorite Disney Princess and favorite flavor of ice cream. Let’s now play around with some of this data.\nFor this lesson, we’re not going to create a different notebook or download the data to our computer. Instead, we’re going to save the data directly into a tibble.\n\nStart a new section: Importing class data\nIn the text, note that we are importing the chart data.\nAdd the code below to get the data.\n\n\n1class &lt;- read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQfwR6DBW5Qv6O5aEBFJl4V8itnlDxFEc1e_-fOAtBMDxXx1GeEGb8o5VSgi33oTYqeFhVCevGGbG5y/pub?gid=0&single=true&output=csv\") |&gt;\n2  clean_names()\n\n3class\n\n\n1\n\nWe create a new object and then fill it by reading the data directly from the web.\n\n2\n\nWe lowercase the variable names using clean_names() from the janitor package.\n\n3\n\nPrint the new object to peek at it.\n\n\n\n\n\n\n  \n\n\n\nSo, now, you should have the data in your environment.\nAnd with this data, we want to build a chart like this:\n\n\n\n\n\n\n7.5.1 Prep princess data\nWhile there are ways for ggplot to calculate values from your data on the fly, I prefer to first build a table of the values I want, and then I will plot it on a chart. It’s helpful to think of these steps as separate so you have a good workflow (clean the data, prepare the data in a table form, and then plot the data).\nToday, our goal will be to make a bar chart, sometimes known as a column chart. This bar chart will show the number of votes for each princess from the data. So, we need to count the number of rows for each value … our typical group_by/summarize/arrange (GSA) process.\nFor this lesson, I’ll use the count() shortcut, since we haven’t used it much. Next, I’ll save the summarized data into a new dataframe called princess_data. Follow along in your notebook:\n\nAdd a section: Princess data\nAdd text that you are creating a data frame to plot.\nAdd the code below to create that data.\n\n\n1princess_data &lt;- class |&gt;\n2  count(princess, name = \"votes\", sort = TRUE)\n  \n3princess_data\n\n\n1\n\nStart with a new object and start filling it with class data.\n\n2\n\nHere we use the count() function to count the number of rows based on the princess column. The argument name = just names the new columns something other than n, and the sort = argument sorts the data in descending order based on the counted column.\n\n3\n\nPrint it out so we can see it.\n\n\n\n\n\n\n  \n\n\n\nAt this point, y’all should be plenty familiar with these summary functions, and the output should be easy to interpret: we’re just counting the number of rows for each princess.\nNow that we have our table data, let’s actually plot it.\n\n\n7.5.2 Build our plot with geom_col\nLike in the previous lesson, we’ll start our plot by creating the first layer: the ggplot() function, which takes the data as its first argument and the aes() mapping layer as its second argument.\n\nAdd some text noting that you’ll now plot.\nAdd the following code chunk, which is the first layer\n\n\nggplot(\n  princess_data,\n1  aes(x = princess, y = votes)\n)\n\n\n1\n\nsets our x and y axes values.\n\n\n\n\n\n\n\nYou’ll see the grid and x/y axis of the data, but no geometries are applied yet, so you won’t see any data. But remember, we’re adding these all in layers.\n\n\n7.5.3 Add the geom_col layer\nNow it is time to add our columns. To do this, we’ll use geom_col(). Similar to geom_point(), geom_col() adds a geometric layer that tells R how to display the data (in this case, with columns as opposed to points). Let’s write this code now.\n\nEdit the plot code to add the ggplot pipe + and on the next line add geom_col().\n\n\nggplot(\n  princess_data,\n  aes(x = princess, y = votes)\n1) +\n2  geom_col()\n\n\n1\n\nDon’t forget to add the + at the end of the previous line\n\n2\n\nThe geom_col() function adds the bars based on the global aesthetics we’ve already set.\n\n\n\n\n\n\n\nOur two-layer chart is getting somewhere now. We’re able to see the data in the plot, but there are a couple issues:\n\nDepending on the width of our notebook, the princess names might collide with each other. We can fix this.\nThe order of the bars is alphabetical instead of in vote order. Again, we can fix it.\n\n\n\n7.5.4 Flip the axes\nOne way to fix the labels is to “flip” the axes, so the x axis becomes the y axis and vice versa. This is the equivalent of rotating the whole figure. When we do this, the axis will turn sideways, making it easier to read the labels. Worth noting: this can be a bit confusing later because the “x” axis is now going up/down (as opposed to left and right).\nLet’s learn how to flip the axes now. We’ll do this by adding a new layer, coord_flip(), which is a special layer that flips the axes. Just like we added the previous geom_col() layer using +, we’ll do the same thing here. Let’s do that now.\n\nEdit your plot chunk to add the ggplot pipe +and coord_flip() on the next line.\n\n\nggplot(\n  princess_data, aes(x = princess, y = votes)\n) +\n  geom_col() + \n1  coord_flip()\n\n\n1\n\nAdding coord_flip() swaps the x and y axes.\n\n\n\n\n\n\n\nAs you can see, rather than having vertical bars, we now have horizontal bars, and the names of each princess are fully displayed and read-able. Much better!\nBut the bars are still in an alphabetical order, as opposed to a vote order, so let’s fix that now.\n\n\n7.5.5 Reorder the bars\nThe bars on our chart are in alphabetical order of the x axis (and reversed thanks to our flip.) We want to order the values based on the votes in the data.\n\n\n\n\n\n\nNote\n\n\n\nComplication alert: Categorical data can have factors, which are like an internal ordering system. Some categories, like months in a year, have an “order” that is not alphabetical. We don’t have that here, but know it is a thing.\n\n\nWe can reorder our categorical values in a plot by editing the x values in our aes() using reorder(). (There is a tidyverse function called fct_reorder() that works the same way for factors.)\nreorder() takes two arguments: The column to reorder, and the column to base that reorder on. It can happen in two different ways, and I’ll be honest and say I don’t know which is easier to comprehend.\n\nx = reorder(princess, votes) says “we shall set x as reordered values of princess based on the order of votes. OR …\nx = princess |&gt; reorder(votes) says “set the x axis as princess and then reorder by votes.\n\nThey both work. Even though I’m a fan of the tidyverse |&gt; construct, I’m going with the first version.\n\nEdit the first line of your chunk to reorder the bars.\n\n\nggplot(\n  princess_data,\n1  aes(x = reorder(princess, votes), y = votes)\n) + \n  geom_col() +\n  coord_flip()\n\n\n1\n\nThis is the line where reorder() is added.\n\n\n\n\n\n\n\nSo now, our princess names are read-able, and the bars are organized in vote size. But what if we wanted to be clearer in our figure, so that we knew the exact number of votes for each princess? Let’s learn how to add this information.\n\n\n7.5.6 Adding a geom_text layer\nNow, we’re really starting to take advantage of the grammar of graphics by including more than one geometric layer. Specifically, we’ll be using geom_text() to add some information to our bar charts.\nAs we mentioned previously, geom layers can take individual aesthetics (that build on top of the global aesthetics you put in the first layer). When using geom_text(), we’ll include some local aesthetics using the aes() argument, to tell ggplot the label we’d like to add to the plot.\n\nEdit your plot chunk to add the + and geom_text() layer on the end of your code.\nSet the aesthetics of the geom_text() function to plot the labels onto the chart based on the number of votes: aes(label = votes), as noted below.\n\n\nggplot(princess_data,\n       aes(x = reorder(princess, votes), y = votes)\n) + \n  geom_col() +\n  coord_flip() + \n1  geom_text(aes(label = votes))\n\n\n1\n\nThis plots a text layer onto the chart based on both the position and values in votes.\n\n\n\n\n\n\n\nWell that did… something. We’ve successfully added the numbers to this plot, but it’s not very pretty. First, the number is plotted at the end of the bar, making it harder to read. So we’ll want to horizontally adjust this by shifting the numbers a bit to the left. Second, black text is really hard to read against a dark grey background. So we’ll change the text of the number to white.\nWe can make both of these edits directly in the geom_text layer.\n\nEdit the last line of your plot chunk to add two new arguments.\nThe first argument you will add is hjust, which moves the text left. (hjust stands for horizontal justification. vjust, or vertical justification, would move it up and down).\nThe second argument you will add is color, which tells ggplot what the color of your text should be.\n\nAs a reminder, you should always separate your arguments within a function using commas (,).\n\nggplot(\n  princess_data,\n  aes(x = reorder(princess, votes), y = votes)\n) + \n  geom_col() +\n  coord_flip() + \n1  geom_text(aes(label = votes), hjust = 2, color = \"white\")\n\n\n1\n\nNote that both hjust and color are NOT inside the aes() function because we are not using the data to control them.\n\n\n\n\n\n\n\nGreat! But we’re still not done. Even though we’ve added labels to each bar chart, we still haven’t added a title, and the titles of our x and y axes are not great. So let’s work on those now.\n\n\n7.5.7 Add some titles and more labels\nNow that we have a chart, with some information displayed in bars, flipped and arranged so we can see information, let’s add to this by giving the chart some labels. We’ll do this by adding a layer of labels to our chart using the the labs() function. We can add and change a number of things with labs(), including creating a title, and changing the x and y axis titles.\n\nEdit the last line of your plot chunk to add the ggplot pipe + and labs() in the next line.\nAdd a title using the title = argument\nAdd a subtitle using the subtitle = argument. This is a great place to put information about your data (like when it was collected).\nAdd a caption using the caption = argument. Put your byline here!\nChange the x and y axes titles using x = and y =.\n\n\nggplot(\n  princess_data,\n  aes(x = reorder(princess, votes), y = votes)\n) + \n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = votes), hjust = 2, color = \"white\") + \n  labs(\n1    title = \"Pick of the princesses\",\n2    subtitle = str_wrap(\"Students in Reporting with Data each voted for their favorite Disney Princess. Some complained that Princess Leia was not an option.\"),\n3    caption = \"By Jo Lukito\",\n4    x = \"Princess choices\",\n5    y = \"Number of votes\"\n  )\n\n\n1\n\nA title to draw you in or communicate your goal.\n\n2\n\nThe subtitle is the place to explain what is needed to understand the chart. In this case we wrap the text in a str_wrap() function so the words don’t run off the chart.\n\n3\n\nA caption is a cood place to add your byline.\n\n4\n\nBecause we use coord_flip() it is actually the x axis that is vertical.\n\n5\n\nAgain, x and y are flipped.\n\n\n\n\n\n\n\nThere you go! You’ve made a chart showing how our classes rated Disney Princesses."
  },
  {
    "objectID": "plots.html#on-your-own-ice-cream",
    "href": "plots.html#on-your-own-ice-cream",
    "title": "7  Intro to ggplot",
    "section": "7.6 On your own: Ice cream!",
    "text": "7.6 On your own: Ice cream!\nNow it is time for you to put these skills to work:\n\nBuild a chart about the favorite ice creams from RWD classes.\n\nSome things to consider:\n\nYou need a new section, etc.\nYou’re starting with the same class data\nYou need to prepare the data based on ice_cream (which is the name of a variable in your class data frame)\nYou need to build the chart\n\nIt’s essentially the same process we used for the princess chart, but using ice_cream variable. That said, I really recommend you write all the code from scratch, ONE LINE AT A TIME, so you can soak in what each line does."
  },
  {
    "objectID": "plots.html#what-weve-learned",
    "href": "plots.html#what-weve-learned",
    "title": "7  Intro to ggplot",
    "section": "7.7 What we’ve learned",
    "text": "7.7 What we’ve learned\nThere is a ton, really.\n\nggplot2 (which is really the ggplot() function) is the charting library for the tidyverse. This whole lesson was about it.\nWe also covered reorder(), which can reorder a variable based on the values in a different variable.\n\nHere are some more references for ggplot:\n\nThe ggplot2 documentation and ggplot2 cheatsheets.\nR for Data Science, Chap 3. Hadley Wickham dives right into plots in his book.\nggplot2: Elegant graphics for Data Analysis by Wickham.\nR Graphics Cookbook has lots of example plots. Good to harvest code and see how to do things.\nThe R Graph Gallery another place to see examples."
  },
  {
    "objectID": "plots-more.html#learning-goals-for-this-chapter",
    "href": "plots-more.html#learning-goals-for-this-chapter",
    "title": "8  Deeper into ggplot",
    "section": "8.1 Learning goals for this chapter",
    "text": "8.1 Learning goals for this chapter\nIn this chapter, we will cover the following topics:\n\nHow to prepare and build a line chart\n\nHow to use themes to change the looks of a chart\n\nMore about aesthetics in layers!\n\nFaceting, or making multiple charts from the same data\n\nHow to make interactive plots with Plotly"
  },
  {
    "objectID": "plots-more.html#references",
    "href": "plots-more.html#references",
    "title": "8  Deeper into ggplot",
    "section": "8.2 References",
    "text": "8.2 References\nggplot2 has a LOT to it and we’ll cover only the basics. Here are some references you might use:\n\nThe ggplot2 documentation and ggplot2 cheatsheets.\nR for Data Science, Chap 3. Hadley Wickham dives right into plots in his book.\nggplot2: Elegant graphics for Data Analysis by Wickham.\nR Graphics Cookbook has lots of example plots. Good to harvest code and see how to do things.\nThe R Graph Gallery another place to see examples."
  },
  {
    "objectID": "plots-more.html#set-up-your-notebook",
    "href": "plots-more.html#set-up-your-notebook",
    "title": "8  Deeper into ggplot",
    "section": "8.3 Set up your notebook",
    "text": "8.3 Set up your notebook\nThis week, we’ll return to our Military Surplus project, but add a new Quarto Document.\n\nOpen your name-military-surplus project.\nCreate a new Quarto Document.\nSet the title as “Military Surplus figures”\nRemove the rest of the boilerplate template.\nSave the file and name it 03-ggplot.qmd.\nCreate a setup chunk and load the tidyverse library. Also load a library called scales, though you shouldn’t have to install it as it comes with the tidyverse package.\n\nYou’ll also need to install the scales package and include that in your library.\n\nIn your Console, run install.packages(\"scales)\nThen back in your notebook add library(scales) to your setup chunk.\n\n\n8.3.1 Let’s get the data\nWe’ll be importing the cleaned data that has all the states.\n\nCreate a new section that notes you are importing\nUse read_rds() to import the file and save it into an object called leso. The file should be at data-processed/01-leso-all.rds if you named it correctly in our analysis .\nGlimpse the data so we can see the column names\n\n\n\nYou should be able to do this by now!\nleso &lt;- read_rds(\"data-processed/01-leso-all.rds\")\n\nglimpse(leso)\n\n\nRows: 67,603\nColumns: 13\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"…\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A…\n$ nsn               &lt;chr&gt; \"2355-01-553-4634\", \"1005-01-587-7175\", \"1240-01-411…\n$ item_name         &lt;chr&gt; \"MINE RESISTANT VEHICLE\", \"MOUNT,RIFLE\", \"SIGHT,REFL…\n$ quantity          &lt;dbl&gt; 1, 10, 9, 10, 1, 1, 1, 3, 1, 17, 10, 7, 6, 4, 4, 1, …\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Kit\", \"Each\", \"Each\", \"Each…\n$ acquisition_value &lt;dbl&gt; 658000.00, 1626.00, 371.00, 15871.59, 10000.00, 6262…\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"Q\", \"C\", \"C\", \"D\", \"C\", \"A\", \"A…\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3…\n$ ship_date         &lt;dttm&gt; 2016-11-09 00:00:00, 2016-09-19 00:00:00, 2016-09-1…\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"…\n$ total_value       &lt;dbl&gt; 658000.00, 16260.00, 3339.00, 158715.90, 10000.00, 6…\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n\n\nAbove we can see the columns were working with."
  },
  {
    "objectID": "plots-more.html#goal-1-line-chart",
    "href": "plots-more.html#goal-1-line-chart",
    "title": "8  Deeper into ggplot",
    "section": "8.4 Goal 1: Line chart",
    "text": "8.4 Goal 1: Line chart\nFor our first chart here we are looking to plot the total acquisitions value for each year in the state of Texas. It can sometimes help to draw on paper the chart you are hoping to make so you can see how to format the data to make it.\nIn our case, we’ll need to summarize our data by the year of the shipping date, then add together all the value of the equipment. Here is how we’ll want to structure our data and the chart we’ll make from it.\n\n\n\n\n  \n\n\n\n\n\n\nBut we don’t have this data in the format, so we need to make it.\n\n8.4.1 Working through logic\nData visualization is an iterative process: you may prepare the data, do the visualization, and then realize you want to prepare the data a different way. Remember that the process of coding can involve trial and error: you’re often testing different thing to see what works.\nThere are lots of choices we could make on how to do this, and I explored several different paths while making this chart. While I won’t go through every possible scenario, I’ll try to explain why I made the decisions I did. If I was doing this on my own, I might record these explorations and thoughts in my notebook so I could have a record of why I did things. It’s not like other ways are wrong, they just have different pros and cons.\nTo kinda walk through this with the chart first:\n\nAlong the X axis we want to plot each year. Since our data starts as individual transfers and their date, this is a clue that we need to group by the year of that shipping date.\nAlong the Y axis we want to plot the value of all the equipment in Texas. That’s a clue that we need to summarize our data to add up the value of the equipment.\n\nThe most exploration came around how I grouped this data by year. There are a myriad of ways to do this, like using year() to pluck the year from the shipping date to make a number like 2023 or to then convert that to text as “2023”. I chose to create a “floor date” so I end up with a real date because it gives me more flexibility with plotting. ggplot has some tools where I can format the date the way I want, though I admit I had to go learn about them to do this lesson!\nThe function floor_date() rounds a date down to a common value, but it remains a real date. If I convert both “2023-10-18” and “2023-10-28” to a floor_date unit of “year” then they both end up as “2023-01-01”, the first day of the year. If I choose to set floor_date as a unit of “month” then both dates end up as “2023-10-01”, the first day of the month in each year. It’s like we are rounding the date down to a specific unit. The key here is it remains a real date with properties like year, month, week and day. I’ll draw upon those properties later.\nWe are going to “create” this date as we group our data. You can read more about this in Appendices chapter Grouping by dates.\n\n\n8.4.2 Wrangling the data\nLet’s filter to the data we want (Texas data before 2023) and group it by the year of the ship_date and summarize to sum the total_value.\n\nAdd a new section: Prepare Texas total values data\nAdd a new chunk and the code below. We’ll pick it apart after.\n\n\ntx_values &lt;- leso |&gt; \n  filter(\n    state == \"TX\",\n    ship_date &lt; \"2023-01-01\"\n  ) |&gt; \n  group_by(year = floor_date(ship_date, unit = \"year\")) |&gt; \n  summarize(yearly_value = sum(total_value))\n\ntx_values\n\n\n\n  \n\n\n\nLet’s walk through the code:\n\nWe start by creating a new object to shove all this stuff into: tx_values. Then we start with the data …\nYou should be familiar enough with the filtering by now to understand what we are doing, but might not realize why. Filtering for just Texas rows makes sense because that is our focus, but you might not snap that the data for the most recent year is incomplete until you see the plot. It’s important to recognize the date range of the data you are plotting so you don’t compare a partial year to a complete one.\nInside this group_by() we have the code that makes our floor date.\n\nWe start as we often do by naming the new column made by the group: year.\nWe then use the floor_date() function on the ship_date variable, setting the unit to “year”.\nYou can see the result of this in the return: Any row with a date within the year “2010” was given a date of “2010-01-01” and then summed togther.\n\nThe last line is the summarize() function where we add all the total_values together. We first named the column yearly_value since that is what we are creating.\nAt the end, we print out the contents of the tx_values object we created.\n\n\n\n8.4.3 Plot the chart\nAlright, so now let’s get ready to use the ggplot() function. I want you to create the plot here one step at a time so you can review how the layers are added.\nIn this new plot, we’ll learn about a new geom layer: geom_line() (recall that in the last chapter, we learned about geom_point() and geom_col()).\n\nStart a new section. Note we are plotting Texas values.\nAdd and run the ggplot() line first (but without the +)\nThen add the + and the geom_point() and run it.\nThen add the + and geom_line() and run it.\nThen add the + and labs(), and run everything.\n\n\n1ggplot(tx_values, aes(x = year, y = yearly_value)) +\n2  geom_point() +\n3  geom_line() +\n  labs(\n    title = \"Yearly value of military surplus acquisitions in Texas\", \n    x = \"Year\", y = \"Cost of acquisitions\",\n    caption = \"Source: Law Enforcement Support Office\"\n  )\n\n\n1\n\nwe create the graph\n\n2\n\nadding the points\n\n3\n\nadding the lines between the points\n\n\n\n\n\n\n\n\n\n8.4.4 Cleaning Up\nAlright, so we have a working plot! But there’s a couple things that are a bit ugly about this plot. First, I’m not digging the weird numbers on the side (what the heck does 1e+07 even mean?!). If we go back up and look at the output from tx_values we can see the yearly_value numbers are pretty large. These large numbers are causing R to read our numbers as “scientific notation” (a math-y way of reading large numbers). For example, the total cost of supplies in 2014 was 42,949,729 (that’s the first spike in our figure, around the 4e + 07 mark). But what a pain to read!\nThere are a number of ways we could fix this. We could go back into the code block where we prepare the data and divide that value by “1000000” and call the values “per million”. Or, we can do what I did and use the scales package to “scale” the output for presentation without changing the underlying number.\nUsing functions from the scales package can be esoteric and unintuitive, but most of the things you would regularly need are covered in the Position and Scales chapter of the ggplot2 book. It is from there that we learn about using a function scale_y_continuous(), the label = argument and the the scale_dollar() function.\n\nAfter the geom_line() line, add the scale_y_continuous function as indicated below.\n\n\nggplot(tx_values, aes(x = year, y = yearly_value)) +\n  geom_point() +\n  geom_line() +\n1  scale_y_continuous(labels = label_dollar()) +\n  labs(\n    title = \"Yearly value of military surplus acquisitions in Texas\", \n    x = \"Year\", y = \"Cost of acquisitions\",\n    caption = \"Source: Law Enforcement Support Office\"\n  )\n\n\n1\n\nThis is the added line for the scale\n\n\n\n\n\n\n\nI added this in the middle of the code because I like to keep the labs() at the bottom of the chart. It doesn’t really matter. What it did was change the labels on the Y axis to use a currency format. Let’s walk through it:\n\nThe scale_y_continous() function allows modification to the Y axis, and there is a companion for the X axis. The term “continuous” refers to the values being numbers. There are similar functions for categories (scale_*_discrete()) and dates (scale_*_date()). They all allow modification of things like the labels, grid lines and axis names. (The * there is placeholder for x or y.)\nWe use that function to set our “labels” with another function from the scales pacakge, label_dollar().\n\nYou can\nAgain, the best resource for these type of things is the Scales chapters in the ggplot2 book.\nTo show just how much granularity you have with ggplot, let’s make one more change to this chart. Those dollar values are still pretty large, and it might be nice to show them in millions of dollars, like “$10M”.\n\nEdit the label_dollar() function to add this argument inside: scale_cut = cut_long_scale().\n\n\nggplot(tx_values, aes(x = year, y = yearly_value)) +\n  geom_point() +\n  geom_line() +\n1  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  labs(\n    title = \"Yearly value of military surplus acquisitions in Texas\", \n    x = \"Year\", y = \"Cost of acquisitions\",\n    caption = \"Source: Law Enforcement Support Office\"\n  )\n\n\n1\n\nWe’re editing this line to add the scale_cut argument to label_dollar().\n\n\n\n\n\n\n\nLet’s show one more change. Notice how the grid lines along the X axis don’t fall along every year? We are getting our “breaks” set every five years, but our “minor breaks” (the fainter lines between the five year increments) sit in the middle of a year. That makes it a little harder to read each year, or even to understand each of those dots fall on a year. We can use the scale_x_date() function to change those by adjust those minor breaks.\n\nIn the global aes() function, wrap the date value inside a function called date(). Currently our “date” is really a calendar date/time datatype called &lt;POSIXct&gt;. I’ve found our next function can be pretty picky about wanting a specific &lt;date&gt; datatype so we are adjusting it to avoid an error.\nAfter the scale_y layer, add a new scale_x_date() later as indicated below.\n\n\ntx_values |&gt; \n1  ggplot(aes(x = date(year), y = yearly_value)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n2  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(\n    title = \"Yearly value of military surplus acquisitions in Texas\", \n    x = \"Year\", y = \"Cost of acquisitions\",\n    caption = \"Source: Law Enforcement Support Office\"\n  )\n\n\n1\n\nSurround “year” with date() function.\n\n2\n\nThis is the added later for scale_x_date().\n\n\n\n\n\n\n\nOK, now the fainter “minor breaks” lines fall on each year, making them a little easier for the eye to track. Again, I had to do some digging to figure that out.\n\n\n8.4.5 Saving plots as an object\nSometimes it is helpful to push the results of a plot into an R object to “save” those configurations. You can continue to add layers to this object, but you won’t need to rebuild the main portions of the chart each time. We’ll do that here so we can explore themes next.\n\nEdit your Texas plot chunk to save it into an R object called tx_plot.\nThen call it after the code so you can see it.\n\n\n1tx_plot &lt;- ggplot(tx_values, aes(x = date(year), y = yearly_value)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(\n    title = \"Yearly value of military surplus acquisitions in Texas\", \n    x = \"Year\", y = \"Cost of acquisitions\",\n    caption = \"Source: Law Enforcement Support Office\"\n  )\n\n2tx_plot\n\n\n1\n\nWe edit this line to add the object at the beginning.\n\n2\n\nSince we saved the plot into an R object above, we have to call it again to see it. We save graphs like this so we can reuse them.\n\n\n\n\n\n\n\nWe can continue to build upon the tx_plot object like we do below with themes, but those changes won’t be “saved” into the R environment unless you assign it to an R object."
  },
  {
    "objectID": "plots-more.html#themes",
    "href": "plots-more.html#themes",
    "title": "8  Deeper into ggplot",
    "section": "8.5 Themes",
    "text": "8.5 Themes\nThe look of the graph is controlled by the theme. There are a number of preset themes you can use. Let’s look at a couple.\n\nCreate a new section saying we’ll explore themes.\nAdd the chunk below and run it.\n\n\ntx_plot +\n  theme_minimal()\n\n\n\n\nThis takes our existing tx_plot and then applies the theme_minimal() look to it. This is actually our “Goal graphic” that we were shooting for at the beginning of the lesson!\nThere are a number of themes built into ggplot, most are pretty simplistic.\n\nEdit your existing chunk to try different themes. Some you might try are theme_classic(), theme_dark() and theme_void().\n\n\n8.5.1 More with ggthemes\nThere are a number of other packages that build upon ggplot2, including ggthemes.\n\nIn your R console, install the ggthemes package using the install.packages() function: install.packages(\"ggthemes\")\nAdd the library(ggthemes) at the top of your current chunk.\nUpdate the theme line to view some of the others options noted below.\n\n\nlibrary(ggthemes)\ntx_plot +\n  theme_economist()\n\n\n\n\n\ntx_plot +\n  theme_fivethirtyeight()\n\n\n\n\n\ntx_plot +\n  theme_stata()\n\n\n\n\nThere is also a theme() function that allows you individually adjust about every visual element on your plot.\nWe do a wee bit of that later."
  },
  {
    "objectID": "plots-more.html#goal-2-multiple-line-chart",
    "href": "plots-more.html#goal-2-multiple-line-chart",
    "title": "8  Deeper into ggplot",
    "section": "8.6 Goal 2: Multiple line chart",
    "text": "8.6 Goal 2: Multiple line chart\nOK, our Texas military surplus information is fine … but how does that compare to neighboring states? Let’s work through building a new chart that shows all those steps. It’s essentially the same chart as we did above, but adding the bordering states along with Texas.\nHere is the data and image we are trying to make:\n\n\n\n\n  \n\n\n\n\n\n\n\n8.6.1 Prepare the data\nWe need to go back to our original leso to get the additional states. But we also know now that we need to create a “floor_date” that has a &lt;date&gt; datatype. Let’s do that as we prepare the data instead of while we are grouping. I’ll sometimes go back into my cleaning notebook to add something like this, but we won’t go quite that far with this.\nIn the interest of time, here is all the code we are using, with annotations to explain each line.\n\n1region_data &lt;- leso |&gt;\n2  filter(state %in% c(\"TX\", \"OK\", \"AR\", \"NM\", \"LA\")) |&gt;\n  mutate(\n3    year = floor_date(ship_date, unit = \"year\") |&gt; date()\n  ) |&gt; \n4  group_by(state, year) |&gt;\n5  summarize(yearly_value = sum(total_value))\n\n6region_data\n\n\n1\n\nWe create a new object to hold the data. We fill that starting with leso.\n\n2\n\nWe filter the state column for our list of five states. Note we are using %in% like we did in Chapter 6, but we’re doing it right inside the filter.\n\n3\n\nHere, inside a mutate(), we create the new variable year and fill it with the floor_date of our shipping date based on the year. We then turn that into a &lt;date&gt; datatype.\n\n4\n\nWe group by state and year. This is much cleaner now that we’ve already created a year value.\n\n5\n\nOur summarize to total the costs.\n\n6\n\nAnd lastly, we print the new object so we can see it.\n\n\n\n\n\n\n  \n\n\n\n\n\n8.6.2 Plot the chart\nFor our next plot, we’ll add a different line for each state. To do this you would use the color aesthetic aes() in the geom_line() geom. Recall that geoms can have their own aes() variable information. This is especially useful for working with a third variable (like when making a stacked bar chart or line plot with multiple lines). Notice that the color aesthetic (meaning that it is in aes) takes a variable, not a color. You can learn how to change these colors here. For now, though, we’ll use the ggplot default colors.\n\nAdd a note that we’ll now build the chart.\nAdd the code chunk below and run it. Look through the comments so you understand it.\n\n\n1ggplot(region_data,\n2       aes(x = year, y = yearly_value)) +\n3  geom_point() +\n4  geom_line(aes(color = state)) +\n5  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n6  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n7       caption = \"Source: Law Enforcement Support Office\")\n\n\n1\n\nPlotting the region_data object\n\n2\n\nSet set the x and y axis\n\n3\n\nAdds the points\n\n4\n\nAdds the line, coloring them based on the state values\n\n5\n\nFixes our labels on the y axis.\n\n6\n\nFixes our minor breaks on the x axis.\n\n7\n\nAdding all our labels with labs() arguments.\n\n\n\n\n\n\n\nAdding this aesthetic not only added color to the lines, it added a legend so we can tell which is which. If we had tried to build this plot without the added color aesthetic, it wouldn’t plot right because it wouldn’t how how to split the lines.\nNotice that only the lines changed colors, and not the points? This is because we only included the aesthetic in the geom_line() geom and not the geom_point() geom.\n\nEdit your geom_point() to add aes(color = state).\nAlso edit the labs() function to add color = \"State\".\n\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n1  geom_point(aes(color = state)) +\n  geom_line(aes(color = state)) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n2       color = \"State\")\n\n\n1\n\nColor aesthetic added here.\n\n2\n\nAdding color = \"State\" here redraws the title above the color legend.\n\n\n\n\n\n\n\nOK, you have a line chart for reference!"
  },
  {
    "objectID": "plots-more.html#on-your-own",
    "href": "plots-more.html#on-your-own",
    "title": "8  Deeper into ggplot",
    "section": "8.7 On your own",
    "text": "8.7 On your own\nI want you to make a line chart of military surplus acquisitions in three states that are different from the five we used above. This is very similar to the chart you just made, but with different values.\nSome things to do/consider:\n\nDo this in a new section and explain it.\nYou’ll need to prepare the data just like we did above to get the right data points and the right states.\nI really suggest you build both chunks (the data prep and the chart) one line at a time so you can see what each step adds."
  },
  {
    "objectID": "plots-more.html#tour-of-some-other-adjustments",
    "href": "plots-more.html#tour-of-some-other-adjustments",
    "title": "8  Deeper into ggplot",
    "section": "8.8 Tour of some other adjustments",
    "text": "8.8 Tour of some other adjustments\nYou don’t have to add these examples below to your own notebook, but here are some examples of other things you can control.\n\n8.8.1 Line width\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n1  geom_line(aes(color = state), size = 1.5) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n       color = \"State\")\n\n\n1\n\nYou can set the line width for a geom, but note it is taking an inputted value, not one from the data. This is why it is outside the aes() function.\n\n\n\n\n\n\n\n\n\n8.8.2 Line type\nThis example adds a linetype = state to the ggplot aesthetic. This gives each state a different type of line.\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n1  geom_line(aes(color = state, linetype = state), size = .5) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n       color = \"State\")\n\n\n1\n\nWe add the linetype here, but have to put it INSIDE the aes() now because we are “mapping” the type based on values in the state variable.\n\n\n\n\n\n\n\nBut note that ggplot added a new legend because it takes the name from the variable “state”. We can fix this through overiding it in the labs() function.\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n  geom_line(aes(color = state, linetype = state), size = .5) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n1       color = \"State\", linetype = \"State\")\n\n\n1\n\nWe add linetype = \"State\" to the labs() function, which would collapse everything into a single legend."
  },
  {
    "objectID": "plots-more.html#facets",
    "href": "plots-more.html#facets",
    "title": "8  Deeper into ggplot",
    "section": "8.9 Facets",
    "text": "8.9 Facets\nFacets are a way to make multiple graphs based on a variable in the data, as it creates a bunch of mini charts you can compare among each other. There are two types, the facet_wrap() and the facet_grid(). There is a good explanation of these in R for Data Science.\nWe’ll start with the chart we have already, and then split it into facets by state.\n\nStart a new section about facets\nAdd the code below to create your chart and view it. This is the same plot we’ve already created\n\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n  geom_line(aes(color = state, linetype = state), size = .5) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n       color = \"State\", linetype = \"State\") +\n1  facet_wrap(~ state)\n\n\n1\n\nThis is the only line we added, setting a facet wrap to split on the state.\n\n\n\n\n\n\n\nSince each state facet is labeled, we don’t really need the color legend, so we can use the theme() function to override that.\n\nAdd the last line here to remove the legend.\n\n\nggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n  geom_line(aes(color = state, linetype = state), size = .5) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Yearly military surplus acquisitions in Texas and bordering states\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n       color = \"State\", linetype = \"State\") +\n  facet_wrap(~ state) +\n1  theme(legend.position = \"none\")\n\n\n1\n\nHere we set the legend.position to “none”, which is the answer to remove it. We could also set that to “bottom” or “left” to move it, if that was our desire. It is not, but you can try it if you like.\n\n\n\n\n\n\n\n\n8.9.1 Facet grids\nA facet_grid() allows you to plot on a combination of variables. We don’t really have enough variables to compare with our leso data but we can show this with the penguins data we used in the last chapter.\nThis chart compares the filpper length and body mass, but it does so for each species of bird by their gender.\n\nlibrary(palmerpenguins)\n\npenguins |&gt; \n1  drop_na() |&gt;\n2  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n3  geom_point(aes(color = sex)) +\n4  facet_grid(sex ~ species)\n\n\n1\n\nThis drops any rows that are missing values, as some don’t have sex marked.\n\n2\n\nOur ggplot chart, much like we did in the last chapter, comparing flippers to weight.\n\n3\n\nWe add the points and color them to distinquish the sex of the birds.\n\n4\n\nWe add the facet_grid that splits the chart by both sex and species."
  },
  {
    "objectID": "plots-more.html#interactive-plots",
    "href": "plots-more.html#interactive-plots",
    "title": "8  Deeper into ggplot",
    "section": "8.10 Interactive plots",
    "text": "8.10 Interactive plots\nWant to make your plot interactive? You can use plotly’s ggplotly() function to transform your graph into an interactive chart.\nTo use plotly, you’ll want to install the plotly package, add the library, and then use the ggplotly() function:\n\nIn your R Console, run install.packages(\"plotly\"). (You only have to do this once on your computer.)\nAdd a new section to note you are creating an interactive chart.\nAdd the code below and run it. Then play with the chart!\n\n\nlibrary(plotly)\n\n1region_plot &lt;- ggplot(region_data,\n       aes(x = year, y = yearly_value)) + \n  geom_point(aes(color = state)) +\n  geom_line(aes(color = state)) +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_long_scale())) +\n  scale_x_date(minor_breaks = breaks_width(\"1 year\")) +\n  labs(title = \"Military police\", \n       x = \"Year\", y = \"Cost of acquisitions\",\n       caption = \"Source: Law Enforcement Support Office\",\n       color = \"State\")\n\nregion_plot |&gt;\n2  ggplotly()\n\n\n1\n\nWe saved our good plot into an object.\n\n2\n\nThen we pipe that object into the ggplotly() function, which makes it interactive.\n\n\n\n\n\n\n\n\nNow you have tool tips on your points when you hover over them.\nThe ggplotly() function is not perfect. For instance, I’ve shortened the title to the point of uselessness because it ran on top of the controls. I’m sure that could be overcome with some effort. Alternatively, you can use plotly’s own syntax to build some quite interesting charts, but it’s a whole new syntax to master."
  },
  {
    "objectID": "plots-more.html#what-we-learned",
    "href": "plots-more.html#what-we-learned",
    "title": "8  Deeper into ggplot",
    "section": "8.11 What we learned",
    "text": "8.11 What we learned\nThere is so much more to ggplot2 than what we’ve shown here, but these are the basics that should get you through the class. At the top of this chapter are a list of other resources to learn more."
  },
  {
    "objectID": "tidy-data.html#goals-for-this-section",
    "href": "tidy-data.html#goals-for-this-section",
    "title": "9  Tidy data",
    "section": "9.1 Goals for this section",
    "text": "9.1 Goals for this section\n\nExplore what it means to have “tidy” data.\nLearn about and use pivot_longer(), pivot_wider() to shape our data for different purposes.\nUse candy data to practice shaping data."
  },
  {
    "objectID": "tidy-data.html#the-questions-well-answer",
    "href": "tidy-data.html#the-questions-well-answer",
    "title": "9  Tidy data",
    "section": "9.2 The questions we’ll answer",
    "text": "9.2 The questions we’ll answer\n\nAre candy colors evenly distributed within a standard package of M&M’s? (i.e., average per color in a standard package)\n\nWe’ll plot the result as a column chart to show the average number of colored candies. We’ll do it first in ggplot, then Datawrapper.\n\nBonus 1: Who got the most candies in their bag?\nBonus 2: What is the average number of candy in a bag?"
  },
  {
    "objectID": "tidy-data.html#what-is-tidy-data",
    "href": "tidy-data.html#what-is-tidy-data",
    "title": "9  Tidy data",
    "section": "9.3 What is tidy data",
    "text": "9.3 What is tidy data\n“Tidy” data is well formatted so each variable is in a column, each observation is in a row and each value is a cell. Our first step in working with any data is to make sure we are “tidy”.\n\nIt’s easiest to see the difference through examples. The data frame below is of tuberculosis reports from the World Health Organization.\n\nEach row is a set of observations (or case) from a single country for a single year.\nEach column describes a unique variable. The year, the number of cases and the population of the country at that time.\n\n\nTable2 below isn’t tidy. The count column contains two different type of values.\n\nWhen our data is tidy, it is easy to manipulate. We can use functions like mutate() to calculate new values for each case.\n\nWhen our data is tidy, we can use the tidyr package to reshape the layout of our data to suit our needs. It gets loaded with library(tidyverse) so we won’t need to load it separately.\n\n9.3.1 Wide vs long data\nIn the figure below, the table on the left is “wide”. There are are multiple year columns describing the same variable. It might be useful if we want to calculate the difference of the values for two different years. It’s less useful if we want plot on a graphic because we don’t have a single “year” column to map as an x or y axes.\nThe table on the right is “long”, in that each column describes a single variable. It is this shape we need when we want to plot values on a chart in ggplot. We can then set our “Year” column as an x-axis, our “n” column on our y-axis, and group by the “Country”.\n\n\n\nWide vs long\n\n\nNeither shape is wrong, they are just useful for different purposes. In fact, you’ll find yourself pivoting the same data in different ways depending on your needs.\n\n\n9.3.2 Why we might want different shaped data\nThere are a myriad of reasons why you might need to reshape your data. Performing calculations on row-level data might be easier if it is wide. Grouping and summarizing calculations might be easier when it is long. ggplot graphics like long data, while Datawrapper sometimes wants wide data to make the same chart.\nI find myself shaping data back and forth depending on my needs."
  },
  {
    "objectID": "tidy-data.html#the-candy-project",
    "href": "tidy-data.html#the-candy-project",
    "title": "9  Tidy data",
    "section": "9.4 The candy project",
    "text": "9.4 The candy project\nLet’s visualize our goal here before we jump into the candy bowl.\nOur first question is this: Are there equal numbers of different-colored candies in a bag of M&M’s?\nTo visually communicate this, we’ll build a chart similar to this one (though this one is about Skittles instead of M&M’s.)\n\n\n\nSkittles example\n\n\nWe’ll build this chart in both ggplot and Datawrapper.\n\n9.4.1 Prepare our candy project\nWe will use candy count data we’ve been collected in Reporting wth Data classes to explore this subject.\nStart a new project.\n\nCreate a new project and call it: yourname-candy\nNo need to create data folders as we’ll just load data directly into the notebook.\nYou can use your index.qmd file … just change the title to “Tidy data: Candy”.\nCreate your setup block and load the tidyverse and janitor.\n\n\n\n9.4.2 Get the data\nWe’ll just load this data directly from Google Sheets into this notebook.\n\nAdd a Markdown section noting that you are importing data.\nAdd this import chunk and run it.\n\n\ncandy_raw &lt;- read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRCGayKLOy-52gKmEoPOj3ZKnOQVtCiooSloiCr-i_ci27e4n1CMPL0Z9s6MeFX9oQuN9E-HCFJnWjD/pub?gid=1456715839&single=true&output=csv\") |&gt; clean_names()\n\n# peek at the data\ncandy_raw\n\n\n\n  \n\n\n\nThis data comes from a Google Sheets document fed by a form that students have filled out, counting the colors of candies in a standard size bag of plain M&Ms.\n\n\n9.4.3 Drop unneeded columns\nFor this exercise we don’t need the timestamp, candy_type or box_code columns. We’ll drop them so we can keep things simple.\n\nCreate new section noting you’ll drop unneeded columns.\nCreate an R chunk and use select() to remove the columns noted above and save the result into a new data frame called candy. Remember you can negate a list of columns like this: !c(col1, col2).\n\nYou’ve done this in the past, so you should be able to do it on your own.\n\n\nYou got this! (But, just in case …)\ncandy &lt;- candy_raw |&gt; \n  select(!c(timestamp, candy_type, box_code))\n\n\n\n\n9.4.4 Peek at the wide table\nLet’s take look closer at this data:\n\ncandy |&gt; head()\n\n\n\n  \n\n\n\nThis is pretty well-formed data. This format would be useful to create a “total” column for each bag, but there are better ways to do this with long data. Same with getting our averages for each color.\n\n\n9.4.5 Where are we going with this data\nLet’s look at our plotting goal again:\n\n\n\nSkittle example\n\n\nWhere we want one axis to have the candy color and the other to have the average number of candy.\nTo plot a chart like this in ggplot or Datawrapper, the data needs to be the same shape, like this:\n\n\n\nColor\nAverage\n\n\n\n\nGreen\n10.9\n\n\nOrange\n12.0\n\n\nPurple\n12.4\n\n\nRed\n11.4\n\n\nYellow\n12\n\n\n\nIt will be easier to accomplish both of these tasks if our data were in the long format.\nSo, instead of this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nred\ngreen\norange\nyellow\nblue\nbrown\n\n\n\n\nChristian\nMcDonald\n2\n17\n11\n4\n16\n4\n\n\n\nWe want this:\n\n\n\nfirst_name\nlast_name\ncolor\ncandies\n\n\n\n\nChristian\nMcDonald\nred\n2\n\n\nChristian\nMcDonald\ngreen\n17\n\n\nChristian\nMcDonald\norange\n11\n\n\nChristian\nMcDonald\nyellow\n4\n\n\nChristian\nMcDonald\nblue\n16\n\n\nChristian\nMcDonald\nbrown\n4"
  },
  {
    "objectID": "tidy-data.html#the-tidyr-verbs",
    "href": "tidy-data.html#the-tidyr-verbs",
    "title": "9  Tidy data",
    "section": "9.5 The tidyr verbs",
    "text": "9.5 The tidyr verbs\nThe two functions we’ll use to reshape are data are:\n\npivot_longer() which “lengthens” data, increasing the number of rows and decreasing the number of columns.\npivot_wider() which “widens” data, increasing the number of columns and decreasing the number of rows.\n\nAgain, the best way to learn this is to present a problem and then solve it with explanation."
  },
  {
    "objectID": "tidy-data.html#pivot-longer",
    "href": "tidy-data.html#pivot-longer",
    "title": "9  Tidy data",
    "section": "9.6 Pivot longer",
    "text": "9.6 Pivot longer\nThis visualization gives you an idea how pivot_longer() works.\n\n\n\nPivot longer\n\n\nEach column of data chosen (the colored ones) is turned into it’s own row of data. Supporting data (the grey columns) are duplicated.\nThe pivot_longer() function needs several arguments: cols=, names_to= and values_to. Below are two examples to pivot the example data shown above.\n\n\n\npivot_longer code\n\n\n\ncols= is where you define a range of columns you want to pivot. For our candy data we want the range red:brown.\nnames_to= allows you to name the new column filled by the column names. For our candy data we want to name this “color” since that’s what those columns described.\nvalues_to= allows you to name the new column filled with the cell data. For us we want to call this “count_candies” since these are the number of candies in each bag.\n\nThere are a number of ways we can describe the cols= argument … anything in tidy-select works. You can see a bunch of examples here.\n\n9.6.1 Pivot our candy data longer\nWhat we want here is six rows for each person’s entry, with a column for “color” and a column for “count_candies”.\nWe are using a range, naming the first “red” and the last column “brown” with : in between. This only works because those columns are all together. Otherwise we could list the all with c(red, blue, green) etc.\n\nAdd a note that you are pivoting the data\nAdd the chunk below and run it\n\n\ncandy_long &lt;- candy |&gt; \n  pivot_longer(\n1    cols = red:brown,\n2    names_to = \"color\",\n3    values_to = \"count_candies\"\n  )\n\ncandy_long |&gt; head()\n\n\n1\n\nSets which columns to pivot based on their names.\n\n2\n\nSets the new column name for the former column names. This would default to “name” if we didn’t set it.\n\n3\n\nSets the new column name for the former values. This would default to “value” if we didn’t set it.\n\n\n\n\n\n\n  \n\n\n\n\n\n9.6.2 Get average candies per color\nTo get the average number of candies per each color, we can use our candy_long data and group_by color (which will consider all the red rows together, etc.) and use summarize() to get the mean.\nThis is very similar to the sum()s we did with military surplus, but you use mean() instead. One thing you have to watch for with mean() is you might need the argument na.rm = TRUE if there are missing or zero values, since you can’t divide by zero. We’ll include that here in case there are bags that don’t have any of a specific color.\nSave the resulting summary table into a new tibble called candy_avg.\n\ncandy_avg &lt;- candy_long |&gt; \n  group_by(color) |&gt; \n1  summarize(avg_candies = mean(count_candies, na.rm = TRUE))\ncandy_avg\n\n\n1\n\nThe na.rm hedges our bets in case some bags are missing a color.\n\n\n\n\n\n\n  \n\n\n\n\n\n9.6.3 Round the averages\nLet’s modify this summary to round the averages to tenths so they will plot nicely on our chart.\nThe round() function needs the column to change, and then the number of digits past the decimal to include.\n\nEdit your summarize function to add the mutate() function below.\n\n\ncandy_avg &lt;- candy_long |&gt; \n  group_by(color) |&gt; \n  summarize(avg_candies = mean(count_candies, na.rm = TRUE)) |&gt; \n  mutate(\n    avg_candies = round(avg_candies, 1)\n  )\n\ncandy_avg\n\n\n\n  \n\n\n\nBONUS POINT OPPORTUNITY: Using a similar method to rounding above, you can also capitalize the names of the colors. You don’t have to do this, but I’ll give you a bonus point if you do:\n\nIn your mutate, add a rule that updates color column using str_to_title(color).\n\nYou can read more about converting the case of a string here. It’s part of the stringr package, which is loaded with tidyverse.\n\n\n9.6.4 On your own: Plot the averages\nNow I want you to use ggplot to create a bar chart that shows the average number of candies by color in a bag. This is very similar to the Disney Princesses bar chart in Intro to ggplot).\n\nBuild a bar chart of average color using ggplot.\n\nSome things to consider:\n\nI want the bars to be ordered by the highest average on top.\nI want you to have a good title, subtitle and byline, along with good axes names. Make sure a reader has all the information they need to understand what you are communicating with the chart.\nInclude the values on the bars.\nChange the theme to something other than the default.\n\nHere is what it should more or less look like, but with good text, etc:\n\n\n\n\n\nThe numbers in the example above may not be up to date, so don’t let that throw you."
  },
  {
    "objectID": "tidy-data.html#introducing-datawrapper",
    "href": "tidy-data.html#introducing-datawrapper",
    "title": "9  Tidy data",
    "section": "9.7 Introducing Datawrapper",
    "text": "9.7 Introducing Datawrapper\nThere are some other great charting tools that journalists use. My favorite is Datawrapper and it is free for the level you need it.\nDatawrapper is so easy I don’t even have to teach you how to use it. They have excellent tutorials.\nWhat you do need is the data to plot, but you’ve already “shaped” it the way you need it. Your candy_avg tibble is what you need.\nHere are the steps I want you to follow:\n\n9.7.1 Review how to make a bar chart\n\nIn a web browser, go to the Datawrapper Academy\nClick on Bar charts\nChoose How to create a bar chart\n\nThe first thing to note there is they show you what they expect the data to look like. Your candy_avg tibble is just like this, but with Color and Candies.\nYou’ll use these directions to create your charts so you might keep this open in its own tab.\n\n\n9.7.2 Start a chart\n\nIn a new browser tab, go to datawrapper.de and click the big Start creating button.\nUse the Login/Sign Up button along the top to create an account or log in if you have one.\nThe first screen you have is where you can Upload data or paste it into the window. We are going to upload the data, but we have to write out the data to your computer first.\n\n\n\n9.7.3 Export your data for Datawrapper\n\nGo back to RStudio.\nCreate a new block and name it something about exporting\nTake your data and pipe it into write_csv(). As an argument to write_csv(), give it a path and name of the file: We can write directly to our project folder as candy_avg.csv.\n\n\ncandy_avg |&gt; write_csv(\"candy_avg.csv\")\n\nThis will save your data file onto your computer in your project folder.\n\n9.7.3.1 posit.cloud export\nIf you are using posit.cloud, you will also need to Export your data from the cloud to get it onto your computer so you can import it into Datawrapper. Do this only if you are using the cloud version.\n\nIn the Files window (bottom right) go inside your data-processed folder.\nClick on the checkbox next to the candy_avg.csv file.\nClick on the More blue gear thing and choose Export.\nYou’ll get a prompt to name the file (which you can keep the same) and then a download button. Click the button.\nThe file should go to your Downloads folder like anything else you download from the Internet.\nWhen you go back to Datawrapper click on the XLS/CSV upload button and go find your file to import.\n\n\n\n\n9.7.4 Build the datawrapper graphic\n\nReturn to Datawrapper in your browser and click on the XLS/CSV upload button and go find your file to import it.\nOnce your data is either uploaded or copied into Datawrapper, you can click Proceed to go to the next step.\n\nYou can now follow the Datawrapper Academy directions to finish your chart.\nWhen you get to the Publish & Embed window, I want you to click the Publish Now button and then add the resulting Link to your visualization: URL to your R Notebook so I can find it for grading."
  },
  {
    "objectID": "tidy-data.html#pivot-wider",
    "href": "tidy-data.html#pivot-wider",
    "title": "9  Tidy data",
    "section": "9.8 Pivot wider",
    "text": "9.8 Pivot wider\nNow that you’ve pivoted data longer, I’d also like to show how you can pivot your table into a wide format. I’ll sometimes do this to make an easier to read display or to shape data a specific way for Datawrapper or some other visualization software. Let’s walk through he concept first.\nAs you can imagine, pivot_wider() does the opposite of pivot_longer(). When we pivot wider we move our data from a “long” format to a “wide” format. We create a new column based categories and values in the data.\n\n\n\nLong to wide\n\n\npivot_wider() needs two arguments:\n\nnames_from = lets us define from which column (or columns) we are pulling values from to create the new column names. In the example above, this would be “V1”.\nvalues_from = lets us say which column will be the values in the new arrangement. In the example above this would be “V2”.\n\nSo the code to flip the data above would be:\ndf |&gt; \n  pivot_wider(names_from = V1, values_from = V2)\n\n9.8.1 Pivot wider example\nLet’s do this with our data, taking our “long” data and making it “wide” again.\nHere is our “long” data:\n\ncandy_long\n\n\n\n  \n\n\n\n… and now we flip it back to having a column for each color:\n\ncandy_long |&gt; \n  pivot_wider(\n1    names_from = color,\n2    values_from = count_candies\n  )\n\n\n1\n\nWithin pivot_wider() we need to tell it from which column to pull the new column headers. i.e., where do we get the “names_from”?\n\n2\n\nThe second thing it needs to know which variable has the data to fill these new columns. i.e., where do we get the “values_from”?\n\n\n\n\n\n\n  \n\n\n\nLet’s practice some more.\n\n\n9.8.2 Choosing what to flip\nLets create a different look at our data that we can spin it around different ways.\nWhen I buy candy for the class, we buy big boxes with many bags in it. There is a “box code” stamped on each package that denotes which box it was packaged in. We’ll re-summarize our data so we get the color averages within each box. I do some other cleanup here, too, to remove bags that came from the wild.\n\nCreate a new section and note you are pivoting wider\nAdd a new chunk with the code below and run it\n\nSee annotations below the code to explain what is going on, in case you are interested.\n\n1candy_box_color &lt;- candy_raw |&gt;\n2  filter(box_code != \"130FXWAC0336\", box_code != \"1524976SE\") |&gt;\n3  select(!c(timestamp, candy_type)) |&gt;\n4  pivot_longer(cols = red:brown, names_to = \"candy_color\", values_to = \"candy_count\") |&gt;\n5  group_by(box_code, candy_color) |&gt;\n6  summarise(avg_candies = mean(candy_count) |&gt; round(1))\n\ncandy_box_color\n\n\n1\n\nWe create a new object to put everything in, then go back to the original data to get the box code.\n\n2\n\nRemoving some rows where there were only one or two bags. Not shown here is the research to find their values.\n\n3\n\nRemoving the timestamp and candy_type variables.\n\n4\n\nPivoting the data longer so we can group and summarize.\n\n5\n\nGroup by the box and color before we …\n\n6\n\nSummarize to get the average number of candies in each bag, within each box. We also round the result.\n\n\n\n\n\n\n  \n\n\n\nOK, now you have a new object called candy_box_color.\nLet’s pivot this data so shows a column for each box, which means that is where we draw our “names_from”:\n\nAdd a note that you’ll pivot our new data\nAdd the code below and run it\n\n\ncandy_box_color |&gt; \n  pivot_wider(names_from = box_code, values_from = avg_candies)\n\n\n\n  \n\n\n\n\n\n9.8.3 Pivot wider on your own\nNow I want you do apply the same pivot_wider() function to that same candy_box_color data, but to have a column for each color and a row for each box.\n\nStart a new section and note this is pivot_wider on your own.\nStart with the candy_box_color data, and then …\nUse pivot_wider() to make the data shaped like this\n\n\n\n\nbox_code\nblue\nbrown\ngreen\norange\nred\nyellow\n\n\n\n\n[box name]\n#\n#\n#\n#\n#\n#"
  },
  {
    "objectID": "tidy-data.html#bonus-questions",
    "href": "tidy-data.html#bonus-questions",
    "title": "9  Tidy data",
    "section": "9.9 Bonus questions",
    "text": "9.9 Bonus questions\nMore opportunities for bonus points on this assignment. These aren’t plots, just data wrangling to find answers.\n\n9.9.1 Most/least candies\nAnswer me this: Who got the most candies in their bag? Who got the least?\nI want a well-structured section (headline, text) with two chunks, one for the most and one for the least.\n\n\n9.9.2 Average total candies in a bag\nAnswer me this: What is the average number of candy in a bag?\nAgain, well-structured section and include the code.\nHint: You need a total number of candies per person before you can get an average."
  },
  {
    "objectID": "tidy-data.html#turn-in-your-work",
    "href": "tidy-data.html#turn-in-your-work",
    "title": "9  Tidy data",
    "section": "9.10 Turn in your work",
    "text": "9.10 Turn in your work\n\nMake sure your notebook Renders.\nPublish it to Quarto Publish. Include the published link at the top of your notebook.\nStuff your project and turn it into the Candy assignment in Canvas."
  },
  {
    "objectID": "tidy-data.html#what-we-learned",
    "href": "tidy-data.html#what-we-learned",
    "title": "9  Tidy data",
    "section": "9.11 What we learned",
    "text": "9.11 What we learned\n\nWe learned what “tidy data” means and why it is important. It is the best shape for data wrangling and plotting.\nWe learned about pivot_longer() and pivot_wider() and we used them to transpose our data.\nWe also used round() to round off some numbers, and you might have used str_to_title() to change the case of the color values."
  },
  {
    "objectID": "denied-cleaning.html#goals-of-the-chapter",
    "href": "denied-cleaning.html#goals-of-the-chapter",
    "title": "10  Denied Cleaning",
    "section": "10.1 Goals of the chapter",
    "text": "10.1 Goals of the chapter\n\nMerge multiple data files with bind_rows()\nJoin data frames with inner_join()\nUse str_remove() to clean data\nIntroduce if_else() for categorization\n\nWe’ll use the results of this chapter in our next one."
  },
  {
    "objectID": "denied-cleaning.html#the-story-an-update-to-denied",
    "href": "denied-cleaning.html#the-story-an-update-to-denied",
    "title": "10  Denied Cleaning",
    "section": "10.2 The story: An update to Denied",
    "text": "10.2 The story: An update to Denied\nIn 2016, the Houston Chronicle published a multi-part series called Denied that outlined how a Texas Education Agency policy started in 2004 could force an audit of schools that have more than 8.5% of their students in special education programs. The story was a Pulitzer Prize finalist. Here’s an excerpt:\n\nOver a decade ago, the officials arbitrarily decided what percentage of students should get special education services — 8.5% — and since then they have forced school districts to comply by strictly auditing those serving too many kids.\n\n\nTheir efforts, which started in 2004 but have never been publicly announced or explained, have saved the Texas Education Agency billions of dollars but denied vital supports to children with autism, attention deficit hyperactivity disorder, dyslexia, epilepsy, mental illnesses, speech impediments, traumatic brain injuries, even blindness and deafness, a Houston Chronicle investigation has found.\n\n\nMore than a dozen teachers and administrators from across the state told the Chronicle they have delayed or denied special education to disabled students in order to stay below the 8.5 percent benchmark. They revealed a variety of methods, from putting kids into a cheaper alternative program known as “Section 504” to persuading parents to pull their children out of public school altogether.\n\nFollowing the Chronicle’s reporting (along with other news orgs), the Texas Legislature in 2017 unanimously banned using a target or benchmark on how many students a district or charter school enrolls in special education.\nWe want to look into the result of this reporting based on three things:\n\nHas the percentage of special education students in Texas changed since the benchmarking policy was dropped?\nHow many districts were above that arbitrary 8.5% benchmark before and after the changes?\nHow have local districts changed?\n\nTo prepare for this:\n\nRead Part 1 of the original Denied series, at least to the heading “A special child.” Pay attention to parts about the “Performance-Based Monitoring Analysis System.”\nRead About this series\nRead this followup about the legislative changes."
  },
  {
    "objectID": "denied-cleaning.html#about-the-data",
    "href": "denied-cleaning.html#about-the-data",
    "title": "10  Denied Cleaning",
    "section": "10.3 About the data",
    "text": "10.3 About the data\nEach year, the Texas Education Agency publishes the percentage of students in special education as part of their Texas Academic Performance Reports. We can download a file that has the percentages for each district in the state.\nThere are some challenges, though:\n\nWe have to download each year individually. There are nine years of data.\nThe are no district names in the files, only a district ID. We can get a reference file, though.\nThere are some differences in formatting for some files.\n\nI will save you the hassle of going through the TAPR database to find and download the individual files, and I will also supply code to clean the files to make them consistent. I’ll try not to get lost in the weeds along the way.\n\n10.3.1 Set up your project\nWith this project you’ll start with a copy I have prepared for you. How you do that differs a little depending on the RStudio platform you are using.\n\n10.3.1.1 If you are using RStudio Desktop\nYou will download a project that is already set up for you and then open it.\nUse the link below to download the project.\n\n\n\n\n\n\n Download rwdir-sped-template-main.zip\n\n\n\n\nFind that file on your computer and uncompress it.\nRename the project folder to yourname-sped but use your name.\nMove the project folder to your rwd folder or wherever you’ve been saving your class projects.\nIn RStudio, choose File &gt; New Project. Choose EXISTING Directory at the next step and then find the folder you saved. Use that to create your project.\n\n\n\n10.3.1.2 If you are a posit.cloud user\n\nFrom your posit.cloud account, go to this shared project\nClick Save a permanent copy so you have your own version.\nRename the project yourname-sped but use your name.\n\n\n\n\n10.3.2 Open, read and run\nOnce you have your project set up …\n\nOpen up the index.qmd and 01-import.qmd notebooks and Render them.\nRead every line\nRun each chunk as you do.\n\nGo ahead. I’ll wait.\n\n\n\n\n\nvia GIPHY\n\nThere is a lot to take in there about where the data came from and how we dealt with it. Here is where you end up:\n\n\nYou have 10 data files for each year and one reference file imported."
  },
  {
    "objectID": "denied-cleaning.html#merging-data-together",
    "href": "denied-cleaning.html#merging-data-together",
    "title": "10  Denied Cleaning",
    "section": "10.4 Merging data together",
    "text": "10.4 Merging data together\nOK, so we have all these different yearly files. Wouldn’t it be a lot easier if these were ONE thing? Indeed, we can merge these files together by stacking them on top of each other. Let’s review the concept using Starburst data:\n\nHere’s another representation of the concept. You have two data sets and you stack them on top of each other where the column names match. (Note here that identical rows in both data sets remain).\n\nSince all of our data files have the same column names, we can easily merge them with function bind_rows().\nLet’s demonstrate through building it.\n\nStart a new section on your R Markdown document and note you are merging data\nAdd a chunk with just the dstud13 data and run it.\n\n\ndstud13\n\n\n\n  \n\n\n\nThe result shows there are 1228 rows and 5 variables in the data, which should match what shows for dstud13 in your Environment tab.\n\nNow edit that chunk to use bind_rows() with dstud14 and run it.\n\n\ndstud13 |&gt; \n  bind_rows(dstud14)\n\n\n\n  \n\n\n\nThis shows we now have 2455 rows and 5 variables. This is good … we’ve addded the rows of dstud14 but we don’t have any new columns because the column names were identical.\nNow edit the chunk to do all these things:\n\nWithin the bind_rows() function, also add the dstud15 dataframe so you can see you are adding more on.\nSave the result of the merge into a new data frame called sped_merged.\nAt the bottom of the chunk print out the sped_merged tibble and pipe it into count(year) so you can make sure you continue to add rows correctly.\n\nIt should look like this:\n\nsped_merged &lt;- dstud13 |&gt; \n  bind_rows(\n    dstud14,\n    dstud15\n  )\n\n# we use this to ensure we bind correctly when we add new years\nsped_merged |&gt; count(year)\n\n\n\n  \n\n\n\nWe are NOT saving the count() result into a new object; We are just printing it to our screen to make sure we get all the years.\nNow that we know this is working, you’ll finish this out on your own.\n\nEdit your chunk to add bind_rows() for the rest of the files dstud16 through dstud22. You just keep tacking them on like we did with dstud15.\nAfter you are done, make sure you look at the sped_merged listing in your environment to make sure you end up with a count for each year of data.\n\n\nYou should end up with 12098 rows.\nOK, we have all our data in one file, but we still don’t know the district names. It’s time to Join our data with our reference file."
  },
  {
    "objectID": "denied-cleaning.html#about-joins",
    "href": "denied-cleaning.html#about-joins",
    "title": "10  Denied Cleaning",
    "section": "10.5 About joins",
    "text": "10.5 About joins\nOK, before we go further we need to talk about joining data. It’s one of those Basic Data Journalism Functions …\n\nWhat joins do is match rows from two data sets that have a column of common values, like an ID or county name. (The district ID column in our case). Columns from the second data set will be added based on where the ID’s match.\nThere are several types of joins. We describe these as left vs right based on which table we reference first (which is the left one). How much data you end up with depends on the “direction” of the join.\n\nAn inner_join puts together columns from both tables where there are matching rows. If there are records in either table where the IDs don’t match, they are dropped.\nA left_join will keep ALL the rows of your first table, then bring over columns from the second table where the IDs match. If there isn’t a match in the second table, then new values will be blank in the new columns.\nA right_join is the opposite of that: You keep ALL the rows of the second table, but bring over only matching rows from the first.\nA full_join keeps all rows and columns from both tables, joining rows when they match.\n\nHere are two common ways to think of this visually.\nIn the image below, The orange represents the data that remains after the join.\n\nThis next visual shows this as tables where only two rows “match” so you can see how non-matches are handled (the lighter color represents blank values). The functions listed there are the tidyverse versions of each join type.\n\n\n10.5.1 Joining our reference table\nIn our case we start with the dref data and then use an inner_join to add all the yearly data values. We’re doing it in this order so the dref values are listed first in our resulting table.\n\nStart a new Markdown section and note we are joining the reference data\nAdd the chunk below and run it\n\n\nsped_joined &lt;- dref |&gt; \n  inner_join(sped_merged, by = \"district\")\n\nsped_joined |&gt; head()\n\n\n\n  \n\n\n\nLet’s explain what is going on here:\n\nWe are creating a new bucket sped_joined to save our data.\nWe start with dref so those fields will be listed first in our result.\nWe then pipe into inner_join() to sped_merged, which will attach our dref data to our merged data when the ID matches in the district variable.\nThe by = \"district\" argument ensures that we are matching based on the district column in both data sets.\n\nWe could’ve left out the by = argument and R would match columns of the same name, but it is best practice to specify your joining columns so it is clear what is happening. You wouldn’t want to be surprised by other columns of the same name that you didn’t want to join on. If you wanted to specify join columns of different names it would look like this: df1 |&gt; inner_join(df2, by = c(\"df1_id\" = \"df2_id\"))\nYou might also glimpse() it so you can see all the columns have been added.\n\nsped_joined |&gt; glimpse()\n\nRows: 11,882\nColumns: 9\n$ district &lt;chr&gt; \"001902\", \"001902\", \"001902\", \"001902\", \"001902\", \"001902\", \"…\n$ distname &lt;chr&gt; \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYU…\n$ cntyname &lt;chr&gt; \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"…\n$ dflalted &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"…\n$ dflchart &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"…\n$ year     &lt;chr&gt; \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020…\n$ dpetallc &lt;dbl&gt; 595, 553, 577, 568, 576, 575, 564, 557, 535, 574, 1236, 1207,…\n$ dpetspec &lt;dbl&gt; 73, 76, 76, 78, 82, 83, 84, 82, 78, 84, 113, 107, 126, 144, 1…\n$ dpetspep &lt;dbl&gt; 12.3, 13.7, 13.2, 13.7, 14.2, 14.4, 14.9, 14.7, 14.6, 14.6, 9…\n\n\nThere are now 11882 rows in our joined data, fewer than what was in the original merged file because some districts (mostly charters) have closed and were not in our reference file. We are comparing only districts that have been open during this time period. For that matter, we don’t want charter or alternative education districts at all, so we’ll drop those next."
  },
  {
    "objectID": "denied-cleaning.html#some-cleanup-filter-and-select",
    "href": "denied-cleaning.html#some-cleanup-filter-and-select",
    "title": "10  Denied Cleaning",
    "section": "10.6 Some cleanup: filter and select",
    "text": "10.6 Some cleanup: filter and select\nFiltering and selecting data is something we’ve done time and again, so you should be able to do this on your own.\nYou will next remove the charter and alternative education districts. This is a judgement call on our part to focus on just traditional public schools. We can always come back later and change if needed.\nYou’ll also remove and rename columns to make the more descriptive.\n\nStart a new markdown section and note you are cleaning up your data.\nCreate an R chunk and start with the sped_joined and then do all these things …\nUse filter() to keep rows where:\n\nthe dflalted field is “N”\nAND the dflchart field is “N”\n\nUse select() to:\n\nremove (or not include) the dflalted and dflchart columns. (You can only do this AFTER you filter with them!)\n\nUse select() or rename() to rename the following columns:\n\nchange dpetallc to all_count\nchange dpetspec to sped_count\nchange dpetspep to sped_percent\n\nMake sure all your changes are saved into a new data frame called sped_cleaned.\n\nI really, really suggest you don’t try to write that all at once. Build it one line at a time so you can see the result as you build your code.\n\n\nI’m being too nice here\nsped_cleaned &lt;- sped_joined |&gt; \n  filter(dflalted == \"N\" & dflchart == \"N\") |&gt; \n  select(\n    district,\n    distname,\n    cntyname,\n    year,\n    all_count = dpetallc,\n    sped_count = dpetspec,\n    sped_percent = dpetspep\n  )\n\n\nYou should end up with 10204 rows and 7 variables."
  },
  {
    "objectID": "denied-cleaning.html#create-an-audit-benchmark-column",
    "href": "denied-cleaning.html#create-an-audit-benchmark-column",
    "title": "10  Denied Cleaning",
    "section": "10.7 Create an audit benchmark column",
    "text": "10.7 Create an audit benchmark column\nPart of this story is to note when a district is above the “8.5%” benchmark that the TEA was using for their audit calculations. It would be useful to have a column that noted if a district was above or below that threshold so we could plot districts based on that flag. We’ll create this new column and introduce the logic of if_else().\nOK, our data looks like this:\n\nsped_cleaned |&gt; head()\n\n\n\n  \n\n\n\nWe want to add a column called audit_flag that says ABOVE if the sped_percent is above “8.5”, and says BELOW if it isn’t. This is a simple true/false condition that is perfect for the if_else() function.\n\nAdd a new Markdown section and note that you are adding an audit flag column\nCreate an r chunk that and run it and I’ll explain after.\n\n\nsped_flag &lt;- sped_cleaned |&gt;\n  mutate(audit_flag = if_else(sped_percent &gt; 8.5, \"ABOVE\", \"BELOW\"))\n\n# this pulls 30 random rows so I can check results\nsped_flag |&gt; sample_n(10)\n\n\n\n  \n\n\n\nLet’s walk through the code above:\n\nWe’re making a new data frame called sped_flag and then starting with sped_cleaned.\nWe use mutate() to create a new column and we name it audit_flag.\nWe set the value of audit_flag to be the result of this if_else() function. That function takes three arguments: A condition test (sped_percent &gt; 8.5 in our case), what is the result if the condition is true (“ABOVE” in our case), and what is the result if the condition is NOT true (“BELOW”) in our case.\nLastly we print out the new data sped_cleaned and pipe it into sample_n() which gives us a number of random rows from the data. I do this because the top of the data was all TRUE so I couldn’t see if the mutate worked properly or not. (Always check your calculations!!)"
  },
  {
    "objectID": "denied-cleaning.html#export-the-data",
    "href": "denied-cleaning.html#export-the-data",
    "title": "10  Denied Cleaning",
    "section": "10.8 Export the data",
    "text": "10.8 Export the data\nThis is something you’ve done a number of times as well, so I leave you to you:\n\nMake a new section and note you are exporting the data\nExport your sped_flag data using write_rds() and save it in your data-processed folder.\n\nIn the next chapter we’ll build an analysis notebook to find our answers!"
  },
  {
    "objectID": "denied-analysis.html#goals-of-this-chapter",
    "href": "denied-analysis.html#goals-of-this-chapter",
    "title": "11  Denied analysis",
    "section": "11.1 Goals of this chapter",
    "text": "11.1 Goals of this chapter\n\nIntroduce datatables() from the DT package\nPractice pivots to prepare data for plotting\nPractice plots to reveal insights in data\nThere are wrap-up assignments that include writing, charts and this analysis\n\nWhen a new concept is introduced, it’s shown and explained here. However, there are also on your own parts where you apply concepts you have learned in previous chapters or assignments."
  },
  {
    "objectID": "denied-analysis.html#project-setup",
    "href": "denied-analysis.html#project-setup",
    "title": "11  Denied analysis",
    "section": "11.2 Project setup",
    "text": "11.2 Project setup\n\nWithin the same project you’ve been working, create a new Quarto Notebook. You might call it 02-analysis.qmd.\nWe will be using a new package so you’ll need to install it. Use your Console to run install.packages(\"DT\").\nInclude the libraries below and run them.\n\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)"
  },
  {
    "objectID": "denied-analysis.html#import-cleaned-data",
    "href": "denied-analysis.html#import-cleaned-data",
    "title": "11  Denied analysis",
    "section": "11.3 Import cleaned data",
    "text": "11.3 Import cleaned data\n\nCreate a section for your import\nImport your cleaned data and call it sped if you want to follow along here.\n\nYou should know how to do all that and I don’t know what you called your export file anyway.\nBut to recap, the data should look like this:\n\nsped |&gt; head()"
  },
  {
    "objectID": "denied-analysis.html#make-a-searchable-table",
    "href": "denied-analysis.html#make-a-searchable-table",
    "title": "11  Denied analysis",
    "section": "11.4 Make a searchable table",
    "text": "11.4 Make a searchable table\nWouldn’t it be nice to be able to see the percentage of special education students for each district for each year? The way our data is formatted now, that’s pretty hard to see with our “long” data here.\nWe could use something more like this? (But with all the years)\n\n\n\ndistname\ncntyname\n2013\n2014\n2015\netc\n\n\n\n\nCAYUGA ISD\nANDERSON\n12.3\n13.7\n13.2\n…\n\n\nELKHART ISD\nANDERSON\n9.1\n8.9\n10.4\n…\n\n\nFRANKSTON ISD\nANDERSON\n10.8\n9.7\n9.7\n…\n\n\nNECHES ISD\nANDERSON\n11.1\n9\n11.1\n…\n\n\nPALESTINE ISD\nANDERSON\n7.7\n7.8\n8.7\n…\n\n\nWESTWOOD ISD\nANDERSON\n9.3\n10\n9.3\n…\n\n\n\nAnd what if you could make that table searchable to find a district by name or county? That would be magic, right?\nWe can do this by first reshaping our data using pivot_wider() and then applying a function called datatable().\n\n11.4.1 Pivot wider\nYou used pivot_wider() with the candy data in Chapter 9, so you can look back at how that was done, but here are some hints:\n\nCreate a new section that notes you are creating a table of district percents.\nFirst use select() to get just the columns you need: distname, cntyname, year and sped_percent.\nThen use pivot_wider()to make a tibble like the one above. Remember that the names_from = argument wants to know which column you want to use to create the names of the new columns. The values_from = argument wants to know which column to pull the cell values from.\nSave the result into a new tibble and call it district_percents_data\n\n\n\nYou won’t need this because I believe in you\ndistrict_percents_data &lt;- sped |&gt; \n  select(distname, cntyname, year, sped_percent) |&gt; \n  pivot_wider(names_from = year, values_from = sped_percent)\n\n\n\n\n11.4.2 Make a datatable\nNow comes the magic.\n\nIn a new R chunk take your district_percents_data and then pipe it into a function called datatable()\n\n\ndistrict_percents_data |&gt;\n  datatable()\n\n\n\n\n\n\n\nThat’s kinda brilliant, isn’t it? You know can search by any value in the table.\nQuick, tell me what was the percentage for Austin ISD in 2020?\nThat will be useful tool for you when you are writing about specific districts."
  },
  {
    "objectID": "denied-analysis.html#choosing-a-chart-to-display-data",
    "href": "denied-analysis.html#choosing-a-chart-to-display-data",
    "title": "11  Denied analysis",
    "section": "11.5 Choosing a chart to display data",
    "text": "11.5 Choosing a chart to display data\nOur first question about this data was this: Has the percentage of special education students in Texas changed since the benchmarking policy was dropped?\nGiven the data we have, can we answer this?\nLet’s think about the charts that might be able to show two related variables like that. Choosing the chart type to display takes experimentation and exploration. Chapter 4 of Nathan Yau’s Data Points book is an excellent look at which chart types help show different data.\nThis decision tree option might also help you think through it.\n\nLastly, another resource to consider this is the ggplot cheatsheet, where it includes possible chart types based on the type of data we are comparing."
  },
  {
    "objectID": "denied-analysis.html#plot-yearly-student-percentage",
    "href": "denied-analysis.html#plot-yearly-student-percentage",
    "title": "11  Denied analysis",
    "section": "11.6 Plot yearly student percentage",
    "text": "11.6 Plot yearly student percentage\nI’ll often do a hand drawing of a chart that might help me understand or communicate data. This helps me think about how to summarize and shape my data to get to that point.\nFor this question Has the percentage of special education students in Texas changed since the benchmarking policy was dropped?, we could show the percentage of special education students for each year. If we are to chart this, the x axis would be the year and the y axis would be the percentage for that year. Perhaps like this:\n\nWe have the percentage of students in special education for each district in each year. We could get an average of those percentages for each year, but that won’t take into account the size of each district. Some districts have a single-digit number of students while others have hundreds of thousands of students.\nBut we also have the number of all students in a district with all_count and the number of special education students sped_count. With these, we can build a more accurate percentage across all the districts. This allows us to calculate our student groups by year.\n\n11.6.1 Build the statewide percentage by year data\nSo, let’s summarize our data. This is a “simple” group_by() and summarize() to get yearly totals, then a mutate() to build our percentage. I’ll supply the logic so you can try writing it yourself.\n\nStart a new notebook section and note you are getting yearly percentages\nIn an R chunk, start with your sped data.\nGroup by the year variable.\nIn summarize, create a sum() of the all_count variable. Call this total_students.\nIn the same summarize, create a sum() of the sped_count variable. You might call this total_sped.\nCheck your results at this point … make sure it makes sense.\nNext use mutate() to create a percentage called sped_percent from your total_students and total_sped summaries. The math for percentage is: (part / whole) * 100. You might want to round that resulting value to tenths as well.\nI suggest you save all that into a new tibble called yearly_percent (because that’s what I’m gonna do).\n\n\n\nMy version\nyearly_percent &lt;- sped |&gt; \n  group_by(year) |&gt; \n  summarise(\n    total_students = sum(all_count),\n    total_sped = sum(sped_count)\n  ) |&gt; \n  mutate(sped_percent = ((total_sped / total_students) * 100) |&gt; round(1))\n\nyearly_percent\n\n\n\n\n  \n\n\n\n\n\n11.6.2 Build the statewide percentage chart\nAnd with that table, you can plot your chart using the year and sped_percent.\n\n1yearly_percent |&gt;\n2  ggplot(aes(x = year, y = sped_percent)) +\n3  geom_col() +\n4  geom_text(aes(label = sped_percent, vjust = -.5))\n\n\n1\n\nI start with the data AND THEN pipe into ggplot.\n\n2\n\nWithin ggplot, set the aesthetics for the x and y axis.\n\n3\n\nAdd geom_col() to build the chart. This is better than geom_bar() because it assumes one number and one categorical datum.\n\n4\n\nThe geom_text() plots the labels on top of the bars based sped_percent but we need to adjust them with vjust = to move them above the bar.\n\n\n\n\n\n\n\nThis definitely shows us that the percentage of students in special education (in traditional public schools) has increased each year since 2017 when the benchmark was dropped and then outlawed by the legislature.\nI would be careful about saying the increase is because of the changes (though that is likely true), but you can certainly say with authority that it has gone up, and interview other people to pontificate on the reasons why.\nBTW, if you wanted to make that chart in datawrapper, you could use the same tibble data."
  },
  {
    "objectID": "denied-analysis.html#districts-by-benchmark-and-year",
    "href": "denied-analysis.html#districts-by-benchmark-and-year",
    "title": "11  Denied analysis",
    "section": "11.7 Districts by benchmark and year",
    "text": "11.7 Districts by benchmark and year\nOur second question is this: How many districts were above that arbitrary 8.5% benchmark before and after the changes?\nIt was in anticipation for this that we built the audit_flag field in our data. With that we can count how many rows have the “ABOVE” or “BELOW” value. (In reality, it’s probably at this point we would discover that might be useful that field is and have to go back to the cleaning notebook to create it. I wanted to get that part out of the way so we can concentrate on the charts.)\nTo decide on which chart to build, we can go back to our chart decision workflow or consult the ggplot cheetsheet. Given we have the discrete values of audit_flag and years, and we want to plot how many districts are counted (which is a continuous value), we’re looking at a stacked or grouped column/bar chart or a line chart.\n\n11.7.1 Build the audit flag data\nBefore we can build the chart, we need to summarize our data. The logic is this: We need to group our data by both the year and the audit_flag, and then count the number of rows for those values.\n\nStart a new Markdown section and note you are counting districts by the audit flag.\nStart with your original sped data.\nGroup your data by both year and audit_flag.\nSummarize your data by counting n() the results. Name the variable count_districts.\nSave the resulting data into a tibble called flag_count_districts. You might print that result out so you can refer to it.\n\n\n\nUse GSA or the count() shortcut\nflag_count_districts &lt;- sped |&gt; \n  count(year, audit_flag, name = \"count_districts\")\n\n# I used count()\n\n\nThe data should look like this:\n\n\n\n\n  \n\n\n\n\n\n11.7.2 Build the audit flag chart\nNow that we have the data, we can build the ggplot column chart. The key new thing here is we are using a new aesthetic fill to apply colors based on the audit_flag column.\n\nAdd some notes you are buliding the first exploratory chart\nAdd an R chunk with the following:\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, fill = audit_flag)) +\n  geom_col()\n\n\n\n\nThis is actually not a bad look because it does clearly show the number of districts below the 8.5% audit benchmark is dropping year by year, and the number of districts above is growing.\nLeave that chart there for reference, but let’s build a new one that is almost the same, but we’ll adjust it to be a grouped column chart instead of stacked. The key difference is we are adding position = \"dodge\" to the column geom.\n\nNote in Markdown text you are building the grouped column version\nAdd a new chunk and start with exactly what you have above.\nUpdate the column geom to this: geom_col(position = \"dodge\")\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, fill = audit_flag)) +\n1  geom_col(position = \"dodge\")\n\n\n1\n\nThis is where we add position = \"dodge\" to set the bars side-by-side.\n\n\n\n\n\n\n\nThat’s not bad at all … it might be the winner.\nLastly, let’s chart this data as a line chart to see if that looks any better or is easier to comprehend.\n\nNote you are visualizing as a line\nAdd the chunk below.\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, group = audit_flag)) +\n  geom_line(aes(color = audit_flag)) +\n1  ylim(0,1000)\n\n\n1\n\nWe added ylim() here because the default didn’t start the y axis at zero. ylim is a shortcut for scale_y_continuous(limits = c(0,1000)).\n\n\n\n\n\n\n\n\n\n11.7.3 Which chart is best?\nRemember, this is the question you are trying to answer: How many districts were above that arbitrary 8.5% benchmark before and after the changes?\nI would say the grouped bar chart (the bars next to each other) is probably best to explain this concept."
  },
  {
    "objectID": "denied-analysis.html#local-districts",
    "href": "denied-analysis.html#local-districts",
    "title": "11  Denied analysis",
    "section": "11.8 Local districts",
    "text": "11.8 Local districts\nWe have one last question: How have local districts changed? i.e., what are the percentages for districts in Bastrop, Hays, Travis and Williamson counties? We want to make sure none of these buck the overall trend.\nYou can certainly use the searchable table we made to get an idea of the number of districts and how the numbers have changed, but you can’t see them there. Searching there does reveal there are too many districts to visualize them all at once. Maybe we can chart one county at a time.\nLooking at the chart suggestions workflow, we are doing a comparison over time of many categories … so our trusty line chart is the horse to hitch.\nTo make that line chart we think about our axes and groups: We need our x axis of year, y axis of the sped_percent and we need to group the lines by their district. Since we need a column that has each year, the long data we started with should suffice:\n\nsped |&gt;\n  head()\n\n\n\n  \n\n\n\nWe just need to filter that down to a single county cntyname == \"BASTROP\" to show this. We can even do this all in one code block. Let’s see if you can follow this logic and build Bastrop county for yourself.\n\nStart a new section that you are looking at local districts\nStart a new chunk with your original sped data\nFilter it to have just rows for BASTROP county\nPipe that result into the ggplot() function\nFor the x axis, you are using year, for the y use sped_percent and for group use distname\nInside your geom_line() set the the color to the district: aes(color = distname).\n\nIt should look like this:\n\n\nMy version\nsped |&gt; \n  filter(cntyname == \"BASTROP\") |&gt; \n  ggplot(aes(x = year, y = sped_percent, group = distname)) +\n  geom_line(aes(color = distname))\n\n\n\n\n\n\n11.8.1 On your own\n\nNow do the same for the other three counties, each in their own code chunk: Hays, Travis and Williamson.\n\nThese charts give you some reference for the local districts. You’ll see the more districts there are within a county, the less effective the line chart becomes. But at least it gives you an idea of which districts are following the trend."
  },
  {
    "objectID": "denied-analysis.html#turn-in-your-project",
    "href": "denied-analysis.html#turn-in-your-project",
    "title": "11  Denied analysis",
    "section": "11.9 Turn in your project",
    "text": "11.9 Turn in your project\n\nMake sure everything runs and Renders properly.\nPublish your changes to Quarto Pub and include the link to your project in your index notebook so I can bask in your glory.\nZip your project folder. (Or export to zip if you are using posit.cloud).\nUpload to the Canvas assignment.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo be clear, it is your zipped project I am grading. The Quarto Pub link is for convenience."
  },
  {
    "objectID": "denied-analysis.html#what-we-learned-in-this-chapter",
    "href": "denied-analysis.html#what-we-learned-in-this-chapter",
    "title": "11  Denied analysis",
    "section": "11.10 What we learned in this chapter",
    "text": "11.10 What we learned in this chapter\n\nbind_rows() allows us to stack similar data into a single object.\ninner_join() and its siblings allow us to add columns based on a common values in the data sets.\nWe utilized count() which is a shortcut for grouping and counting rows of data."
  },
  {
    "objectID": "functions.html#importexport",
    "href": "functions.html#importexport",
    "title": "Appendix A — R Functions",
    "section": "A.1 Import/Export",
    "text": "A.1 Import/Export\n\nread_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(\"path/to/file.csv\").\nwrite_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(\"path/to/file.rds\").\nreadxl is a package we didn’t use, but it has read_excel() that allows you to import from an Excel file, including specified sheets and ranges.\nclean_names() from the library(janitor) package standardizes column names."
  },
  {
    "objectID": "functions.html#data-manipulation",
    "href": "functions.html#data-manipulation",
    "title": "Appendix A — R Functions",
    "section": "A.2 Data manipulation",
    "text": "A.2 Data manipulation\n\nselect() to select columns. Example: select(col01, col02) or select(-excluded_col).\nrename() to rename a column. Example: rename(new_name = old_name).\nfilter() to filter rows of data. Example: filter(column_name == \"value\").\n\nSee Relational Operators like ==, &gt;, &gt;= etc.\nSee Logical operators like &, | etc.\nSee is.na tests if a value is missing.\n\ndistinct() will filter rows down to the unique values of the columns given.\narrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc())\nmutate() changes and existing column or creates a new one. Example: mutate(new_col = (col01 / col02)).\nround() is a base R function that can round a number to a set decimal point. Often used within a mutate() function.\nrecode(), if_else() and case_when() are all functions that can be used with mutate() to create new categorizations with your data.\npivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns. Example: pivot_longer(cols = 3:5, names_to = \"new_key_col_name\", values_to = \"new_val_col_name\") will take the third through the fifth columns and turn each value into a new row of data. It will put them into two columns: The first column will have the name you give it in names_to and contain the old column name that corresponds to each value pivoted. The second column will have the name of whatever you set in values_to and will contain all the values from each of the columns.\npivot_wider() is the opposite of pivot_longer(). Example: pivot_wider(names_from = col_of_key_values, values_from = col_with_values). See the link."
  },
  {
    "objectID": "functions.html#aggregation",
    "href": "functions.html#aggregation",
    "title": "Appendix A — R Functions",
    "section": "A.3 Aggregation",
    "text": "A.3 Aggregation\n\ngroup_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. We often add arrange() to these, calling this our GSA functions. Example: group_by(song, artist) %&gt;% summarize(weeks = n(), top_chart_position = min(peak_position)). To break or remove groupings, use ungroup().\ncount() is a shortcut for GSA that count the number rows based on variable groups you feed it."
  },
  {
    "objectID": "functions.html#math",
    "href": "functions.html#math",
    "title": "Appendix A — R Functions",
    "section": "A.4 Math",
    "text": "A.4 Math\nThese are the function often used within summarize():\n\nn() to count the number of rows. n_distinct() counts the unique values.\nsum() to add things together.\nmean() to get an average.\nmedian() to get the median.\nmin() to get the smallest value. max() for the largest.\n+, -, *, / are math operators similar to a calculator."
  },
  {
    "objectID": "posit-cloud.html#setting-up-a-new-project",
    "href": "posit-cloud.html#setting-up-a-new-project",
    "title": "Appendix B — Using posit.cloud",
    "section": "B.1 Setting up a new project",
    "text": "B.1 Setting up a new project\nSince posit.cloud works with projects be default, there are no templates with example files. Since we are working with the Quarto Website project type in this class, you have to manually configure some files. It isn’t hard.\n\nStart posit.cloud if you haven’t already\nStart a new project\nAt the top of the page, you should name your project something other than “Untitled Project”\n\n\nB.1.1 Install packages\nThis is a list of the packages we use the most in the class. I would start with these, and install others only as needed.\n\nIn the Console, copy and paste this command and run it.\n\ninstall.packages(c(\n  \"quarto\",\n  \"rmarkdown\",\n  \"tidyverse\",\n  \"janitor\"\n  )\n)\nIt will take some time to run. Your internet connection will have an impact on the speed.\n\n\nB.1.2 Create the Quarto file\n\nUse the new document toolbar button to create a Text file\nPaste in the code below.\nSave the file and name it _quarto.yml.\n\nThe name must be exact.\nproject:\n  type: website\n\nwebsite:\n  title: \"Site name\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      #- filename.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n    df-print: paged\n    code-overflow: wrap\n\n\nB.1.3 Create your index file\n\nUse the new document toolbar button to create a new Quarto Document\nFor the Title field, use your project name, like “Billboard project”\nUncheck the visual editor button\nImmediately save the file and name it index.qmd\n\nAt some point you’ll likely add a new file and want to replace filename.qmd with your filename and remove the # comment. You’ll can add other files there as you create them. This adds them to the website navigation.\nYou should be good to go with a new project at this point. You can Render your index to see what the site looks like at this point."
  },
  {
    "objectID": "posit-cloud.html#exporting-a-project",
    "href": "posit-cloud.html#exporting-a-project",
    "title": "Appendix B — Using posit.cloud",
    "section": "B.2 Exporting a project",
    "text": "B.2 Exporting a project\nYou can export your project as a .zip file to turn in to an assignment or share with others.\n\nIn the Files pane, click the box next to the Cloud icon to select all your files.\nUnder the More gear there is a dropdown. Click on that.\nChoose Export from the More menu.\n\n\n\n\nExporting a project\n\n\nThis should download all your files as a .zip file, which you can upload to Canvas."
  },
  {
    "objectID": "posit-cloud.html#share-your-project",
    "href": "posit-cloud.html#share-your-project",
    "title": "Appendix B — Using posit.cloud",
    "section": "B.3 Share your project",
    "text": "B.3 Share your project\nIt is possible to share your posit.cloud project with another user on the service and they will get a copy of it.\nYou can find directions for that here."
  },
  {
    "objectID": "group-dates.html#setting-up",
    "href": "group-dates.html#setting-up",
    "title": "Appendix C — Grouping by dates",
    "section": "C.1 Setting up",
    "text": "C.1 Setting up\nWe need to set up our notebook with libraries and data before we can talk specifics. We need to load both the tidyverse and lubridate libraries. Lubridate is installed with the tidyverse package, but you have to load it separately.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nAnd we need our cleaned Billboard Hot 100 data.\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100 |&gt; glimpse()\n\nRows: 338,500\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08…\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H…\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby …\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…"
  },
  {
    "objectID": "group-dates.html#plucking-date-parts",
    "href": "group-dates.html#plucking-date-parts",
    "title": "Appendix C — Grouping by dates",
    "section": "C.2 Plucking date parts",
    "text": "C.2 Plucking date parts\nIf you look at the lubridate cheetsheet under “GET AND SET DATE COMPONENTS” you’ll see functions to pluck out parts of a date, like year().\nIf we have a date, like perhaps Taylor Swift’s birthday, we can pluck the year from it.\n\nyear(\"1989-12-13\")\n\n[1] 1989"
  },
  {
    "objectID": "group-dates.html#grouping-by-a-date-part-on-the-fly",
    "href": "group-dates.html#grouping-by-a-date-part-on-the-fly",
    "title": "Appendix C — Grouping by dates",
    "section": "C.3 Grouping by a date part on the fly",
    "text": "C.3 Grouping by a date part on the fly\nLet’s show how this might be useful through an example question:\nWhich performer has the most appearances on the chart in a given year?\nThe logic works like this:\n\nGroup all the records by performer AND the year of the chart_date\nSummarize and count the rows\n\n\nhot100 |&gt; \n  group_by(\n    year(chart_date),\n    performer\n  ) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n\n\n  \n\n\n\nIt looks like Morgan Wallen has the most appearances and 2023 is only half over as of this running. When I ran this code last year The Beatles topped the list. There is definitely some kinda story here.\nAnyway, notice how the year column name is kinda shite? We would not be able to easily reference that variable later, so we should rename that AS we make the group:\n\nhot100 |&gt; \n  group_by(\n    yr = year(chart_date), # added \"yr = \" here\n    performer\n  ) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n\n\n  \n\n\n\nIt is a good practice to rename any grouping variable made from a function like that. FWIW, it would’ve worked if I called the new column year, but I named it yr so I’m less likely to confuse it with the function year(). It’s a personal preference what to name the new column."
  },
  {
    "objectID": "group-dates.html#making-reusable-date-parts",
    "href": "group-dates.html#making-reusable-date-parts",
    "title": "Appendix C — Grouping by dates",
    "section": "C.4 Making reusable date parts",
    "text": "C.4 Making reusable date parts\nIf you think you’ll use a date parts more than once, then it makes sense to create a new columns and save them. You might make several date parts, but we’ll start with only one.\nI usually go back to my cleaning notebook to add these once I recognize I need them, and then rerun everything.\nTo make this easier to show, I’ve created a random sample of data with only the chart_date and title columns. Here is our sample:\n\n# you wouldn't normally do this!!!!\nhot100_sample &lt;- hot100 |&gt; slice_sample(n = 6) |&gt; select(chart_date, title)\n\nhot100_sample\n\n\n\n  \n\n\n\n\nC.4.1 Let’s make a year\nHere’s how we do it:\n\nWe use mutate to create a new column.\nWe name the new column yr.\nWe set the value of yr to equal the year() of chart_date.\n\n\nhot100_sample |&gt; \n  mutate(\n    yr = year(chart_date)\n  )\n\n\n\n  \n\n\n\n\n\nC.4.2 The magical month\nWe can also pluck out the month of the date, which is pretty useful if you want to measure seasonality within a year, like hot days of summer, etc. The default month() function pulls the month as a number.\n\nhot100_sample |&gt; \n  mutate(\n    mo = month(chart_date)\n  )\n\n\n\n  \n\n\n\nBut there are some options within month() to give us month NAMES that are ordered as factors instead of alphabetical.\n\nhot100_sample |&gt; \n  mutate(\n    mo_label = month(chart_date, label = TRUE),\n    mo_long = month(chart_date, label = TRUE, abbr = FALSE)\n  ) |&gt; \n  arrange(mo_label)\n\n\n\n  \n\n\n\nNote the datatype &lt;ord&gt; under the column mo_label and mo_long. That means this is an “ordered factor” and that when arranged by those labels it will list in MONTH order instead of alphabetical order, which is quite useful.\n\n\nC.4.3 Floor dates\nSometimes you want to count the number of records within a month and year, like all the songs in January 2000, then February 2000, etc. One way to do that is to create a floor_date, which gives you the “lowest” date within a certain unit like year or month. It’s easiest to show with our sample data:\n\nhot100_sample |&gt; \n  mutate(\n    fl_month = floor_date(chart_date, unit = \"month\"),\n    fl_year = floor_date(chart_date, unit = \"year\")\n  )\n\n\n\n  \n\n\n\nYou can see the resulting new columns are real dates, but they are normalized:\n\nThe fl_month gives you the first day of the month for that chart_date.\nThe fl_year gives you the first day of the year for that chart_date.\n\nLet’s put this to use with an example. I’ll create a fl_month on the fly to find Recent appearances by Taylor Swift. I’ll also do the year() on the fly in my filter.\n\nswift_month &lt;- hot100 |&gt; \n  filter(\n    performer == \"Taylor Swift\",\n    chart_date &gt;= \"2020-07-01\"\n  ) |&gt; \n  group_by(\n    fl_month = floor_date(chart_date, unit = \"month\")\n  ) |&gt; \n  summarize(appearances = n())\n\nswift_month\n\n\n\n  \n\n\n\nAnd chart it:\n\nswift_month |&gt; \n  ggplot(aes(x = fl_month, y = appearances)) +\n  geom_col() +\n  geom_text(aes(label = appearances), vjust = -.3) +\n  scale_x_date(date_labels=\"%b %Y\",date_breaks  =\"1 month\") +\n  guides(x =  guide_axis(angle = 45)) +\n  labs(\n    x = \"Month and Year\",\n    y = \"Number of appearances\",\n    title = \"Appearances on Billboard Hot 100 by Taylor Swift since 'Folklore' release\"\n  )\n\n\n\n\nCan you guess when she released her albums?"
  },
  {
    "objectID": "count.html#setup-and-import",
    "href": "count.html#setup-and-import",
    "title": "Appendix D — A counting shortcut",
    "section": "D.1 Setup and import",
    "text": "D.1 Setup and import\nWe don’t normally put these together, but we’re just setting up a quick demonstration\n\nlibrary(tidyverse)\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")"
  },
  {
    "objectID": "count.html#basic-count",
    "href": "count.html#basic-count",
    "title": "Appendix D — A counting shortcut",
    "section": "D.2 Basic count",
    "text": "D.2 Basic count\nWe’re going to rework our first quest of the Billboard analysis:\nWhich performer had the most appearances on the Hot 100 chart at any position?\nOur logic is we want to count the number of rows based on each performer. We do this by adding the variables we want to group as arguments to count():\n\nhot100 |&gt; \n  count(performer)\n\n\n\n  \n\n\n\n\nD.2.1 Sort the results\nIf we want the highest counted row at the top (and we almost always do) then we can add an argument: sort = TRUE.\n\nhot100 |&gt; \n  count(performer, sort = TRUE)\n\n\n\n  \n\n\n\n\n\nD.2.2 Name the new column\nNotice the counted table is called n. We can rename that with another argument, name = and give it the name we want in quotes.\n\nhot100 |&gt; \n  count(performer, sort = TRUE, name = \"appearances\")\n\n\n\n  \n\n\n\n\n\nD.2.3 Filter results as normal\nTo cut off the results, we just filter as we normally would.\n\nhot100 |&gt; \n  count(performer, sort = TRUE, name = \"appearances\") |&gt; \n  filter(appearances &gt; 650)\n\n\n\n  \n\n\n\nSo the code above does the same things here as we did in our first Billboard quest, but with fewer lines."
  },
  {
    "objectID": "count.html#grouping-on-multiple-variables",
    "href": "count.html#grouping-on-multiple-variables",
    "title": "Appendix D — A counting shortcut",
    "section": "D.3 Grouping on multiple variables",
    "text": "D.3 Grouping on multiple variables\nWe can group on multiple variables by adding them. We’ll show this with the second quest:\nWhich song (title & performer) has been on the charts the most?\n\nhot100 |&gt; \n  count(\n    title,\n    performer,\n    sort = TRUE,\n    name = \"appearances\") |&gt; \n  filter(appearances &gt;= 70)"
  },
  {
    "objectID": "troubleshooting.html#when-things-break",
    "href": "troubleshooting.html#when-things-break",
    "title": "Appendix E — Troubleshooting",
    "section": "E.1 When things break",
    "text": "E.1 When things break\nThe first and best troubleshooting advice I can give it this:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\nWhen you write and run code one line at a time, problems are easier to find.\nBeyond that there are generally two categories of problems: You wrote “it” wrong, or you used “it” wrong. The first can sometimes be hard to see just like any typo.\nSome tips:\n\nRead the error message and look for words you recognize. You may not understand the error exactly, but words can be clues to what part of your code is wrong.\nCheck the spelling of variables, values and functions, especially if the error message says something like object ‘wahtever’ not found. If you are writing code that depends on matching strings and you are getting unexpected results, check those strings. Word case (as in Title case) can matter.\nCheck the code for balanced parenthesis and other punctuation problems. RStudio will show you matching parenthesis with highlighting, and will indicate problems with red X icons and red underlines in your code. Writing beautiful, well-indenting can sometimes help avoid and spot these types of errors.\n\nBeyond that, here are some common things students come across:\n\nThere is no package called ‘packagename’: You are trying to use a library that is named wrong or you don’t have installed. You can install packages with install.packages('packagename') where ‘packagename’ is replaced with the name of the package you actually want.\nForgetting to use quotation marks when they are needed: install.packages(\"gclus\") will work, while install.packages(gclus) will generate an error.\nCould not find function ‘functionname’: You either misspelled the function or you are missing a library() in your setup. It’s best practice to have every library() loaded in a setup chunk near the top of the notebook.\nUsing the wrong case: help(), Help(), and HELP() are three different functions (and only the first one will work)\nForgetting to include the parentheses in a function call: help() rather than help. Even if there are no options, you still need the ().\nUsing the \\ in a path name on Windows” R sees the backlash character as an escape character. setwd(\"c:\\mydata\") will generate an error. Use setwd(\"c:/mydata\") or setwd(\"c:\\\\mydata\") instead."
  },
  {
    "objectID": "troubleshooting.html#learning-how-things-work",
    "href": "troubleshooting.html#learning-how-things-work",
    "title": "Appendix E — Troubleshooting",
    "section": "E.2 Learning how things work",
    "text": "E.2 Learning how things work\nThere are infinite ways to write code incorrectly or use a function improperly. Documentation and experience (sometimes of others) are key to these challenges.\n\nE.2.1 Help docs\nOne way to find documentation is through the built-in Help function within RStudio. If you look at the pane at the bottom-right of RStudio, you’ll see tabs for “Files”, “Plots”, “Packages” and “Help”. Click on the Help tab.\nYou can type in a function or part of a function and get a list of items. If you search for “count” and hit return you’ll get documentation on how to use it. It takes some getting used to in reading the docs, but the examples at the bottom are often useful.\nThere are also some Console commands to find things:\n\n\n\n\n\n\n\nFunction\nAction\n\n\n\n\nhelp.start()\nGeneral help\n\n\nhelp(\"foo\") or ?foo\nHelp on function foo (the quotation marks are optional)\n\n\nhelp.search(\"foo\") or ??foo\nSearch the help system for instances of the string foo\n\n\nexample(\"foo\")\nExamples of function foo (the quotation marks are optional)\n\n\n\n\n\nE.2.2 Good Googling\nAnother way to get help is to Google for it, but there is an art to it especially since there are other data science languages and programs with similar terms as R. Some tips:\n\nUse “in R” in your search: How to merge data frames in R\nUse the name of the package if you now it: Add labels with ggplot\nUse “tidyverse” if appropriate: convert text to date with tidyverse\n\nThere are plenty of Stack Overflow answers along with different tutorials from blogs and such. It is a well-used language, so there are lots of answers to help. Too many, sometimes.\n\n\nE.2.3 Tidyverse docs and cheatsheets\nIt is worth becoming familiar with the tidyverse site. Click on each icon from the home page to learn what each package does. R is also big on cheatsheets, which are printable pages that go through all the verbs. They can be a bit much at first, but useful once you use R more.\nWe’ll try to put together a list of other resources and tutorials. You can find some I’ve collected already here."
  },
  {
    "objectID": "troubleshooting.html#r-frequently-asked-questions",
    "href": "troubleshooting.html#r-frequently-asked-questions",
    "title": "Appendix E — Troubleshooting",
    "section": "E.3 R Frequently asked Questions",
    "text": "E.3 R Frequently asked Questions\n\nE.3.1 Functions, objects and variables\nThe names of things and how they are used are important in R, and can cause confusion. The term date could represent any number of things in R code depending on how you are using it, and that can be confusing. Knowing the difference between functions, objects and variables and how they are referenced in code helps.\n\nVariables are like the column names from a spreadsheet table. If you think of a data frame (or tibble in R) as a spreadsheet table, then when you reference a “variable” name, it is that column. A data frame of police calls might have a date column that has the date/time of the call in each row, like “2021-01-06 17:29:38”.\nFunctions are collections of code that solve specific problems. In R, they are always followed by parenthesis, like this: date(), which is a function to pull only the date from a date/time variable. There are often arguments inside a function, like date(date) could be a function pull the date “2021-01-06” from a date/time variable called date. Knowing that functions always have parenthesis is a good clue to help figure out how something is being used.\nObjects are stored values in R. You can name objects anything you like, including unwise things like date, since that is already a function and maybe a variable.\n\nThat is all to prove it can be confusing. When you have a chance to name things, make good choices.\n\nE.3.1.1 Naming things\nBe thoughtfully obvious about the names you choose for objects and variables.\n\nAvoid naming things with what could be a function name.\nAvoid spaces. Use an underscore _ or dash - instead. I use underscores for naming things in code, but dashes for naming files or folders that could become URLs at some point.\nBe descriptive. Name things what they are, like police_calls.\nBe consistent. If you have multiple date variables like open_date and close_date then it is easy to know and select() them.\n\n\n\n\nE.3.2 Some R code basics\n\n&lt;- is known as an “assignment operator” – it means “Make the object named to the left equal to the output of the code to the right”\n= makes an object equal to a value, which is similar to &lt;- but used within a function.\n== tests whether the objects on either end are equal. This is often used in filtering data.\n& means AND, in Boolean logic\n| means OR, in Boolean logic\n! means NOT, in Boolean logic\nWhen referring to values entered as text, or to dates, put them in quote marks like this: \"United States\", or \"2016-07-26\". Numbers are not quoted\nWhen entering two or more values as a list, combine them using the function c, for combine, with the values separated by commas, for example: c(\"2017-07-26\", \"2017-08-04\")\nAs in a spreadsheet, you can specify a range of values with a colon, for example: c(1:10) creates a list of integers (whole numbers) from one to ten.\nSome common operators:\n\n+ - add, subtract\n* / multiply, divide\n&gt; &lt; greater than, less than\n&gt;= &lt;= greater than or equal to, less than or equal to\n!= not equal to\n\nHandling null values:\n\nNulls are designated as NA\nis.na(x) looks for nulls within variable x.\n!is.na(x) looks for non-null values within variable x\n\n\nHere, is.na() is a function."
  },
  {
    "objectID": "explore.html#start-by-listing-questions",
    "href": "explore.html#start-by-listing-questions",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.1 Start by listing questions",
    "text": "F.1 Start by listing questions\nIt’s likely you’ve acquired data because you needed it to add context to a story or situation. Spend a little time at the beginning brainstorming as list of questions you want to answer. (You might ask a colleague to participate: the act of describing the data set will reveal questions for both of you.) I like to start my R notebooks with this list."
  },
  {
    "objectID": "explore.html#understand-your-data",
    "href": "explore.html#understand-your-data",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.2 Understand your data",
    "text": "F.2 Understand your data\nBefore you start working on your data, make sure you understand what all the columns and values mean. Look at your data dictionary, or talk to the data owner to make sure you understand what you are working with.\nTo get a quick summary of all the values, you can use a function called summary() to give you some basic stats for all your data. Here is an example from the Billboard Hot 100 data we used in a class assignment.\n\n\n\nSummary of billboard data\n\n\nA summary() will show you the data type for each column, and then for number values it will show you the min, max, median, mean and other stats."
  },
  {
    "objectID": "explore.html#pay-attention-to-the-shape-of-your-data",
    "href": "explore.html#pay-attention-to-the-shape-of-your-data",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.3 Pay attention to the shape of your data",
    "text": "F.3 Pay attention to the shape of your data\nIs your data long or wide?\nWide data adds new observations as columns, with the headers describing the observation. Official reports and Excel files from agencies are often in this format:\n\n\n\nCountry\n2018\n2017\n\n\n\n\nUnited States\n20,494,050\n19,390,604\n\n\nChina\n13,407,398\n12,237,700\n\n\n\nLong data is where each row in the data is a single observation, and each column is an attribute that describes that observation. Data-centric languages and applications like R and Tableau typically prefer this format.\n\n\n\nCountry\nYear\nGDP\n\n\n\n\nUnited States\n2018\n20,494,050\n\n\nUnited States\n2017\n19,390,604\n\n\nChina\n2018\n13,407,398\n\n\nChina\n2017\n12,237,700\n\n\n\nThe shape of the data will determine how you go about analyzing it. They are both useful in different ways. Wide data allows you to calculate columns to show changes. Visualization programs will sometimes want a long format to more easily categorize values based on the attributes.\nYou can pivot your data with pivot_longer() and pivot_wider to change the shape of your data."
  },
  {
    "objectID": "explore.html#counting-and-aggregation",
    "href": "explore.html#counting-and-aggregation",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.4 Counting and aggregation",
    "text": "F.4 Counting and aggregation\nA large part of data analysis is counting and sorting, or filtering and then counting and sorting. Depending on the program you are using you may approach it differently but think of these concepts:\n\nF.4.1 Counting rows based on a column\nIf you are just counting the number of rows based on the values within a column (or columns), then count() is the key. When you use count() like this, a new column called n is created to hold the count of the rows. You can rename n with the name = \"new_name\" argument, and you can change the sorting to descending order using the sort = TRUE argument.\nIn this example, we are counting the number of rows for each princess in our survey data, the arranging them in descending order.\nsurvey |&gt; \n  count(princess, name = \"votes\", sort = TRUE)\n\n\n\nprincess\nvotes\n\n\n\n\nMulan\n14\n\n\nRapunzel (Tangled)\n7\n\n\nJasmine (Aladdin)\n6\n\n\nAriel (Little Mermaid)\n5\n\n\nTiana (Princess and the Frog)\n2\n\n\nAurora (Sleeping Beauty)\n1\n\n\nBelle (Beauty and the Beast)\n1\n\n\nMerida (Brave)\n1\n\n\nSnow White\n1\n\n\n\n\n\nF.4.2 Sum, mean and other aggregations\nIf you want to aggregate values in a column, like adding together values, or to find a mean or median, then you will want to use the GSA combination: group_by() on your columns of interest, then use summarize() to aggregate the data in the manner you choose, like sum(), mean() or the number of rows n(). You can then use arrange() to order the result however you want.\nHere is an example where we use group_by and summarize() to add together values in our mixed beverage data. In this case, we had multiple rows for each name/address group, but we wanted to add together total_receipts() for each group.\nreceipts |&gt; \n  group_by(location_name, location_address) |&gt; \n  summarize(\n    total_sales = sum(total_receipts)\n  ) |&gt; \n  arrange(desc(total_sales))\n\n\n\n\n\n\n\n\nlocation_name\nlocation_address\ntotal_sales\n\n\n\n\nWLS BEVERAGE CO\n110 E 2ND ST\n35878211\n\n\nRYAN SANDERS SPORTS\n9201 CIRCUIT OF THE AMERICAS BLVD\n20714630\n\n\nW HOTEL AUSTIN\n200 LAVACA ST\n15435458\n\n\nROSE ROOM/ 77 DEGREE\n11500 ROCK ROSE AVE\n14726420\n\n\nTHE DOGWOOD DOMAIN\n11420 ROCK ROSE AVE STE 700\n14231072\n\n\n\nThe result will have all the columns you included in the group, plus the columns you create in your summarize statement. You can summarize more than one thing at a time, like the number of rows numb_rows = n() and average of the values average = mean(column_name).\n\n\nF.4.3 Creating columns to show difference\nSometimes you need to perform math on two columns to show the difference between them. Use mutate() to create the column and do the math. Here’s a pseudo-code example:\nnew_or_reassigned_df &lt;- df |&gt; \n  mutate(\n    new_col_name = (part_col / total_col) * 100\n  )"
  },
  {
    "objectID": "explore.html#cleaning-up-categorical-data",
    "href": "explore.html#cleaning-up-categorical-data",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.5 Cleaning up categorical data",
    "text": "F.5 Cleaning up categorical data\nIf you are going to count our summarize rows based on categorical data, you should make sure the values in that column are clean and free of typos and values that might better be combined.\nSome strategies you might use:\n\nCreate a count() of the column to show all the different values and how often they show up.\nYou might want to use mutate() to create a new column and then update the values there. Or you might use recode() the set specific values to new values.\n\nIf you find you have hundreds of values to clean, then come see me. There are some other tools like OpenRefine that you can learn fairly quickly to help."
  },
  {
    "objectID": "explore.html#time-as-a-variable",
    "href": "explore.html#time-as-a-variable",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.6 Time as a variable",
    "text": "F.6 Time as a variable\nIf you have dates in your data, then you almost always want to see change over time for different variables.\n\nSummarize records by year or month as appropriate and create a Bar or Column chart to show how the number of records for each time period.\nDo you need to see how different categories of data have changed over time? Consider a line chart that shows those categories in different colors.\nIf you have the same value for different time periods, do might want to see the change or percent change in those values. You can create a new column using mutate() to do the math and show the difference."
  },
  {
    "objectID": "explore.html#explore-the-distributions-in-your-data",
    "href": "explore.html#explore-the-distributions-in-your-data",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.7 Explore the distributions in your data",
    "text": "F.7 Explore the distributions in your data\nWe didn’t talk about histograms in class, but sometimes you might want see the “distribution” of values in your data, i.e. how the values vary within the column. Are many of the values similar? A histogram can show this.\nHere is an example of a histogram use wells data exploring the borehole_depth (how deep the well is). Each bar represents the number of wells broken down in 100ft depth increments (set with binwidth=100). So the first bar shows that most of the wells (more than 7000) are less than 100 feet deep.\nwells |&gt; \n  ggplot(aes(x = borehole_depth)) +\n  geom_histogram(binwidth = 100)\n\n\n\nBorehole depth histogram\n\n\nWhile there are wells deeper than 1000 feet, they are so few they don’t even show on the graphic.\nYou’ll rarely use a histogram as a graphic with a story because they are more difficult to explain to readers. But they do help you to understand how much values differ within a column.\n\nF.7.1 More on histograms\nIf you google around, you might see other ways to create a histogram, including hist() and qplot(). You might stick with the ggplot’s geom_histogram() since you already are familiar with the syntax.\n\nTutorial on histograms using ggplot from DataCamp.\nR Cookbook on histograms."
  },
  {
    "objectID": "explore.html#same-ideas-using-spreadsheets",
    "href": "explore.html#same-ideas-using-spreadsheets",
    "title": "Appendix F — Exploring a new dataset",
    "section": "F.8 Same ideas using spreadsheets",
    "text": "F.8 Same ideas using spreadsheets\nCheck out this resource by David Eads on the same topic, with some more specifics about Google Sheets."
  },
  {
    "objectID": "charts-tips.html#titles-descriptions-and-annotations",
    "href": "charts-tips.html#titles-descriptions-and-annotations",
    "title": "Appendix G — Chart production tips",
    "section": "G.1 Titles, descriptions and annotations",
    "text": "G.1 Titles, descriptions and annotations\nChart titles and descriptions can be some of the most difficult writing you can do as a journalist. You don’t want to describe the steps of your analysis nor say the obvious, but you do need to give the reader all the relevant detail needed to understand the chart. Write titles, descriptions an annotations as if the chart stands alone, and a reader knows nothing before viewing it.\nSome tips paraphrased from Nathan Yau’s Data Points book …\n\nThe title – typically larger and bolder fonts — sets the stage or describes what people should see or look for in the data. A descriptive title also helps. For example, “Rising Gas Prices” says more about the chart than just “Gas Prices.” By including “Rising”, it presents the conclusion immediately, and readers will look to the chart to verify and see the details. Saying just “Gas Prices” leaves the data interpretation to the readers and places them in the exploration phase.\nThe description or lead-in text is used to prepare readers for what a chart shows, but in further detail. The text expands on what the title declares, where the data is from, how it was derived, or what it means (best charts do this, says @crit). Basically, it’s information that might help others understand the data better but often doesn’t directly point to the specific elements.\nTo explain specifics points or areas, you can use lines and arrows as an annotation layer on top of a chart. This places descriptions directly in the context of the data so tha a readers doesn’t have to look outside a graph for additional information to fully understand what you show."
  },
  {
    "objectID": "charts-tips.html#other-considerations",
    "href": "charts-tips.html#other-considerations",
    "title": "Appendix G — Chart production tips",
    "section": "G.2 Other considerations",
    "text": "G.2 Other considerations\n\nProper data encodings and visual cues: Think about what you are trying to convey with the graphic and plot your data in a fashion that furthers that understanding. (See Nathan Yau’s Data Points, Chap. 3.)\nLegends for encodings: If your plots include labels for categories on the chart, you may not need a separate legend, but be sure readers can distinguish items.\nLabels for axis: They help describe the value being plotted. In some obvious cases where the meaning in clear, like years, they may be dropped.\nInclude unit values to further understanding: Sometimes the value can be added to the plot itself, other times grid lines may be enough.\nAnnotations: Add explanations to the plot if they help readers understand nuance of what you area trying to convey.\nSource of the data: This is the course of the data, not the delivery method. (i.e., The Comptroller of Public accounts, not data.texas.gov.)\nYour byline: Credit yourself and your publications."
  }
]