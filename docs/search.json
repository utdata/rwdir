[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting with Data in R",
    "section": "",
    "text": "About this book\nReporting with Data in R is a series of lessons and instructions used for courses in the School of Journalism and Media, Moody College of Communication at the University of Texas at Austin. The first two versions of the book were written by Associate Professor of Practice Christian McDonald. Assistant Professor Jo Lukito began collaborating and teaching sections of the Reporting with Data in spring 2022 and Anastasia Goodwin is teaching from this material as an adjunct in fall 2023.\nThis means the writting voice or point of view may change or be confusing at times. For instance, Prof.¬†McDonald uses a Mac and Prof.¬†Lukito uses a PC, so keyboard commands and screenshot examples may flip back and forth. We‚Äôll try to note the author and platform at the beginning of each chapter.\nThis version is being revised for fall 2023 version. Earlier versions are linked below."
  },
  {
    "objectID": "index.html#a-word-from-prof.-mcdonald",
    "href": "index.html#a-word-from-prof.-mcdonald",
    "title": "Reporting with Data in R",
    "section": "A word from Prof.¬†McDonald",
    "text": "A word from Prof.¬†McDonald\nI‚Äôm a strong proponent of what I call Scripted Journalism, a method of committing data-centric journalism in a programmatic, repeatable and transparent way. There are a myriad of programming languages that further this, including Python (pandas using Jupyter) and JavaScript (Observable), but we‚Äôll be using R, Quarto and RStudio.\nR is a super powerful, open-source programming language for data that is deep with features and an awesome community of users who build upon it. No matter the challenge before you in your data storytelling, there is probably a package available to help you solve that challenge. Probably more than one.\nThere is always more than one way to do things in R. This book is a Tidyverse-oriented, opinionated collection of lessons intended to teach students new to programming and R for the expressed act of committing journalism. As a beginner course, we strive to make it as simple as possible, which means we may not go into detail about alternative (and possibly better) ways to accomplish tasks in favor of staying in the Tidyverse and reducing options to simplify understanding. We rarely discuss differences from base R; Tidyverse is our default."
  },
  {
    "objectID": "index.html#tips-on-the-book-style",
    "href": "index.html#tips-on-the-book-style",
    "title": "Reporting with Data in R",
    "section": "Tips on the book style",
    "text": "Tips on the book style\nWe will try to be consistent in the way we write documentation and lessons. But we are human, so sometimes we break our own rules, but in general keep the following in mind.\n\nThings to do\nThings to DO are in ordered lists:\n\nDo this thing.\nThen do this thing.\n\nExplanations are usually in text, like this very paragraph.\nSometimes details will be explained in lists:\n\nThis is the first thing I want you to know.\nThis is the second. You don‚Äôt have to DO these things, just know about them.\n\n\n\nCode blocks\nThis book often runs the code that is shown, so you ‚Äôll see the code and the result of that code below it.\n\n1 + 1\n\n[1] 2\n\n\n\nCopying code blocks\nWhen you see R code in the instructions, you can roll your cursor over the right-corner and click on the copy icon to copy the code clock content to your clipboard.\n\nYou can then paste the code inside your R chunk.\nThat said, typing code yourself has many, many benefits. You learn better when you type yourself, make mistakes and have to fix them. We encourage you to always type short code snippets. Leave the copying to long ones.\n\n\nFenced code\nSometimes we need to show code chunk options that are added, like when explaining how to name chunks. In those cases, you may see the code chunk with all the tick marks, etc. like this:\n\n```{r block-named}\n1 + 1\n```\n\n[1] 2\n\n\nor\n\n```{r}\n#| label: block-named-yaml\n\n1 + 1\n```\n\n[1] 2\n\n\nYou can still copy/paste these blocks, but you‚Äôll get the entire code block, not just the contents.\n\n\nHidden code\nSometimes we want to include code in the book but not display it so you can try the to write the code yourself first. When we do this, it will look like this:\n\n\nClick here to show the code\n1 + 1\n\n\n[1] 2\n\n\n\n\n\nNotes, some important\nWe will use callouts to set off a less important aside:\n\n\n\n\n\n\nMarkdown was developed by JOHN GRUBER, as outlined on his Daring Fireball blog.\n\n\n\nBut sometimes those asides are important. We usually indicate that:\n\n\n\n\n\n\nImportant\n\n\n\nYou really should learn how to use Markdown as you will use it the whole semester, and hopefully for the rest of your life."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Reporting with Data in R",
    "section": "About the authors",
    "text": "About the authors\n\nChristian McDonald\nI‚Äôm a career journalist who most recently served as data and projects editor at the Austin American-Statesman before joining the University of Texas at Austin faculty full-time in fall 2018 as an assistant professor of practice. I‚Äôve taught data-related course at UT since 2013. I also serve as the innovation director of the Dallas Morning News Journalism Innovation Endowment.\n\nThe UT Data Github: utdata\nTwitter: crit\nEmail: christian.mcdonald@utexas.edu\n\n\n\nJo Lukito\nI‚Äôm an aspiring-journalist-turned-academic who studies journalism and digital media. To make a long story short (tl;dr): I trained to be a journalist as an undergraduate student, but just fell in love with researching and supporting journalism. I completed my Ph.D in 2020, and my dissertation focused on international trade reporting (which relies on plenty o‚Äô data). I also do a ton of social media research (especially in politics and disinformation), so if you‚Äôre interested in the social media beat, I‚Äôm your gal!\n\nProf.¬†Jo Lukito‚Äôs git: jlukito\nTwitter: JosephineLukito\nEmail: jlukito@utexas.edu\nWebsite: https://www.jlukito.com/"
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Reporting with Data in R",
    "section": "Versions",
    "text": "Versions\nThe first version of this book from spring 2019 is buried somewhere in the commit history, but we‚Äôve archived other recent editions:\n\nFall 2021\nSpring 2022\nFall 2022\nSpring 2023\n\n\n\n\n\n\n\nNote\n\n\n\nüëÜ These links are currently broken. We‚Äôll need to figure that out."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reporting with Data in R",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nLet‚Äôs just say this is free on the internet. We don‚Äôt make any money from it and you shouldn‚Äôt either."
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Reporting with Data in R",
    "section": "Other resources",
    "text": "Other resources\nThis text stands upon the shoulders of giants and by design does not cover all aspects of using R. Here are some other useful books, tutorials and sites dedicated to R. There are other task-specific tutorials and articles sprinkled throughout the book in the Resources section of select chapters.\n\nR for Data Science\nThe Tidyverse site, which has tons of documentation and help.\nThe RStudio Cheatsheets.\nggplot2: Elegant Graphics for Data Analysis\nR Graphics Cookbook\nThe R Graph Gallery another place to see examples.\nPractical R for Journalism by Sharon Machlis, an editor with PC World and related publications. Sharon is a longtime proponent of using R in journalism.\nSports Data Analysis and Visualization and Data Journalism with R and the Tidyverse by Matt Waite, a professor at the University of Nebraska-Lincoln.\nR for Journalists site by Andrew Tran, a reporter at the Washington Post and University of Texas alum. A series of videos and tutorials on using R in a journalism setting."
  },
  {
    "objectID": "install.html#mac-vs-pc",
    "href": "install.html#mac-vs-pc",
    "title": "1¬† Install Party",
    "section": "1.1 Mac vs PC",
    "text": "1.1 Mac vs PC\nWe‚Äôre a big fan of using keyboard commands to do operations in any program, but Prof.¬†McDonald references this from a Mac perspective and Prof.¬†Lukito references this from a PC perspective. So if we say use Cmd+S or Command+S to save, that translates to Cntl+S or Control+S on a PC. Importantly, Cmd (for Mac) and Cntl (for PC) are fairly synonymous, but there may be other differences. Usually, it‚Äôs easy to figure the translation out by looking at menu items in RStudio to figure out the Mac/PC command.\nAs we said, Prof.¬†McDonald uses a Mac and Prof.¬†Lukito uses a PC, so we‚Äôll be able to troubleshoot it regardless of the operating system that you use! We will try to note that at the top of a chapter.\nWe will install R and RStudio. It might take some time depending on your Internet connection.\nIf you are doing this on your own you might follow this tutorial. But below you‚Äôll find the basic steps."
  },
  {
    "objectID": "install.html#installing-r",
    "href": "install.html#installing-r",
    "title": "1¬† Install Party",
    "section": "1.2 Installing R",
    "text": "1.2 Installing R\nOur first task is to install the R programming language onto your computer.\n\nGo to the https://cloud.r-project.org/.\nClick on the link for your operating system.\nThe following steps will differ slightly based on your operating system.\n\nFor Macs, you want the ‚Äúlatest package‚Äù unless you have an ‚ÄúM1‚Äù Mac (Nov.¬†2020 or newer), in which case choose the arm64.pkg version.\nFor Windows, you want the ‚Äúbase‚Äù package. You‚Äôll need to decide whether you want the 32- or 64-bit version. (Unless you‚Äôve got a pretty old system, chances are you‚Äôll want 64-bit.)\n\n\nThis should be pretty self explanatory: once you download the installation file, you should be able to run it on your respective computers to install R.\nYou‚Äôll never ‚Äúlaunch‚Äù R as a program in a traditional sense, but you need it on your computers (it‚Äôs mostly so that the computer can recognize R as a ‚Äúlanguage‚Äù). In all situations (in this class, and beyond), we‚Äôll use RStudio, which is next."
  },
  {
    "objectID": "install.html#installing-rstudio",
    "href": "install.html#installing-rstudio",
    "title": "1¬† Install Party",
    "section": "1.3 Installing RStudio",
    "text": "1.3 Installing RStudio\nRStudio is an ‚Äúintegrated development environment‚Äù ‚Äì or IDE ‚Äì for programming in R. Basically, it‚Äôs the program you will use when doing work for this class.\n\nGo to https://www.rstudio.com/download.\nScroll down to the versions and find RStudio Desktop and click on the Download button.\nIt should take you down the page to the version you need for your computer.\nInstall it. Should be like installing any other program on your computer.\n\n\n1.3.1 Getting ‚ÄúGit‚Äù errors on Macs\nIf later during package installation you get errors that mention ‚Äúgit‚Äù or ‚Äúxcode-select‚Äù then say yes! and do it."
  },
  {
    "objectID": "install.html#class-project-folder",
    "href": "install.html#class-project-folder",
    "title": "1¬† Install Party",
    "section": "1.4 Class project folder",
    "text": "1.4 Class project folder\nTo keep things consistent and help with troubleshooting, we recommend that you save your work in the same location all the time.\n\nOn both Mac and Windows, every user has a ‚ÄúDocuments‚Äù folder. Open that folder. (If you don‚Äôt know where it is, ask us to help you find it.)\nCreate a new folder called ‚Äúrwd‚Äù. Use all lowercase letters.\n\nWhen we create new ‚ÄúProjects‚Äù, I want you to always save them in the Documents/rwd folder. This just keeps us all on the same page."
  },
  {
    "objectID": "install.html#rstudio-cloud",
    "href": "install.html#rstudio-cloud",
    "title": "1¬† Install Party",
    "section": "1.5 RStudio Cloud",
    "text": "1.5 RStudio Cloud\nIf your computer is aging and you find you have trouble with RStudio, there is RStudio Cloud. You can try it for free but might end up on the $5/month plan before the end of the semester, depending on your use. It works quite well."
  },
  {
    "objectID": "intro.html#rstudio-tour",
    "href": "intro.html#rstudio-tour",
    "title": "2¬† Introduction to R",
    "section": "2.1 RStudio tour",
    "text": "2.1 RStudio tour\nWhen you launch RStudio, you‚Äôll get a screen that looks like this:\n\n\n\nRStudio launch screen"
  },
  {
    "objectID": "intro.html#updating-preferences",
    "href": "intro.html#updating-preferences",
    "title": "2¬† Introduction to R",
    "section": "2.2 Updating preferences",
    "text": "2.2 Updating preferences\nThere is a preference in RStudio that I would like you to change. By default, the program wants to save a the state of your work (all the variables and such) when you close a project, but that is not good practice. We‚Äôll change that.\n\nGo to the RStudio menu and choose Preferences\nUnder the General tab, uncheck the first four boxes.\nOn the option ‚ÄúSave Workspace to .Rdata on exit‚Äù, change that to Never.\nClick Apply to save the change (but don‚Äôt close the box yet).\n\n\n\n\nRStudio preferences\n\n\nNext we will set some value is the Code pane.\n\nOn the left options, click on the Code pane.\nCheck the box for Use native pipe operator, |&gt;.\nClick OK to save and close the box.\n\n\n\n\nNative pipe preference\n\n\nWe‚Äôll get into why we did this part later."
  },
  {
    "objectID": "intro.html#starting-a-new-project",
    "href": "intro.html#starting-a-new-project",
    "title": "2¬† Introduction to R",
    "section": "2.3 Starting a new Project",
    "text": "2.3 Starting a new Project\nWhen we work in RStudio, we will create ‚ÄúProjects‚Äù to hold all the files related to one another. This sets the ‚Äúworking directory‚Äù, which is a sort of home base for the project.\n\nClick on the second button that has a green +R sign.\nThat brings up a box to create the project with several options. You want New Directory (unless you already have a Project directory, which you don‚Äôt for this.)\nFor Project Type, choose New Project.\nNext, for the Directory name, choose a new name for your project folder. For this project, use ‚Äúfirstname-first-project‚Äù but use YOUR firstname.\nFor the subdirectory, you want to use the Browse button to find your new rwd folder we created earlier.\n\nI want you to be anal about naming your folders. It‚Äôs a good programming habit.\n\nUse lowercase characters.\nDon‚Äôt use spaces. Use dashes.\nFor this class, start with your first name.\n\n\n\n\nRstudio project name, directory\n\n\nWhen you hit Create Project, your RStudio window will refresh and you‚Äôll see the yourfirstname-first-project.Rproj file in your Files list."
  },
  {
    "objectID": "intro.html#using-r-notebooks",
    "href": "intro.html#using-r-notebooks",
    "title": "2¬† Introduction to R",
    "section": "2.4 Using R Notebooks",
    "text": "2.4 Using R Notebooks\nFor this class, we will almost always use RNotebooks. This format allows us to write text in between our blocks of code. The text is written in a language called RMarkdown, a juiced-up version of the common documentation syntax used by programmers, Markdown. We‚Äôll learn that in a moment.\n\n2.4.1 Create your first notebook\n\nClick on the button at the top-left of RStudio that has just the green + sign.\nChoose the item R Notebook.\n\nThis will open a new file with some boilerplate R Markdown code. It serves as an example, explanation and template all in one.\n\nAt the top between the --- marks, is the metadata. This is written using YAML, and what is inside are commands for the R Notebook. Don‚Äôt sweat the YAML syntax too much right now, as we won‚Äôt be editing it often.\nNext, you‚Äôll see a couple of paragraphs of text that describes how to use an RNotebook. It is written in RMarkdown, and has some inline links and bold commands, which you will learn,\nThen you will see an R code chunk that looks like the figure below.\n\n\n\n\nR code chunk\n\n\nLet‚Äôs take a closer look at this:\n\nThe three back tick characters (the key found at the top left on your keyboard) followed by the {r} indicate that this is a chunk of R code. The last three back ticks say the code chunk is over.\nThe {r} bit can have some parameters added to it. We‚Äôll get into that later.\nThe line plot(cars) is R programming code. We‚Äôll see what those commands do in a bit.\nThe green right-arrow to the far right is a play button to run the code that is inside the chunk.\nThe green down-arrow and bar to the left of that runs all the code in the Notebook up to that point. That is useful as you make changes in your code and want to rerun what is above the chunk in question.\n\n\n\n2.4.2 Save the .Rmd file\n\nDo Cmd+S (ctrl+s for a PC) or hit the floppy disk icon to save the file.\nIt will ask you what you want to name this file. Call it 01-first-file.Rmd.\n\nWhen you do this, you may see another new file created in your Files directory. It‚Äôs the pretty version of the notebook which we‚Äôll see in a minute.\nIn the metadata portion of the file, give your notebook a better title.\n\nReplace ‚ÄúR Notebook‚Äù in the title: \"R Notebook\" code to be ‚ÄúChristian‚Äôs first notebook‚Äù, but use your name.\n\n\n\n2.4.3 Run the notebook\nThere is only one chunk to run in this notebook, so:\n\nClick on the green right-arrow to run the code. The keyboard command (from somewhere within the chunk) is Cmd+Shift+Return.\n\nYou should get something like this:\n\n\n\nCars plot\n\n\nWhat you‚Äôve done here is create a plot chart of a piece of sample data that is already inside R. (FWIW, It is the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s.)\nBut that wasn‚Äôt a whole lot of code to see there is a relationship with speed vs stopping distance, eh?\nThis is a ‚Äúbase R‚Äù plot. We‚Äôll be using the tidyverse ggplot methods later in the semester.\n\n\n2.4.4 A note about RMarkdown\nWe always want to annotate our code to explain what we are doing. To do that, we use a syntax called RMarkdown, which is an R-specific version of Markdown. We use this syntax because it both makes sense when you‚Äôre typing up your code and because it makes a very pretty version in HTML when we ‚Äúknit‚Äù our project. You can see how it to write RMarkdown here. Interesting tidbit: you can create slidedecks, websites, and handouts using RMarkdown, so it is useful in a lot of different ways.\nThis entire book is written in RMarkdown.\nHere is an example:\n## My dating age\n\nThe following section details the [socially-acceptable maximum age of anyone you should date](https://www.psychologytoday.com/us/blog/meet-catch-and-keep/201405/who-is-too-young-or-too-old-you-date).\n\nThe math works like this:\n\n- Take your age\n- subtract 7\n- Double the result\n\nThe ## line is a headline. Add more ### and you get a smaller headline, like subheads.\nThere is a full blank return between each element, including paragraphs of text.\nIn the first paragraph we have embedded a hyperlink. We put the words we want to show inside square brackets and the URL in parenthesis DIRECTLY after the closing square bracket: [words to link](https://the_url.org).\nThe - at the beginning of a line creates a bullet list. (You can also use *). Those lines need to be one after another without blank lines.\n\n\nGo ahead and copy the code above and add it as text in the notebook so you can see it works later.\n\n\n\n2.4.5 Adding new code chunks\nThe text after the chart describes how to insert a new code chunk. Let‚Äôs do that.\n\nAdd a couple of returns before the paragraph of text about code chunks.\nUse the keys Cmd+Option+i to add the chunk.\nYour cursor will be inserted into the middle of the chunk. Type in this code in the space provided:\n\n\n# update 55 to your age\nage &lt;- 55\n(age - 7) * 2\n\n[1] 96\n\n\n\nChange for ‚Äú55‚Äù to your real age.\nWith your cursor somewhere in the code block, use the key command Cmd+Shift+Return, which is the key command to RUN ALL LINES of code chunk.\n\n\nNOTE: To run an individual line, use Cmd+Return while on that line.\n\nCongratulations! The answer given at the bottom of that code chunk is the socially-acceptable maximum age of anyone you should date.\nThrowing aside whether the formula is sound, let‚Äôs break down the code.\n\n# update 55 to your age is a comment. It‚Äôs a way to explain what is happening in the code without being considered part of the code. We create comments by starting with #. You can also add a comment at the end of a line.\nage &lt;- 55 is assigning a number (55) to an R object/variable called (age). A variable is a placeholder. It can hold numbers, text or even groups of numbers. Variables are key to programming because they allow you to change a value as you go along.\nThe next part is simple math: (age - 7) * 2 takes the value of age and subtracts 7, then multiplies by 2.\nWhen you run it, you get the result of the math equazion, [1] 95 in my case. That means there was one observation, and the value was ‚Äú96‚Äù. For the record, my wife is much younger than that.\n\nNow you can play with the number assigned to the age variable to test out different ages. Do that.\n\n\n2.4.6 Practice adding code chunks\nNow, on your own, add a similar section that calculates the minimum age of someone you should date, but using the formula (age / 2) + 7.\n\nAdd a RMarkdown headline and text describing what you are doing.\nCreate a code chunk that that calculates the formula based on your age.\nInclude a comment within the code block.\n\nYou don‚Äôt need (or want) to recreate the age variable. Use the one that is already in your notebook.\n\n\n2.4.7 Preview the report\nThe rest of the boilerplate text here describes how you can Preview and Knit a notebook. Let‚Äôs do that now.\n\nPress Cmd+Shift+K to open a Preview.\n\nThis will open a new window and show you the ‚Äúpretty‚Äù notebook that we are building.\nPreview is a little different than Knit, which runs all the code, then creates the new knitted HTML document. It‚Äôs Knit to HMTL that you‚Äôll want to do before turning in your assignments. That is explained below.\n\n\n2.4.8 The toolbar\nOne last thing to point out before we turn this in: The toolbar that runs across the top of the R Notebook file window. The image below explains some of the more useful tools, but you REALLY should learn and use keyboard commands when they are available.\n\n\n\nR Notebook toolbar\n\n\n\n\n2.4.9 Knit the final workbook\n\nSave your File with Cmd+S.\nClick on the dropdown next to the Run menu item and choose Restart R and Run All Chunks. We do this to make sure everything still works.\nUse the Knit button in the toolbar to choose Knit to HTML.\n\nThis will open your knitted file. Isn‚Äôt it pretty?"
  },
  {
    "objectID": "intro.html#turning-in-our-projects",
    "href": "intro.html#turning-in-our-projects",
    "title": "2¬† Introduction to R",
    "section": "2.5 Turning in our projects",
    "text": "2.5 Turning in our projects\nIf you now look in your Files pane, you‚Äôll see you have four files in our project. (Note the only one you actually edited was the .Rmd file.)\n\n\n\nFiles list\n\n\nThe best way to turn in all of those files into Canvas is to compress them into a single .zip file that you can upload to the assignment.\n\nIn your computer‚Äôs Finder, open the Documents/rwd folder.\nFollow the directions for your operating system linked below to create a compressed version of your yourname-final-project folder.\nCompress files on a Mac.\nCompress flies on Windows.\nUpload the resulting .zip file to the assignment for this week in Canvas.\n\nIf you find you make changes to your R files after you‚Äôve zipped your folder, you‚Äôll need to delete the zip file and compress it again.\nBecause we are building ‚Äúrepeatable‚Äù code, I‚Äôll be able to download your .zip files, uncompress them, and the re-run them to get the same results.\nWell done! You‚Äôve completed the first level and earned the Beginner badge."
  },
  {
    "objectID": "counts-import.html#learning-goals-of-this-lesson",
    "href": "counts-import.html#learning-goals-of-this-lesson",
    "title": "3¬† Summarize: count - import",
    "section": "3.1 Learning goals of this lesson",
    "text": "3.1 Learning goals of this lesson\n\nPractice organized project setup.\nLearn a little about data types available to R.\nLearn about R packages, how to install and import them.\nLearn how to download and import CSV files using the readr package.\nIntroduce the Tibble/Data Frame.\nString functions together with the pipe: |&gt; (or %&gt;%).\nLearn how to modify data types (date) and select() columns.\n\nWe‚Äôll be exploring the Billboard Hot 100 charts along the way. Eventually you‚Äôll find the answers to a bunch of questions in this data and write about it."
  },
  {
    "objectID": "counts-import.html#basic-steps-of-this-lesson",
    "href": "counts-import.html#basic-steps-of-this-lesson",
    "title": "3¬† Summarize: count - import",
    "section": "3.2 Basic steps of this lesson",
    "text": "3.2 Basic steps of this lesson\nBefore we get into our storytelling, we have to set up our project, get our data and make sure it is in good shape for analysis. This is pretty standard for any new project. Here are the major steps we‚Äôll cover in detail for this lesson (and many more to come):\n\nCreate your project structure\nFind the data and (usually) get it onto on your computer\nImport the data into your project\nClean up column names and data types\nExport cleaned data for later analysis\n\nSo this chapter is about collecting and cleaning data. We‚Äôll handle the analysis separately in another chapter. I usually break up projects this way (cleaning vs analyzing) to stay organized and to avoid repeating the same cleaning steps."
  },
  {
    "objectID": "counts-import.html#create-a-new-project",
    "href": "counts-import.html#create-a-new-project",
    "title": "3¬† Summarize: count - import",
    "section": "3.3 Create a new project",
    "text": "3.3 Create a new project\nYou‚Äôve done this once already in Chapter 2 so you got this! But here are the basic steps for you to follow:\n\nLaunch RStudio\nMake sure you don‚Äôt have an existing project open. Use File &gt; Close project if you do.\nUse the +R button to create a New Project in a New Directory.\nName the project yourfirstname-billboard and put it in your ~/Documents/rwd folder.\nUse the + button and use R Notebook to start a new notebook.\nChange the title to ‚ÄúBillboard Hot 100 Import‚Äù.\nDelete the other boilerplate text.\nSave the file as 01-cleaning.Rmd.\n\nWe named this notebook starting with 01- because we will eventually end up with multiple notebooks that depend on each other and we will need to know the order to run them in the future.\n\n3.3.1 Describe the goals of the notebook\nWe‚Äôll add our first bit of R Markdown just after the meta data to explain what we are doing. Add this text to your notebook:\n## Goals of this notebook\n\nSteps to prepare our data:\n\n- Download the data\n- Import into R\n- Clean up data types and columns\n- Export for next notebook\nWe want to start each notebook with a list like this so our future selves and others know what the heck we are trying to accomplish.\nWe will also write R Markdown like this for each new ‚Äúsection‚Äù or goal in the notebook.\n\n\n3.3.2 The R Package environment\nWe have to back up from the step-by-step nature of this lesson and talk a little about the R programming language.\nR is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful.\nWhat happens is developers will find it difficult to do a certain task, so they will write an R ‚ÄúPackage‚Äù of code that helps them with that task. They share that code with the community, and suddenly the R garage has an ‚Äúultimate set of tools‚Äù that would make Spicoli‚Äôs dad proud.\nOne set of these tools is the tidyverse developed by Hadley Wickham and his team at RStudio. It‚Äôs a set of R packages for data science that work together in similar ways. Prof.¬†Lukito and I are worshipers of the tidyverse worldview and we‚Äôll use these tools extensively. While not required reading, I highly recommend Wickham‚Äôs book R for data science, which is free.\nThere are also a series of useful tidyverse cheatsheets that can help you as you use the packages and functions from the tidyverse. We‚Äôll refer to these throughout the course.\n\n\n3.3.3 Installing and using packages\nThere are two steps to using an R package:\n\nInstall the package onto your computer by using install.packages(\"package_name\"). You only have to do this once for each computer, so I usually do it using the R Console instead of in an R Notebook.\nInclude the library using library(package_name). This has to be done for each R Notebook or script that uses it, so it is usually one of the first things you‚Äôll see in a notebook.\n\n\nNote that you use ‚Äúquotes‚Äù around the package name when you are installing, but you DON‚ÄôT need quotes when you load the library.\n\nWe will install several packages to use in this project. To do this, we will use the Console, which we haven‚Äôt talked about much yet.\n\n\n\nThe Console and Terminal\n\n\n\nUse the image above to orient yourself to the R Console and Terminal.\nIn the Console, type in:\n\ninstall.packages(\"tidyverse\")\nAs you type into the Console, you‚Äôll see some type-assist hints on what you need. You can use the arrow keys to select one and hit the tab key to complete that command, then enter the values you need. If it asks you to install ‚Äúfrom source‚Äù, type Yes and hit return.\nYou‚Äôll see a bunch of response fly by in the Console. It‚Äôs probably all fine unless it ends the last response with an error.\nWhen you install the ‚Äútidyverse‚Äù package you actually get eight individual packages. You‚Äôll see references to ‚Äúdplyr‚Äù and even some functions referenced as dplyr::filter() but they are all part of the tidyverse. You can explore the list at tidyverse.org but know there are other packages from the same folks that we use and load separately.\n\nWe need two other packages as well, so also do:\n\ninstall.packages(\"janitor\")\ninstall.packages(\"lubridate\")\nWe‚Äôll use janitor to clean up our data column names, among other things. A good reference to learn more is the janitor vignette.\nWe‚Äôll use lubridate to fix some dates, which are a special complicated thing in programming. While Lubridate is part of the tidyverse universe, we have to install and load it separately.\nYou only have to install the packages once on your computer (though you have to load the libraries into each new notebook, which is next).\n\n\n3.3.4 Load the libraries\nNext, we‚Äôre going to tell our R Notebook to actually use these three libraries we‚Äôve just installed.\n\nIn your notebook, after the metadata at the top, use Cmd+option+i to insert an R code chunk.\nIn that chunk, type in the three libraries and run the code block with Cmd+Shift+Return.\n\nThis is the code you need:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\n\nYour output will look something like this:\n\n\n\nLibraries imported\n\n\nDon‚Äôt sweat the Conflicts listed there. It‚Äôs all good.\n\n\n3.3.5 Create a directory for your data\nNext we are going to create some folders in our project folder. We‚Äôll use some standard names for consistency.\nThe first folder is called data-raw. We are creating this folder because we want to keep a pristine version of our original data that we never change or overwrite. This is a core data journalism commandment: Thou shalt not change original data.\nIn your Files pane at the bottom-right of Rstudio, there is a New Folder icon.\n\nClick on the New Folder icon.\nName your new folder data-raw. This is where we‚Äôll put raw data that we don‚Äôt want to overwrite.\nAlso create another new folder called data-processed. This is were we will write data. We separate them to help avoid overwriting our raw data.\n\nOnce you‚Äôve done that, they should show up in the file explorer in the Files pane. Click the refresh button if you don‚Äôt see them. (The circlish thing at top right of the screenshot below. You might have to widen the pane to see it.)\n\n\n\nDirectory made\n\n\nYour .Rproj file name is likely a different name (and that is OK) and you can ignore the .gitignore I have there.\n\n\n3.3.6 About data sources\nDepending on the data source, importing can be brilliantly easy or a major pain in the rear. It all depends on how well-formatted the data is.\nIn this class, we will primarily use data stored as CSVs (Comma Separated Value), Excel files and APIs (Application Programming Interface).\n\nCSVs are a kind of lowest-common-denominator for data. Most any database or program can import or export them. It‚Äôs just text with a , or some other separator between each value.\nExcel files are good, but can get messy when humans get involved. They often have multiple header rows, columns used in multiple ways, notes added, etc. Just know you might have to clean them up before or after importing them.\nAPIs are systems designed to respond to programming. In the data world, we often use the APIs by writing a query to ask a system to return a selection of data. By definition, the data is well structured. You can often determine the file type of the output as part of the API call, including ‚Ä¶\nJSON (or JavaScript Object Notation) is the data format preferred by JavaScript. R can read it, too. It is often the output format of APIs, and prevalent enough that you need to understand how it works. We‚Äôll get into that later in semester.\n\nDon‚Äôt get me wrong ‚Ä¶ there are plenty of other data types and connections available through R, but those are the ones we‚Äôll deal with most in this book.\nWhile our data may be found on the internet in a data portal or through an API, we typically want to download a copy of it to our own machine for safe keeping. We will do that here, but let‚Äôs talk about the data first."
  },
  {
    "objectID": "counts-import.html#the-billboard-hot-100",
    "href": "counts-import.html#the-billboard-hot-100",
    "title": "3¬† Summarize: count - import",
    "section": "3.4 The Billboard Hot 100",
    "text": "3.4 The Billboard Hot 100\nThe Billboard Hot 100 singles charts has been the music industry‚Äôs standard record chart since its inception on Aug.¬†4th, 1958. The rankings, published by Billboard Media, are currently based on sales (physical and digital), radio play, and online streaming. The methods and policies of the chart have changed over time.\nThe data we will use was compiled by Prof.¬†McDonald from a number of sources. When you write about this data (and you will), you should source it as the Billboard Hot 100 from Billboard Media, since that is where it originally came from and they are the ‚Äúowner‚Äù of the data.\n\n3.4.1 Data dictionary\nTake a look at the current chart. Our data contains many (but not quite all) of the elements you see there. Each row of data (or observation as they are known in R) represents a song and the corresponding position on that week‚Äôs chart. Included in each row are the following columns (a.k.a. variables):\n\nCHART DATE: The release date of the chart\nTHIS WEEK: The current ranking as of the chart date\nTITLE: The song title\nPERFORMER: The performer of the song\nLAST WEEK: The ranking on the previous week‚Äôs chart\nPEAK POS.: The peak rank the song has reached as of the chart date\nWKS ON CHART: The number of weeks the song has appeared as of the chart date\n\n\n\n3.4.2 Let‚Äôs download our data\nSince this data is stored as a CSV file on Github, we could import it directly into our project from there, but there are benefits to downloading the file to your computer first. The hosted file could change later in ways you can‚Äôt control, and you would need access to the file the next time you ran your notebook. If you download the file to keep your own copy, you have more control of your future.\nSince this is a new ‚Äúsection‚Äù of our R Notebook, we‚Äôll note what we are doing and why in Markdown.\n\nAdd a Markdown headline ## Downloading data and some text explaining you are downloading data. Add a note about where the data comes from and include a link to the original source. (These are notes to your future self to explain what you are doing.)\nCreate an R chunk and copy/paste the following inside it. (hint: use the copy icon at the top right):\n\ndownload.file(\"https://github.com/utdata/rwdir/blob/main/data-raw/hot100_assignment.csv?raw=true\", \"data-raw/hot100_archive.csv\")\nThis download.file function takes at least two arguments:\n\nThe URL of the file you are downloading.\nThe path and name of where you want to save the resulting file.\n\nNote those two arguments are in quotes. The path includes the folder and file name as it relates to where your R Notebook is. In this case we need to go inside the data-raw/ folder and then name the file hot100_archive.csv.\nWhen you run this, it should save the file and then give you output similar to this:\ntrying URL 'https://github.com/utdata/rwdir/blob/main/data-raw/hot100_assignment.csv?raw=true'\nContent type 'text/plain; charset=utf-8' length 17960724 bytes (17.1 MB)\n==================================================\ndownloaded 17.1 MB\nThat‚Äôs not a small file at 17 MB and 300,000 rows, but it‚Äôs not a huge one, either.\n\n\n3.4.3 Consider commenting the download\nNow that we‚Äôve downloaded the data to our computer, we don‚Äôt need to run this line of code again unless we know our data source has updated. We can ‚Äúcomment‚Äù the code to preserve it but keep it from running again if we re-run our notebook (and we will).\n\nPut a # at the beginning of the line with the download.file() function.\nIn Markdown above it, add a note to your future self that you commented the download for now.\n\nAdding comments in programming is a common thing and every programming language has a way to do it. It is a way for the programmer to write notes to their future self, colleagues or ‚Äî like in this case ‚Äî comment out some code that you want to keep, but don‚Äôt want to execute when the program is run.\nWhen using R Notebooks we write most of our ‚Äúexplaining‚Äù text outside of code chunks using a syntax called R Markdown. R Markdown is designed to be readable as written, but it is given pretty formatting when ‚Äúprinted‚Äù as a PDF or HTML file. 1\nBut, sometimes it makes more sense to explain something right where the code is being executed. Inside R code chunks, anything following one or more hashes # inside a code chunk will be a comment. The text is there in the chunk, but it won‚Äôt be executed as code.\n(Yes, this is a bit confusing since #‚Äôs are used for comments in code chunks but headlines in the R Markdown parts of the file.)"
  },
  {
    "objectID": "counts-import.html#import-the-data",
    "href": "counts-import.html#import-the-data",
    "title": "3¬† Summarize: count - import",
    "section": "3.5 Import the data",
    "text": "3.5 Import the data\nNow that we have the data on our computer, let‚Äôs import it into our notebook so we can see it.\nSince we are doing a new thing, we should note that with a Markdown headline and text.\n\nAdd a Markdown headline: ## Import data\nAdd some text to explain that we are importing the Billboard Hot 100 data.\nAfter your description, add a new code chunk (Cmd+Option+i).\n\nWe‚Äôll be using the read_csv() function from the tidyverse readr package, which is different from read.csv that comes with base R. read_csv() is mo betta.\nInside the function we put in the path to our data, inside quotes. If you start typing in that path and hit tab, it will complete the path. (Easier to show than explain).\n\nAdd the following code into your chunk and run it.\n\nread_csv(\"data-raw/hot100_archive.csv\")\n\nNote the path to the file is in quotes.\n\nYou get two results printed to your notebook.\nThe first result called ‚ÄúR Console‚Äù shows what columns were imported and the data types. It‚Äôs important to review these to make sure things happened the way that expected. In this case it noted which columns came in as text (chr), or numbers (dbl). The red colored text in this output is NOT an indication of a problem.\n\n\n\nRConsole output\n\n\nThe second result spec_tbl_df prints out the data like a table. The data object is a tibble, which is a fancy tidyverse version of a ‚Äúdata frame‚Äù.\n\nI will use the term tibble and data frame interchangably. Think of tibbles and data frames like a well-structured spreadsheet. They are organized rows of data (called observations) with columns (called variables) where every column is a specific data type.\n\n\n\n\nData output\n\n\nWhen we look at the data output in RStudio, there are several things to note:\n\nBelow each column name is an indication of the data type. This is important.\nYou can use the arrow icon on the right to page through the additional columns.\nYou can use the paging numbers and controls at the bottom to page through the rows of data.\nThe number of rows and columns is displayed.\n\nAt this point we have only printed this data to the screen. We have not saved it in any way, but that is next."
  },
  {
    "objectID": "counts-import.html#assign-our-import-to-an-r-object",
    "href": "counts-import.html#assign-our-import-to-an-r-object",
    "title": "3¬† Summarize: count - import",
    "section": "3.6 Assign our import to an R object",
    "text": "3.6 Assign our import to an R object\nNow that we know how to find our data, we next need to assign it to an R object so it can be named thing we can reuse. We don‚Äôt want to re-import the data over and over each time we need it.\nThe syntax to create an object in R can seem weird at first, but the convention is to name the object first, then insert stuff into it. So, to create an object, the structure is this:\n# this is pseudo code. Don't put it in your notebook.\nnew_object &lt;- stuff_going_into_object\nThink of it like this: You must have a bucket before you can fill it with water. We ‚Äúname‚Äù the bucket, then fill it with data. That bucket is then saved into our ‚Äúenvironment‚Äù, meaning it is in memory where we can access it easily by calling its name.\nLet‚Äôs make a object called hot100 and fill it with our imported tibble.\n\nEdit your existing code chunk to look like this. You can add the &lt;- by using Option+- as in holding down the Option key and then pressing the hyphen:\n\n\nhot100 &lt;- read_csv(\"data-raw/hot100_archive.csv\")\n\nRun that chunk and several things happen:\n\nWe no longer see the result of the data in the notebook because we created an object instead of printing it.\nIn the Environment tab at the top-right of RStudio, you‚Äôll see the hot100 object listed.\n\nClick on the blue play button next to hot100 and it will expand to show you a summary of the columns.\nClick on the name hot100 and it will open a ‚ÄúView‚Äù of the data in another window, so you can look at it in spreadsheet form. You can even sort and filter it.\n\nOnce you‚Äôve looked at the data, close the data view with the little x next to the tab name.\n\n\n3.6.1 Print a peek to your R Notebook\nSince we can‚Äôt see the data after we assign it, let‚Äôs print the object to our notebook so we can refer to it.\n\nEdit your import chunk to add the last two lines of this, including #:\n\n# create the object, then fill it with data from the csv\nhot100 &lt;- read_csv(\"data-raw/hot100_archive.csv\")\n\n# peek at the data\nhot100\n\nYou can use the green play button at the right of the chunk, or preferrably have your cursor inside the chunk and do Cmd+Shift+Return to run all lines. (Cmd+Return runs only the current line.)\n\n\n\n3.6.2 Glimpse the data\nThere is another way to peek at the data that I prefer because it is more compact and shows you all the columns and data examples without scrolling: glimpse().\n\nIn your existing chunk, edit the last line to add the glimpse() function as noted below.\n\nI‚Äôm showing the return here as well. Afterward I‚Äôll explain the pipe: |&gt;.\nhot100 &lt;- read_csv(\"data-raw/hot100_archive.csv\")\n\n# peek at the data\nhot100 |&gt; glimpse()\n\n\nRows: 336,100\nColumns: 7\n$ `CHART WEEK`   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/20‚Ä¶\n$ `THIS WEEK`    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ TITLE          &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The ‚Ä¶\n$ PERFORMER      &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives‚Ä¶\n$ `LAST WEEK`    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 1‚Ä¶\n$ `PEAK POS.`    &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12‚Ä¶\n$ `WKS ON CHART` &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, ‚Ä¶\n\n\nThe glimpse shows there are 300,000+ rows and 7 columns in our data. Each column is then listed out with its data type and the first several values in that column.\n\n\n3.6.3 About the pipe |&gt;\nWe need to break down this code a little: hot100 |&gt; glimpse().\nWe are starting with the object hot100, but then we follow it with |&gt;, which is called a pipe. The pipe is concept that takes the result of an object or function and passes it into another function. Think of it like a sentence that says ‚ÄúAND THEN‚Äù the next thing.\nIt might look like there are no arguments inside glimpse(), but what we are actually doing is passing the hot100 tibble into it like this: glimpse(hot100). For almost every function in R the first argument is ‚Äúwhat data are you taking about?‚Äù The pipe allows us to say ‚Äúhey, take the data we just mucked with (i.e., the code before the pipe) and use that in this function.‚Äù\nYou can‚Äôt start a new line with a pipe. If you are breaking your code into multiple lines, then the |&gt; needs to be at the end of a line and the next line should be indented so there is a visual clue it is related to line above it, like this:\nhot100 |&gt; \n  glimpse()\n\nThere is a keyboard command for the pipe |&gt;: Cmd+Shift+m. Learn that one!\n\n\n\nA rabbit dives into a pipe\nThe concept of the pipe was first introduced by tidyverse developers in 2014 in a package called magrittr. They used the symbol %&gt;% as the pipe. It was so well received the concept was written directly into base R in 2021, but using the symbol |&gt;. Hadley Wickham‚Äôs 2022 rewriting of R for Data Science uses the base R pipe |&gt; by default. You can configure which to use in RStudio.\n(This switch to |&gt; is quite recent and this book is being rewritten to migrate from %&gt;%, but it might take a while to catch all references, especially in images. For now, assume |&gt; and %&gt;% are interchangeable. Also note there is A LOT of code in the wild using the magrittr pipe %&gt;%, so you‚Äôll find many references on Stack Overflow and elsewhere.)"
  },
  {
    "objectID": "counts-import.html#cleaning-data",
    "href": "counts-import.html#cleaning-data",
    "title": "3¬† Summarize: count - import",
    "section": "3.7 Cleaning data",
    "text": "3.7 Cleaning data\nData is dirty. Usually because a human was involved at some point, we humans are fallible.\nData problems are often revealed when importing so it is good practice to check for problems and fix them right away. We‚Äôll face some of those challenges in this project, but we should talk about what is good vs dirty data.\nThis ‚ÄúChecking Your Data‚Äù section of this DataCamp tutorial has a good outline of what makes good data, but in general it should:\n\nHave a single header row with well-formed column names.\n\nOne column name for each column. No merged cells.\nShort names are better than long ones.\nSpaces in names make them harder to work with. Use and _ or . between words. I prefer _ and lowercase text.\n\nRemove notes or comments from the files.\nEach column should have the same kind of data: numbers vs words, etc.\n\n\n3.7.1 Cleaning column names\nSo, given those notes above, we should clean up our column names. This is why we have included the janitor package, which includes a neat function called clean_names()\n\nEdit the first line of your chunk to add a pipe and the clean_names function: %&gt;% clean_names()\n\n\nhot100 &lt;- read_csv(\"data-raw/hot100_assignment.csv\") %&gt;% clean_names()\n\n# peek at the data\nhot100 |&gt; glimpse()\n\nRows: 336,100\nColumns: 7\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022‚Ä¶\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch‚Ä¶\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",‚Ä¶\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,‚Ä¶\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, ‚Ä¶\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49‚Ä¶\n\n\nThis function has cleaned up your names, making them all lowercase and using _ instead of periods between words. Believe me when I say this is helpful when you are writing code. It makes type-assist work better and you can now double-click on a column name to select all of it and copy and paste somewhere else. When you have spaces or dashes in an object you can‚Äôt double-click on it to select all of it.\n\n\n3.7.2 Fixing the date\nDates in programming are a tricky data type because they are represented essentially as the number of seconds before/after January 1, 1970. Yes, that‚Äôs crazy, but it is also cool because that allows us to do math on them. So, to use our chart_date properly in R we need to convert it from the text into a real date datatype. (If you wish, you can read more about why dates are tough in programming.)\nConverting text into dates can be challenging, but the tidyverse universe has a package called lubridate to ease the friction. (Get it?).\nSince we are doing something new, we want to start a new section in our notebook and explain what we are doing.\n\nIn Markdown add a headline: ## Fix our dates.\nAdd some text that you are using lubridate to create a new column with a real date.\nAdd a new code chunk. Remember Cmd+Option+i will do that.\n\nWe will be changing or creating our data, so we will create a new object to store it in. We do this so we can go back and reference the unchanged data if we need to. Because of this, we‚Äôll set up a chunk of code that allows us to peek at what is happening while we write our code. We‚Äôll do this kind of setup often when we are working out how to do something in code.\n\nAdd the following inside your code chunk.\n\n\n# part we will build upon\nhot100_date &lt;- hot100\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 336,100\nColumns: 7\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022‚Ä¶\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch‚Ä¶\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",‚Ä¶\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,‚Ä¶\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, ‚Ä¶\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49‚Ä¶\n\n\nLet‚Äôs break this down:\n\nI have a comment starting with # to explain the first part of the code.\nWe created a new ‚Äúbucket‚Äù called hot100_date and we fill it with our hot100 data. Yes, as of now they are exactly the same.\nI have a blank line for clarity.\nThen another comment.\nThen we glimpse the new hot100_date object so we can see changes as we work on it.\n\n\nTo be clear, we haven‚Äôt changed any data yet. We just created a new object like the old object.\n\n\n3.7.2.1 Working with mutate()\nWe are going to use the text of our date field chart_date to create a new converted date. We will use the dplyr function mutate() to do this, with some help from lubridate.\n\ndplyr is the tidyverse package of functions to manipulate data. We‚Äôll use functions from it a lot. It is loaded with the library(tidyverse) so you don‚Äôt have to load it separately.\n\nLet‚Äôs explain how mutate works first: Mutate changes every value in a column. You can either create a new column or overwrite an existing one.\nWithin the mutate function, we name the new thing first (the bucket!) and then fill it with the new value.\n# this is psuedo code. You don't have to include or run it.\ndata |&gt; \n  mutate(\n    newcol = new_stuff_from_math_or_whatever\n  )\nThat new value could be arrived at through math or any combination of other functions. In our case, we want to convert our old text-based date to a real date, and then assign it back to the ‚Äúnew‚Äù column.\n\nA note about my pseudo code above: I strategically used returns to make the code more readable. This code would work the same if it were all on the same line, but writing it this way helps me understand it. RStudio will help you indent properly this as you type. (Easier to show than explain.)\n\n\nEdit your chunk to add the changes below and run it. I implore you to type the changes so you see how RStudio helps you write it. Use tab completion, etc.\n\n\n# part we will build upon\nhot100_date &lt;- hot100 |&gt; \n  mutate(\n    chart_date = mdy(chart_week)\n  )\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 336,100\nColumns: 8\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022‚Ä¶\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch‚Ä¶\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",‚Ä¶\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,‚Ä¶\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, ‚Ä¶\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49‚Ä¶\n$ chart_date   &lt;date&gt; 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-‚Ä¶\n\n\nAgain, let‚Äôs break down what we‚Äôve done:\n\nAt the end of the first line, we added the pipe |&gt; because we are taking our hot100 data AND THEN we will mutate it.\nNext, we start the mutate() function. If you use type assist and tab completion to type this in, your cursor will end up in the middle of the parenthesis. This allows you to then hit your Return key to split it into multiple lines with proper indenting. We do this so we can more clearly see what inside the mutate ‚Ä¶ where the real action is going on here. It‚Äôs also possible to make multiple changes within the same mutate, and putting each one on their own line makes that more clear.\nInside the mutate, we first name our new column chart_date and then we set that equal to mdy(chart_week), which is explained next.\n\nThe mdy() function is part of the lubridate package. Lubridate allows us to parse text and then turn it into a real date if we tell it the order of the date values in the original data.\n\nOur original date was something like ‚Äú7/17/1965‚Äù. That is month, followed by day, followed by year.\nWe use the lubridate function mdy() to say ‚Äúthat‚Äôs the order this text is in, now please convert this into a real date‚Äù, which properly shows as YYYY-MM-DD. Lubridate is smart enough to figure out if you have / or - between your values in the original text.\n\nIf your original text is in a different date order, then you look up what lubridate function you need to convert it. I typically use the cheatsheet in the lubridate documentation. You‚Äôll find them in the PARSE DATE-TIMES section.\n\n\n3.7.2.2 Check the result!\nThis new chart_date column is added as the LAST column of our data. After doing any kind of mutate you want to check the result to make sure you got the results you expected, which is why we didn‚Äôt just overwire the original chart_week column. That‚Äôs also why we built our code this way with glimpse() so we can see example of our data from both the first and the last column. (We‚Äôll rearrange all the columns in a bit once we are done cleaning everything.)\nCheck your glimpse returns ‚Ä¶ did your dates convert correctly?"
  },
  {
    "objectID": "counts-import.html#arrange-the-data",
    "href": "counts-import.html#arrange-the-data",
    "title": "3¬† Summarize: count - import",
    "section": "3.8 Arrange the data",
    "text": "3.8 Arrange the data\nJust to be tidy, we want ensure our data is arranged to start with the oldest week and ‚Äútop‚Äù of the chart and then work forward through time and rank.\ni.e., let‚Äôs arrange this data so that the oldest data is at the top.\nSorting data is not a particularly difficult concept to grasp, but it is one of the Basic Data Journalism Functions, so watch this video:\n\n\n\nIn R, the sorting function we use is called arrange().\nWe‚Äôll build upon our existing code and use the pipe |&gt; to push it into an arrange() function. Inside arrange we‚Äôll feed it the columns we wish to sort by.\n\nEdit your chunk to the following to add the arrange() function:\n\n\n# part we will build upon\nhot100_date &lt;- hot100 |&gt; \n  mutate(\n    chart_date = mdy(chart_week)\n  ) |&gt; \n  arrange(chart_date, this_week)\n\n# peek at the result\nhot100_date |&gt; glimpse()\n\nRows: 336,100\nColumns: 8\n$ chart_week   &lt;chr&gt; \"8/4/1958\", \"8/4/1958\", \"8/4/1958\", \"8/4/1958\", \"8/4/1958‚Ä¶\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ title        &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard He‚Ä¶\n$ performer    &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby D‚Ä¶\n$ last_week    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ wks_on_chart &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ chart_date   &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-‚Ä¶\n\n\nNow when you look at the glimpse, the first record in the chart_date column is from ‚Äú1958-08-04‚Äù and the first in the this_week is ‚Äú1‚Äù, which is the top of the chart.\nJust to see this all clearly in table form, we‚Äôll print the top of the table to our screen so we can see it.\n\nAdd a line of Markdown text in your notebook explaining your are looking at the table.\nAdd a new code chunk and add the following.\n\n\nIn case you haven‚Äôt noticed yet, the code printout in your notebook is a lot cooler than in this online book.\n\n\nhot100_date |&gt; head(10)\n\n# A tibble: 10 √ó 8\n   chart_week this_week title          performer last_week peak_pos wks_on_chart\n   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1 8/4/1958           1 Poor Little F‚Ä¶ Ricky Ne‚Ä¶        NA        1            1\n 2 8/4/1958           2 Patricia       Perez Pr‚Ä¶        NA        2            1\n 3 8/4/1958           3 Splish Splash  Bobby Da‚Ä¶        NA        3            1\n 4 8/4/1958           4 Hard Headed W‚Ä¶ Elvis Pr‚Ä¶        NA        4            1\n 5 8/4/1958           5 When           Kalin Tw‚Ä¶        NA        5            1\n 6 8/4/1958           6 Rebel-'rouser  Duane Ed‚Ä¶        NA        6            1\n 7 8/4/1958           7 Yakety Yak     The Coas‚Ä¶        NA        7            1\n 8 8/4/1958           8 My True Love   Jack Sco‚Ä¶        NA        8            1\n 9 8/4/1958           9 Willie And Th‚Ä¶ The John‚Ä¶        NA        9            1\n10 8/4/1958          10 Fever          Peggy Lee        NA       10            1\n# ‚Ñπ 1 more variable: chart_date &lt;date&gt;\n\n\nThis just prints the first 10 lines of the data.\n\nUse the arrows to look at the other columns of the data (which you can‚Äôt see in the book).\n\n\nNote: It‚Äôs OK that the last_week columns has ‚ÄúNA‚Äù for those first rows because this is the first week ever for the chart. There was no last_week.\n\n\n3.8.1 Getting summary stats\nPrinting your data to the notebook can only tell you so much. Yes, you can arrange by different columns to see the maximum and minimum values, but it‚Äôs hard to get an overall sense of your data that way when there is 300,000 rows like we have here. Luckily there is a nice function called summary() that is similar to glimpse() that gives you some summary statistics for each columns.\n\nAdd some Markdown text that you‚Äôll print summary stats of your data.\nAdd a new R chunk and put the following in and run it\n\n\nhot100_date |&gt; summary()\n\n  chart_week          this_week        title            performer        \n Length:336100      Min.   :  1.0   Length:336100      Length:336100     \n Class :character   1st Qu.: 26.0   Class :character   Class :character  \n Mode  :character   Median : 51.0   Mode  :character   Mode  :character  \n                    Mean   : 50.5                                        \n                    3rd Qu.: 75.0                                        \n                    Max.   :100.0                                        \n                                                                         \n   last_week         peak_pos       wks_on_chart     chart_date        \n Min.   :  0.00   Min.   :  1.00   Min.   : 1.00   Min.   :1958-08-04  \n 1st Qu.: 23.00   1st Qu.: 13.00   1st Qu.: 4.00   1st Qu.:1974-09-14  \n Median : 47.00   Median : 38.00   Median : 7.00   Median :1990-10-20  \n Mean   : 47.44   Mean   : 40.83   Mean   : 9.23   Mean   :1990-10-19  \n 3rd Qu.: 71.00   3rd Qu.: 65.00   3rd Qu.:13.00   3rd Qu.:2006-11-25  \n Max.   :100.00   Max.   :100.00   Max.   :91.00   Max.   :2022-12-31  \n NA's   :32460                                                         \n\n\nThese summary statistics can be informative for us. It is probably the easiest way to check what the newest and oldest dates are in your data (see the Min. and Max. returns for chart_date). You get an average (mean) and median for each number, too. You might notice potential problems in your data, like if we had a this_week number higher than ‚Äú100‚Äù (we don‚Äôt)."
  },
  {
    "objectID": "counts-import.html#selecting-columns",
    "href": "counts-import.html#selecting-columns",
    "title": "3¬† Summarize: count - import",
    "section": "3.9 Selecting columns",
    "text": "3.9 Selecting columns\nNow that we have the fixed date column, we don‚Äôt need the old chart_week version that is text. We‚Äôll use this opportunity to discuss select(), which is another concept in our Basic Data Journalism Functions series, so watch this:\n\n\n\nIn R, the workhorse of the select concept is the function called ‚Äî you guessed it ‚Äî select(). In short, the function allows us to choose a subset of our columns. We can either list the ones we want to keep or the ones we don‚Äôt want.\nLike a lot of things in R, select() is really easy at its most basic level: List the columns you want to keep. But select() can also be very powerful as you learn more options. There are a bunch of selection helpers like starts_with() and ends_with() and everything(). You can choose ranges with numbers 1:3 or column names col_name1:col_name9, or a combination c(col_nam1, col_name3). You can specify what NOT to keep with the negate symbol ! like this !c(col_nam1, col_name3). You can even rename columns as you select them, which we‚Äôll do below.\nIn our case we will select the columns want to keep, and we‚Äôll rename some as we do.\n\nAdd a Markdown headline: ## Selecting columns.\nExplain in text we are dropping the text date column and renaming others.\nAdd the code below and then I‚Äôll explain it. We again are setting this up to create a new object and view the changes.\n\n\nhot100_clean &lt;- hot100_date |&gt; \n  select(\n    chart_date,\n    # this renames a column with new name first\n    current_rank = this_week,\n    title,\n    performer,\n    previous_rank = last_week,\n    peak_rank = peak_pos,\n    wks_on_chart\n  )\n\nhot100_clean |&gt; glimpse()\n\nRows: 336,100\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08‚Ä¶\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H‚Ä¶\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby ‚Ä¶\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n\n\nLine by line, it goes like this:\n\nName our new object hot100_clean and fill it with the result of hot100_date and then the modifications that follow.\nWe use the select() function to choose our columns to keep, renaming some of them along the way:\n\nWe start with chart_date which is our real date. We just won‚Äôt ever name the text version so we won‚Äôt keep it.\nWhen we get to our this_week column, we rename it to current_rank. We‚Äôre doing this because we‚Äôll rename all the ‚Äúranking‚Äù columns with something that includes _rank at the end. We‚Äôre being (too) clever here so later we can use select(ends_with(\"_rank\")) to get those ranking columns together. Note the R concept of naming the new thing first before filling it.\nFor title and performer we just list them because we want them to come next but we don‚Äôt need to change the names at all.\nFor the other columns we rename or not as needed.\n\nLastly, we glimpse the new object to check it.\n\nThere are other ways to accomplish the same thing, but this works for us."
  },
  {
    "objectID": "counts-import.html#exporting-data",
    "href": "counts-import.html#exporting-data",
    "title": "3¬† Summarize: count - import",
    "section": "3.10 Exporting data",
    "text": "3.10 Exporting data\n\n3.10.1 Single-notebook philosophy\nI have a pretty strong opinion that you should be able to freshly open any R Notebook in your project and run it from top to bottom without it breaking. In short, one notebook should not be dependent on running another open notebook.\nThis is why I had you name this notebook 01-cleaning.Rmd with a 01 at the beginning. We‚Äôll number our notebooks in the order they should be run. It‚Äôs an indication that before we can use the notebook 02-analysis (next lesson!) that the 01-cleaning.Rmd notebook has to be run first.\n\nI use 01- instead of just 1- in case there are more than nine notebooks. I want them to appear in order in my files directory. I‚Äôm anal retentive like that, which is a good trait for a data journalist.\n\nSo we will create an exported file from our first notebook that can be used in the second one. Once we create that file, the second notebook can be opened and run at any time.\nWhy make this so complicated?\nThe answer is consistency. When you follow the same structure with each project, you quickly know how to dive into that project at a later date. If everyone on your team uses the same structure, you can dive into your teammates code because you already know how it is organized. If we separate our importing/cleaning into it‚Äôs own file to be used by many other notebooks, we can fix future cleaning problems in ONE place instead of many places.\nOne last example to belabor the point: It can save time. I‚Äôve had import/cleaning notebooks that took 20 minutes to process. Imagine if I had to run that every time I wanted to rebuild my analysis notebook. Instead, the import notebook spits out a clean file that can be imported in a fraction of that time.\nThis was all a long-winded way of saying we are going to export our data now.\n\n\n3.10.2 Exporting as rds\nWe are able to pass cleaned data between notebooks because of a native R data format called .rds. When we export in this format it saves not only rows and columns, but also the data types. (If we exported as CSV, we would potentially have to re-fix the date or other data types when we imported again.)\nWe will use another readr function called write_rds() to create our file to pass along to the next notebook, saving the data into the data-processed folder we created earlier. We are separating it from our data-raw folder because ‚ÄúThou shalt not change original data‚Äù even by accident. By always writing data to this different folder, we hopefully avoid accidentally overwriting our original data.\n\nCreate a Markdown headline ## Exports and write a description that you are exporting files to .rds.\nAdd a new code chunk and add the following code:\n\n\n\nhot100_clean |&gt; \n   write_rds(\"data-processed/01-hot100.rds\")\n\nSo, here we are starting with the hot100_clean tibble that we saved earlier. We then pipe |&gt; the result of that into a new function write_rds(). In addition to the data, the function needs to know where to save the file, so in quotes we give the path to where and what we want to call the file: \"data-processed/01-hot100.rds\".\nRemember, we are saving in data-processed because we never export into data-raw. We are naming the file starting with 01- to indicate to our future selves that this output came from our first notebook. We then name it, and use the .rds extension."
  },
  {
    "objectID": "counts-import.html#naming-chunks",
    "href": "counts-import.html#naming-chunks",
    "title": "3¬† Summarize: count - import",
    "section": "3.11 Naming chunks",
    "text": "3.11 Naming chunks\nI didn‚Äôt want to break our flow 2 of work to explain this earlier, but I want you to name all your chunks so you can use a nice feature in RStudio to jump up and down your notebook.\nLet me show you and example of why first. Look at the bottom of your window above the console and you‚Äôll see a dropdown window. Click on that.\nHere is mine, but yours will be different:\n\n\n\nRStudio bookmarks\n\n\nYou‚Äôll notice that my chunks have names, but yours probably don‚Äôt. It‚Äôs pretty helpful have these names so you know what the chunk does. You can use this menu to skip up and down the notebook.\nHow to name a chunk? Well, I can‚Äôt show you in code because it is not rendered in the book, but here is a picture:\n\n\n\nNamed chunks\n\n\nSee where I have {r download}? I named it that because that is what the chunk does.\n\nI suggest using a single word or - or _ between words.\nThere are other configurations we can do between the {} here, but that is for another day.\n\n\nGo back through your notebook and name all your chunks.\nUnder the Run menu, choose Restart R and run all chunks.\n\nMake sure that your Notebook runs all the way from top to bottom. The order of stuff in the notebook matters and you can make errors as you edit up and down the notebook. You always want to do this before you finish a notebook."
  },
  {
    "objectID": "counts-import.html#knit-your-page",
    "href": "counts-import.html#knit-your-page",
    "title": "3¬† Summarize: count - import",
    "section": "3.12 Knit your page",
    "text": "3.12 Knit your page\nLastly, I want you to Knit your notebook so you can see the pretty HTML version.\n\nNext to the Preview menu in the notebook tool bar, click the little dropdown to see the knitting options.\nChoose Knit to HTML.\n\n\n\n\nKnit to HTML\n\n\nAfter you do this, the menu will probably change to just Knit and you can just click on it to knit again.\nThis will open a nice reader-friendly version of your notebook. You could send that file (called 01-cleaning.html) to your editor and they could open it in a web browser.\n\nI use these knit files to publish my work on Github, but it is a bit more involved to do all that so we‚Äôll skip that for now."
  },
  {
    "objectID": "counts-import.html#review-of-what-weve-learned-so-far",
    "href": "counts-import.html#review-of-what-weve-learned-so-far",
    "title": "3¬† Summarize: count - import",
    "section": "3.13 Review of what we‚Äôve learned so far",
    "text": "3.13 Review of what we‚Äôve learned so far\nMost of this lesson has been about importing and cleaning data. Importing and cleaning data into R (or any data science program) can sometimes be quite challenging, depending on the circumstances. Here we were working with well-formed data, but we still used quite a few tools from the tidyverse like readr (read_csv, write_rds) and dplyr (select, mutate).\nHere are the functions we used and what they do. Most are linked to documentation sites:\n\ninstall.packages() downloads an R package to your computer. Typically executed from within the Console and only once per computer. We installed the tidyverse, janitor and lubridate packages.\nlibrary() loads a package. You need it for each package in each notebook, like library(tidyverse).\nread_csv() imports a csv file. You want that one, not read.csv.\nclean_names() is a function in the janitor package that standardizes column names.\nglimpse() is a view of your data where you can see all of the column names, their data type and a few examples of the data.\nhead() prints the first 6 rows of your data unless you specify a different integer within the function.\nmutate() changes data. You can create new columns or overwrite existing ones.\nmdy() is a lubridate function to convert text into a date. There are other functions for different date orders.\nselect() selects columns in your tibble. You can list all the columns to keep, or use - to remove columns. There are many variations.\nsummary() gives you some quick summary statistics about your data like min, max, mean, median.\nwrite_rds() writes data out to a file in a format that preserves data types."
  },
  {
    "objectID": "counts-import.html#whats-next",
    "href": "counts-import.html#whats-next",
    "title": "3¬† Summarize: count - import",
    "section": "3.14 What‚Äôs next",
    "text": "3.14 What‚Äôs next\nCleaning data is just the first step of exploring a data set. We‚Äôll work through the next chapter before we turn in any work on this.\nPlease reach out to me if you have questions on what you‚Äôve done so far. These are important skills you‚Äôll use on future projects."
  },
  {
    "objectID": "counts-import.html#footnotes",
    "href": "counts-import.html#footnotes",
    "title": "3¬† Summarize: count - import",
    "section": "",
    "text": "I‚Äôll sometimes just say ‚ÄúWrite in Markdown‚Äù since R Markdown is just a fancy version of the original markdown. Beware my rabbit holes.‚Ü©Ô∏é\nBreak the flow ‚Ä¶ I wrote that and laughed out loud at all the silly asides (like this one) that I throw into intricate documentation. Like if. You are welcome and I‚Äôm sorry.‚Ü©Ô∏é"
  },
  {
    "objectID": "counts-analysis.html#goals-of-this-lesson",
    "href": "counts-analysis.html#goals-of-this-lesson",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.1 Goals of this lesson",
    "text": "4.1 Goals of this lesson\n\nTo use the group_by/summarize/arrange combination to count rows of data.\nTo use filter in two ways: to focus data before summarizing, and to logically end summarized lists.\nWe‚Äôll also cover some more complex filters using and/or logic.\nIntroduce the shortcut count() function\n\n\nOF NOTE: I updated the data used in this book so some written word references to code results might be different than the actual data in the book or what you get on your computer. I‚Äôve tried to catch them all, but I‚Äôm a fallible human."
  },
  {
    "objectID": "counts-analysis.html#the-questions-well-answer",
    "href": "counts-analysis.html#the-questions-well-answer",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.2 The questions we‚Äôll answer",
    "text": "4.2 The questions we‚Äôll answer\nNow that we have the Billboard Hot 100 charts data in our project it‚Äôs time to find the answers to the following questions:\n\nWhich performer had the most appearances on the Hot 100 chart at any position?\nWhich title/performer combination has been on the charts the most number of weeks at any position?\nWhich title/performer combination was No.¬†1 for the most number of weeks?\nWhich performer had the most songs reach No.¬†1?\nWhich performer had the most songs reach No.¬†1 in the most recent five years?\nHow many appearances for a specific performer in each year?\nWhich performer had the most Top 10 hits overall?\n\n\nWhat are your guesses for the questions above? NO PEEKING!\n\nIn each case we‚Äôll talk over the logic of finding the answer and the code to accomplish it.\nBefore we can get into the analysis, we want to set up a new notebook to separate our cleaning from our analysis."
  },
  {
    "objectID": "counts-analysis.html#setting-up-an-analysis-notebook",
    "href": "counts-analysis.html#setting-up-an-analysis-notebook",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.3 Setting up an analysis notebook",
    "text": "4.3 Setting up an analysis notebook\nAt the end of the last notebook we exported our clean data as an .rds file. We‚Äôll now create a new notebook and import that data. It will be much easier this time.\n\nIf you don‚Äôt already have it open, go ahead and open your Billboard project.\nIf your import notebook is still open, go ahead and close it.\nUse the + menu to start a new R Notebook.\nUpdate the title as ‚ÄúBillboard analysis‚Äù and then remove all the boilerplate text below the YAML metadata.\nSave the file as 02-analysis.Rmd in your project folder.\nCheck your Environment tab (top right) and make sure the environment is empty. We don‚Äôt want to have any leftover data. If there is, then go under the Run menu and choose Restart R and Clear Output.\n\nSince we are starting a new notebook, we need to set up a few things. First up we want to list our goals.\n\nAdd a headline and text describing the goals of this notebook. You are exploring the Billboard Hot 100 charts data.\nGo ahead and copy all the questions we outlined above into your notebook.\nFormat those questions as a nice list. Start each line with a - or * followed by a space. There should be a blank line above and below the entire LIST but not between the items. List tiems should be on sequential lines ‚Ä¶ and it is the only markdown item like that.\nNow add a another headline (two hashes) called Setup.\nAdd a chunk, also name it ‚Äúsetup‚Äù and add the tidyverse and lubridate libraries.\nRun the chunk to load the library.\n\n\n4.3.1 Import the cleaned data\nWe need to import the data that we cleaned and exported in the last notebook. It‚Äôs just a couple of lines of code and you could write them all out and run it, but here is where I tell you for the first of a 1,000 times:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\nYes, sometimes for the sake of brevity I will give you multiple lines of code at once, but to understand what is actually going on you should add and run that code one line at a time.\nWe‚Äôll painstakingly walk through that process here to belabor the point. Here are our goals for this bit:\n\nDocument that we are importing clean data\nImport our cleaned data\nFill an R object with that data\nGlimpse the R Object so we can check it\n\nI want you to:\n\nStart a Markdown section with a headline noting you are importing the cleaned data. Add any other text notes you might need for yourself.\nAdd a code chunk and name it ‚Äúimport‚Äù.\nIn your code chunk, add this line of code:\n\nread_rds(\"data-processed/01-hot100.rds\")\nThis read_rds() function is just like the read_csv() function we used in our last notebook to read in data from our computer, except we are reading a different kind of file format: RDS. The argument (in quotes) is the path where our file is on our computer.\n\nRun that line of code.\n\nWhat did you expect to happen? It should print the results of the data in your notebook. Did it?\nIf you got an error, you should read that error output for some clues about what happened. When using a read_ function problems are usually about the path ‚Ä¶ that you have the folder structure wrong or something misspelled.\n\nCATCHING COMMON MISTAKES: This guide about common R mistakes is worth bookmarking and reading.\n\nOur next goal is to take all that data and put it into a new R object.\n\nEdit your one line of code to assign the data to an object hot100. Remember we will put the new object FIRST (our bucket) and then use the &lt;- operator to put the data inside it.\nRun it!\n\nIt should look like this:\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\nWhat happened when you ran the code? Did you get an error? Did you get a result printed to the screen?\nThat last one was kind of a trick question. When you save data into an object it does NOT print it to your notebook, but it does save that object into your ‚ÄúEnvironment‚Äù, which means it is in memory and can be called and run at any time. You should see hot100 listed in your Environment tab the top right of RStudio.\nNow, before I started this quest, I knew I wanted that data inside the hot100 object and could‚Äôve written it like that from the beginning. But I didn‚Äôt becuase most ‚Äúread‚Äù problems are in the path, so I wanted to make sure that function was written correctly first. Also, when I put the data into the object I can‚Äôt see it until I print it out again, so I do it one piece at at time. You should, too.\nSo next we‚Äôll print the data out to our screen. We do that by just by calling the object.\n\nEdit your code chunk to add a blank line then the object and run it, like this:\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100\nThis should print out the data to your screen so you can see the first couple of columns and the first 10 rows of data.\nBut what we really need is to see all the column names, the data types and an example of the data. We can get that with our glimpse() function.\n\nEdit the last line of the chunk to add the pipe and the glimpse() function\nRun it.\n\nYour code (and result) should look like this now:\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100 |&gt; glimpse()\n\nRows: 336,100\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08‚Ä¶\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H‚Ä¶\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby ‚Ä¶\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n\n\nAgain, I could‚Äôve written all that code at once before running it, as I‚Äôve written code like that for many years. I still write code one line at a time, and you should, too. It is the only way to make sure your code is correct. It‚Äôs easier to find problems. So, for the second of a 1,000 times:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\n\nIMPORTANT NOTE: If you are looking at my code and writing it into your notebook, you have to run each line of code BEFORE you add the pipe |&gt; at the end of that line. A pipe |&gt; must have a function directly following it on the same or next (or same) line to work."
  },
  {
    "objectID": "counts-analysis.html#introducing-dplyr",
    "href": "counts-analysis.html#introducing-dplyr",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.4 Introducing dplyr",
    "text": "4.4 Introducing dplyr\nOne of the packages within the tidyverse is dplyr. Dplyr allows us to transform our data frames in ways that let us explore the data and prepare it for visualizing. It‚Äôs the R equivalent of common Excel functions like sort, filter and pivoting.\n\nThere is a cheatsheet on the dplyr that you might find useful.\n\n\n\n\nImages courtesy Hadley. Yes, I personally asked him. OK, it was email, but still.\n\n\nWe‚Äôve used select(), mutate() and arrange() already, but we‚Äôll introduce more dplyr functions in this chapter.\nWe will use these dplyr functions to find the answers to our questions."
  },
  {
    "objectID": "counts-analysis.html#most-appearances",
    "href": "counts-analysis.html#most-appearances",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.5 Most appearances",
    "text": "4.5 Most appearances\nOur first question: Which performer had the most appearances on the Hot 100 chart at any position?\nLet‚Äôs work through the logic of what we need to do before I explain exactly how to do it.\nEach row in the data is one song ranked on the chart. It includes a field with the name of the ‚Äúperformer‚Äù so we know who recorded it.\nSo, to figure out how many times a performer is in the data, we need to count the number of rows with the same performer.\nWe‚Äôll use the tidyverse‚Äôs version of Group and Aggregate to get this answer. It is actually two different functions within dplyr that often work together: group_by() and summarize()\n\n4.5.1 Group & Aggregate\nBefore we dive into the code, let‚Äôs review this video about ‚ÄúGroup and Aggregate‚Äù to get a handle on the concept.\n\n\n\nWe‚Äôll dive deep into this next.\n\n\n4.5.2 Summarize\nWe‚Äôll start with summarize() first because it can stand alone.\nThe summarize() function computes summary tables describing your data. We‚Äôre usually finding a single number to describe a column of data, like the ‚Äúaverage‚Äù of numbers in column.\nIn our case we want a ‚Äúsummary‚Äù about the number of times a specific performer appears in data, hence we use summarize().\n\nTHEY BE THE ZAME: summarize() and summarise() are the same function, as R supports both the American and UK spelling of summarize. They work the same and I don‚Äôt care which you use.\n\nHere is an example of summarize() in a different context:\n\n\n\nLearn about your data with summarize()\n\n\nThe example above is giving us two summaries: It is applying a function mean() (or average) on all the values in the lifeExp column, and then again with min(), the lowest life expectancy in the data.\nMuch like the mutate() function we used earlier, within summarize() we list the name of the new column first, then assign to it the function and column we want to accomplish using =.\nAgain, in our case (as we work toward finding the performer with most appearances) we want to summarize the number of rows, and there is a function for that: n(). (Think ‚Äúnumber of observations‚Äù.) Every row in the data is an appearance ‚Ä¶ we just need to count how many rows have each performer.\nBut first, to show how this works, we‚Äôll count all the rows in our data. Let‚Äôs write the code and run it, then I‚Äôll explain:\n\nSet up a new section with a Markdown headline, text and explain you are looking for most appearances.\nAdd a named code chunk and add the following:\n\n\nhot100 |&gt; \n  summarize(\n    appearances = n()\n  )\n\n# A tibble: 1 √ó 1\n  appearances\n        &lt;int&gt;\n1      336100\n\n\n\nWe start with the tibble first and then pipe into summarize().\nWithin the function, we define our summary:\n\nWe name the new column ‚Äúappearances‚Äù because that is a descriptive column name for our result.\nWe set that new column to count the number of rows.\n\n\nBasically we are summarizing the total number of rows in the data. Through 2021 there were 330,800 rows.\n\nAN ASIDE: Like I did earlier with mutate, I often break up the arguments inside summarize() into new lines so they are easier to read, like above.\n\nBut I bet you‚Äôre asking: Professor, we want to count the number of times an performer has appeared, right?\nThis is where we bring in a close friend to summarize() ‚Ä¶ the group_by() function.\n\n\n4.5.3 Group by\nThe group_by() function pre-sorts data into groups so that whatever function follows is applied within each of the groups. That means group_by() always has something following it, and that something is usually summarize().\nIf we group_by() performer before we summarize/count our rows, it will put all of the rows with ‚ÄúAerosmith‚Äù together, then all the ‚ÄúBad Company‚Äù rows together, etc. and then it will count the rows within those groups ‚Ä¶ first those for Aerosmith and then those for Bad Company.\n\nModify your code block to add the group_by:\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n())\n\n# A tibble: 10,492 √ó 2\n   performer                  appearances\n   &lt;chr&gt;                            &lt;int&gt;\n 1 \"\\\"Groove\\\" Holmes\"                 14\n 2 \"\\\"Little\\\" Jimmy Dickens\"          10\n 3 \"\\\"Pookie\\\" Hudson\"                  1\n 4 \"\\\"Weird Al\\\" Yankovic\"             91\n 5 \"$NOT & A$AP Rocky\"                  1\n 6 \"'N Sync\"                          172\n 7 \"'N Sync & Gloria Estefan\"          20\n 8 \"'N Sync Featuring Nelly\"           20\n 9 \"'Til Tuesday\"                      53\n10 \"(+44)\"                              1\n# ‚Ñπ 10,482 more rows\n\n\nWhat we get in return is a summarized table that shows all 10,000+ different performers that have been on the charts, and the number of rows in which they appear in the data.\nThat‚Äôs great, but who had the most?\n\n\n4.5.4 Arrange the results\nRemember in our import notebook when we sorted all the rows by the oldest chart date? We‚Äôll use the same arrange() function here, but we‚Äôll change the result to descending order, because journalists almost always want to know the most of something.\n\nEdit your chunk to add the pipe and arrange function below and run it, then I‚Äôll explain.\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n# A tibble: 10,492 √ó 2\n   performer     appearances\n   &lt;chr&gt;               &lt;int&gt;\n 1 Taylor Swift         1178\n 2 Elton John            889\n 3 Madonna               857\n 4 Drake                 832\n 5 Kenny Chesney         777\n 6 Tim McGraw            739\n 7 Keith Urban           674\n 8 Stevie Wonder         659\n 9 Rod Stewart           657\n10 Mariah Carey          634\n# ‚Ñπ 10,482 more rows\n\n\n\nWe added the arrange() function and fed it the column of ‚Äúappearances‚Äù. If we left it with just that, then it would list the smallest values first.\nWithin the arrange function we wrapped our column in another function: desc() to change the order to ‚Äúdescending‚Äù, or the most on the top.\n\n\nI sometimes pipe the column name into the desc() function like this: arrange(appearances |&gt; desc()). You can also put a - before the column name like this: arrange(-appearances)) but I think is out of favor among tidyverse worldview folk.\n\n\n\n4.5.5 Get the top of the list\nWe‚Äôve printed 10,000+ rows of data into our notebook when we really only wanted the Top 10 or so. You might think it doesn‚Äôt matter, but your knitted HTML file will store all that data and can make it a big file (as in hard drive space), so I try to avoid that when I can.\nWe can use the head() command again to get our Top 10. It will give us a specified number of rows at the top of the table (with a default of six if we don‚Äôt specify.) There is a corresponding tail() function to get the last rows.\n\nPipe the result into head() function set to 10 rows.\n\n\nhot100 |&gt;\n  group_by(performer) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(appearances |&gt; desc()) |&gt; \n  head(10)\n\n# A tibble: 10 √ó 2\n   performer     appearances\n   &lt;chr&gt;               &lt;int&gt;\n 1 Taylor Swift         1178\n 2 Elton John            889\n 3 Madonna               857\n 4 Drake                 832\n 5 Kenny Chesney         777\n 6 Tim McGraw            739\n 7 Keith Urban           674\n 8 Stevie Wonder         659\n 9 Rod Stewart           657\n10 Mariah Carey          634\n\n\nIf I was to explain all of the code above in English, I would describe it as this:\n\nWe start with the hot100 data AND THEN\nwe group by the data by performer AND THEN\nwe summarize it by counting the number of rows in each group, calling the count ‚Äúappearances‚Äù AND THEN\nwe arrange the result by appearances in descending order AND THEN\nwe keep just the first 10 rows\n\nSince we have our answer here and we‚Äôre not using the result later, we don‚Äôt need to create a new object or anything. Printing it to our notebook is sufficient.\nSo, Taylor Swift ‚Ä¶ is that who you guessed? A little history here, Swift past Elton John in the summer of 2019. Elton John has been around a long time, but Swift‚Äôs popularity at a young age, plus changes in how Billboard counts plays in the modern era (like streaming) has rocketed her to the top. (Sorry, Rocket Man). And it doesn‚Äôt hurt that she is re-releasing her entire catalog (Taylor‚Äôs version)!\n\nAN IMPORTANT NOTE: The list we‚Äôve created here is based on unique performer names, and as such considers collaborations separately. For instance, Drake is near the top of the list but those are only songs he performed alone and not the many, many collaborations he has done with other performers. So, songs by ‚ÄúDrake‚Äù are counted separately than ‚ÄúDrake featuring Future‚Äù and even ‚ÄúFuture featuring Drake‚Äù. You‚Äôll need to make this clear when you write your data drop in a later assignment."
  },
  {
    "objectID": "counts-analysis.html#titleperformer-combo-with-most-appearances",
    "href": "counts-analysis.html#titleperformer-combo-with-most-appearances",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.6 Title/performer combo with most appearances",
    "text": "4.6 Title/performer combo with most appearances\nOur quest here is this: Which title/performer combination has been on the charts the most number of weeks at any position?\nThis is very similar to our quest to find the artist with the most appearances, but we have to consider both title and performer together because different artists can perform songs of the same name. For example, Adele‚Äôs song ‚ÄúHold On‚Äù entered the Hot 100 at 49 in December 2021, but 18 different performers have had a song titled ‚ÄúHold On‚Äù on the Hot 100.\n\nStart a new section (headline, text describing goal and a new code chunk.)\nAdd the code below ONE LINE AT A TIME and run it and then I‚Äôll outline it below.\n\nWRITE ONE LINE OF CODE. RUN IT. CHECK IT. REPEAT\nRemember, you write and run the line BEFORE you add the pipe |&gt;!\n\nhot100 |&gt; # start with the data, and then ...\n  group_by(performer, title) |&gt; # group by performer and title, and then ..\n  summarize(appearances = n()) |&gt; # count the rows with n(), and then ...\n  arrange(desc(appearances)) # arrange by appearances in descending order\n\n`summarise()` has grouped output by 'performer'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 30,425 √ó 3\n# Groups:   performer [10,492]\n   performer                                 title                   appearances\n   &lt;chr&gt;                                     &lt;chr&gt;                         &lt;int&gt;\n 1 Glass Animals                             Heat Waves                       91\n 2 The Weeknd                                Blinding Lights                  90\n 3 Imagine Dragons                           Radioactive                      87\n 4 AWOLNATION                                Sail                             79\n 5 Jason Mraz                                I'm Yours                        76\n 6 LeAnn Rimes                               How Do I Live                    69\n 7 LMFAO Featuring Lauren Bennett & GoonRock Party Rock Anthem                68\n 8 OneRepublic                               Counting Stars                   68\n 9 Adele                                     Rolling In The Deep              65\n10 Jewel                                     Foolish Games/You Were‚Ä¶          65\n# ‚Ñπ 30,415 more rows\n\n\nThe logic works like this:\n\nWe want to count combinations over two columns: title, performer. When you group_by more then one column, it will group rows where the values are the same in all columns. i.e.¬†all rows with both ‚ÄúRush‚Äù as a performer and Tom Sawyer as a title are in the same group. Rows with ‚ÄúRush‚Äù and Red Barchetta will be considered in a different group.\nWithin summarize(), we can name the new column first (we chose appearances as a name here), then describe what should fill it. In this case we filled the column using the n(), which counts the number of rows in each group.\nOnce you have a summary table, we sort it by appearances and set it to descending order, which puts the highest value on the top.\n\nWe will often use group_by(), summarize() and arrange() together, which is why I‚Äôll refer to this as the GSA trio. They are like three close friends that always want to hang out together.\nSo, what was your guess or this one? A little bit of history in that answer ‚Ä¶ The Weeknd‚Äôs Blinding Lights passed Imagine Dragon‚Äôs Radioactive some time in 2021, but then Glass Animals‚Äô Heat Waves topped the chart in 2022.\n\n4.6.1 Introducing filter()\nI showed you head() in the previous quest and that was useful to quickly limit that list, but it does so indiscriminately. In this case, if we use the default head() function that retains six rows, it would cut right in the middle of a tie at 68 records (at least with data through 2021). A better strategy is to cut off the list at a logical place using filter(). Let‚Äôs dive into this new function:\nFiltering is one of those Basic Data Journalism Functions:\n\n\n\nThe dplyr function filter() reduces the number of rows in our data based on one or more criteria within the data.\nThe syntax works like this:\n# this is psuedo code. don't run it\ndata |&gt; \n  filter(variable comparison value)\n\n# example\nhot100 |&gt; \n  filter(performer == \"Judas Priest\")\nThe filter() function typically works in this order:\n\nWhat is the variable (or column) you are searching in.\nWhat is the comparison you want to do. Equal to? Greater than?\nWhat is the observation (or value in the data) you are looking for?\n\nNote the two equals signs == in our Judas Priest example above. It‚Äôs important to use two of them when you are asking if a value is ‚Äútrue‚Äù or ‚Äúequal to‚Äù, as a single = typically means you are assigning a value to something.\n\n4.6.1.1 Comparisons: Logical tests\nThere are a number of these logical tests for the comparison:\n\n\n\nOperator\nDefinition\n\n\n\n\nx &lt; y\nLess than\n\n\nx &gt; y\nGreater than\n\n\nx == y\nEqual to\n\n\nx &lt;= y\nLess than or equal to\n\n\nx &gt;= y\nGreater than or equal to\n\n\nx !- y\nNot equal to\n\n\nx %in% c(y,z)\nIn a group\n\n\nis.na(x)\nIs NA (missing values)\n\n\n!is.na(x)\nIs not NA\n\n\n\nWhere you apply a filter matters. If we filter before group by/summarize/arrange (GSA) we are focusing the data before we summarize. If we filter after the GSA, we are affecting only the results of the summarize function, which is what we want to do here.\n\n\n4.6.1.2 Filter to a logical cutoff\nIn this case, I want you to use filter after the GSA actions to include only results with 65 or more appearances.\n\nEdit your current chunk to add a filter as noted in the example below. I‚Äôll explain it after.\n\n\nhot100 |&gt;\n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc()) |&gt; \n  filter(appearances &gt;= 65) # this is the new line\n\n`summarise()` has grouped output by 'performer'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 √ó 3\n# Groups:   performer [10]\n   performer                                 title                   appearances\n   &lt;chr&gt;                                     &lt;chr&gt;                         &lt;int&gt;\n 1 Glass Animals                             Heat Waves                       91\n 2 The Weeknd                                Blinding Lights                  90\n 3 Imagine Dragons                           Radioactive                      87\n 4 AWOLNATION                                Sail                             79\n 5 Jason Mraz                                I'm Yours                        76\n 6 LeAnn Rimes                               How Do I Live                    69\n 7 LMFAO Featuring Lauren Bennett & GoonRock Party Rock Anthem                68\n 8 OneRepublic                               Counting Stars                   68\n 9 Adele                                     Rolling In The Deep              65\n10 Jewel                                     Foolish Games/You Were‚Ä¶          65\n\n\nLet‚Äôs break down that last line:\n\nfilter() is the function.\nThe first argument in the function is the column we are looking in, appearances in our case.\nWe then provide a comparison operator &gt;= to get ‚Äúgreater than or equal to‚Äù.\nWe then give the value to compare, 65 in our case."
  },
  {
    "objectID": "counts-analysis.html#titleperformer-with-most-weeks-at-no.-1",
    "href": "counts-analysis.html#titleperformer-with-most-weeks-at-no.-1",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.7 Title/Performer with most weeks at No.¬†1",
    "text": "4.7 Title/Performer with most weeks at No.¬†1\nWe introduced filter() in the last quest to limit the summary. For this quest you‚Äôll need to filter the data before the group by/summarize/arrange trio.\nLet‚Äôs review the quest: Which performer/title combination was No.¬†1 for the most number of weeks?\nWhile this quest is very similar to the one above, it really helps to think about the logic of what you need and then build the query one line at a time to make each line works.\nLet‚Äôs talk through the logic:\n\nWe are starting with our hot100 data.\nDo we want to consider all the data? In this case, no: We only want titles that have a current_rank of 1. This means we will filter before any summarizing.\nThen we want to count the number of rows with the same performer and title combinations. This means we need to group_by both performer and title.\nSince we are counting rows, we need use n() as our summarize function, which counts the number or rows in each group.\n\nSo let‚Äôs step through this with code:\n\nCreate a section with a headline, text and code chunk\nStart with the hot100 data and then pipe into filter().\nWithin the filter, set the current_rank to be == to 1.\nRun the result and check it\n\n\nhot100 |&gt; \n  filter(current_rank == 1)\n\n# A tibble: 3,361 √ó 7\n   chart_date current_rank title  performer previous_rank peak_rank wks_on_chart\n   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 1958-08-04            1 Poor ‚Ä¶ Ricky Ne‚Ä¶            NA         1            1\n 2 1958-08-11            1 Poor ‚Ä¶ Ricky Ne‚Ä¶             1         1            2\n 3 1958-08-18            1 Nel B‚Ä¶ Domenico‚Ä¶             2         1            3\n 4 1958-08-25            1 Littl‚Ä¶ The Eleg‚Ä¶             2         1            4\n 5 1958-09-01            1 Nel B‚Ä¶ Domenico‚Ä¶             2         1            5\n 6 1958-09-08            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            6\n 7 1958-09-15            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            7\n 8 1958-09-22            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            8\n 9 1958-09-29            1 It's ‚Ä¶ Tommy Ed‚Ä¶             3         1            7\n10 1958-10-06            1 It's ‚Ä¶ Tommy Ed‚Ä¶             1         1            8\n# ‚Ñπ 3,351 more rows\n\n\nThe result should show only titles with a 1 for current_rank.\nThe rest of our logic is just like our last quest. We need to group by the title and performer and then summarize using n() to count the rows.\n\nEdit your existing chunk to add the group_by and summarize functions. Name your new column appearances and set it to count the rows with n().\n\n\nWhile ONE LINE AT A TIME is still ‚Äúthe Way‚Äù, group_by() won‚Äôt actually show you any different results that without it, so I usually write group_by() and summarize() together. Or I write the summarize() line first to make sure it works, then edit in the group_by() line to split the data before the summary.\n\n\n\nTry this on your own before you peek for the answer\n\n\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n())\n\n`summarise()` has grouped output by 'performer'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 1,144 √ó 3\n# Groups:   performer [757]\n   performer                          title                          appearances\n   &lt;chr&gt;                              &lt;chr&gt;                                &lt;int&gt;\n 1 'N Sync                            It's Gonna Be Me                         2\n 2 24kGoldn Featuring iann dior       Mood                                     8\n 3 2Pac Featuring K-Ci And JoJo       How Do U Want It/California L‚Ä¶           2\n 4 50 Cent                            In Da Club                               9\n 5 50 Cent Featuring Nate Dogg        21 Questions                             4\n 6 50 Cent Featuring Olivia           Candy Shop                               9\n 7 6ix9ine & Nicki Minaj              Trollz                                   1\n 8 ? (Question Mark) & The Mysterians 96 Tears                                 1\n 9 A Taste Of Honey                   Boogie Oogie Oogie                       3\n10 ABBA                               Dancing Queen                            1\n# ‚Ñπ 1,134 more rows\n\n\n\nLook at your results to make sure you have the three columns you expect: performer, title and appearances.\nThis doesn‚Äôt quite get us where we want because it lists the results alphabetically by the performer. You need to arrange the data to show us the most appearances at the top.\n\nEdit your chunk to add the arrange() function to sort by appearances in desc() order. This is just like our last quest.\n\n\n\nMaybe check your last chunk on how you used arrange, then try it before checking the answer here.\n\n\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc())\n\n`summarise()` has grouped output by 'performer'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 1,144 √ó 3\n# Groups:   performer [757]\n   performer                                         title           appearances\n   &lt;chr&gt;                                             &lt;chr&gt;                 &lt;int&gt;\n 1 Lil Nas X Featuring Billy Ray Cyrus               Old Town Road            19\n 2 Luis Fonsi & Daddy Yankee Featuring Justin Bieber Despacito                16\n 3 Mariah Carey & Boyz II Men                        One Sweet Day            16\n 4 Harry Styles                                      As It Was                15\n 5 Boyz II Men                                       I'll Make Love‚Ä¶          14\n 6 Elton John                                        Candle In The ‚Ä¶          14\n 7 Los Del Rio                                       Macarena (Bays‚Ä¶          14\n 8 Mariah Carey                                      We Belong Toge‚Ä¶          14\n 9 Mark Ronson Featuring Bruno Mars                  Uptown Funk!             14\n10 The Black Eyed Peas                               I Gotta Feeling          14\n# ‚Ñπ 1,134 more rows\n\n\n\nYou have your answer now (you go, Lil Nas) but we are listing more than 1,000 rows. Let‚Äôs cut this off at a logical place like we did in our last quest.\n\nUse filter() to cut your summary off at appearances of 14 or greater.\n\n\n\nYou‚Äôve done this before ‚Ä¶ try it on your own!\n\n\nhot100 |&gt;\n  filter(current_rank == 1) |&gt; \n  group_by(performer, title) |&gt;\n  summarize(appearances = n()) |&gt;\n  arrange(appearances |&gt; desc()) |&gt; \n  filter(appearances &gt;= 14)\n\n`summarise()` has grouped output by 'performer'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 √ó 3\n# Groups:   performer [11]\n   performer                                         title           appearances\n   &lt;chr&gt;                                             &lt;chr&gt;                 &lt;int&gt;\n 1 Lil Nas X Featuring Billy Ray Cyrus               Old Town Road            19\n 2 Luis Fonsi & Daddy Yankee Featuring Justin Bieber Despacito                16\n 3 Mariah Carey & Boyz II Men                        One Sweet Day            16\n 4 Harry Styles                                      As It Was                15\n 5 Boyz II Men                                       I'll Make Love‚Ä¶          14\n 6 Elton John                                        Candle In The ‚Ä¶          14\n 7 Los Del Rio                                       Macarena (Bays‚Ä¶          14\n 8 Mariah Carey                                      We Belong Toge‚Ä¶          14\n 9 Mark Ronson Featuring Bruno Mars                  Uptown Funk!             14\n10 The Black Eyed Peas                               I Gotta Feeling          14\n11 Whitney Houston                                   I Will Always ‚Ä¶          14\n\n\n\nNow you have the answers to the performer/title with the most weeks at No.¬†1 with a logical cutoff. If you add to the data later, that logic will still hold and not cut off arbitrarily at a certain number of records."
  },
  {
    "objectID": "counts-analysis.html#performer-with-most-titles-to-reach-no.-1",
    "href": "counts-analysis.html#performer-with-most-titles-to-reach-no.-1",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.8 Performer with most titles to reach No.¬†1",
    "text": "4.8 Performer with most titles to reach No.¬†1\nOur new quest is this: Which performer had the most titles reach No.¬†1? The answer might be easier to guess if you know music history, but perhaps not.\nThis sounds similar to our last quest, but there is a distinct difference. (That‚Äôs a bad joke that will reveal itself here in a bit.)\nAgain, let‚Äôs think through the logic of what we have to do to get our answer:\n\nWe need to consider only No.¬†1 songs (filter!)\nBecause a song could be No.¬†1 for more than one week, we need to consider the same title/performer combination only once. Another way to say this is we need unique or distinct combinations of title/performer. (We‚Äôll introduce a new function to find this.)\nOnce we have all the unique No.¬†1 songs in a list, then we can group by performer and count the number of times they are on the list.\n\nLet‚Äôs start by getting the No.¬†1 songs. You‚Äôve did this in the last quest.\n\nCreate a new section with a headline, text and code chunk.\nStart with the hot100 data and filter it so you only have current_rank of 1.\n\n\nhot100 |&gt; \n  filter(current_rank == 1)\n\n# A tibble: 3,361 √ó 7\n   chart_date current_rank title  performer previous_rank peak_rank wks_on_chart\n   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 1958-08-04            1 Poor ‚Ä¶ Ricky Ne‚Ä¶            NA         1            1\n 2 1958-08-11            1 Poor ‚Ä¶ Ricky Ne‚Ä¶             1         1            2\n 3 1958-08-18            1 Nel B‚Ä¶ Domenico‚Ä¶             2         1            3\n 4 1958-08-25            1 Littl‚Ä¶ The Eleg‚Ä¶             2         1            4\n 5 1958-09-01            1 Nel B‚Ä¶ Domenico‚Ä¶             2         1            5\n 6 1958-09-08            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            6\n 7 1958-09-15            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            7\n 8 1958-09-22            1 Nel B‚Ä¶ Domenico‚Ä¶             1         1            8\n 9 1958-09-29            1 It's ‚Ä¶ Tommy Ed‚Ä¶             3         1            7\n10 1958-10-06            1 It's ‚Ä¶ Tommy Ed‚Ä¶             1         1            8\n# ‚Ñπ 3,351 more rows\n\n\nNow look at the result. Note how ‚ÄúPoor Little Fool‚Äù shows up more than once? Other songs do as well. If we counted rows by performer now, that would tell us the number of weeks they‚Äôve had No.¬†1 songs, not how many different songs have made No.¬†1.\n\n4.8.1 Using distinct()\nThe next challenge in our logic is to show only unique performer/title combinations. We do this with distinct().\nWe feed the distinct() function with the variables we want to consider together, in our case the perfomer and title. All other columns are dropped since including them would mess up their distinctness.\n\nEdit your chunk to add the distinct() function to your code chunk.\n\n\nhot100 |&gt; \n  filter(current_rank == 1) |&gt; \n  distinct(title, performer)\n\n# A tibble: 1,144 √ó 2\n   title                           performer                       \n   &lt;chr&gt;                           &lt;chr&gt;                           \n 1 Poor Little Fool                Ricky Nelson                    \n 2 Nel Blu Dipinto Di Blu (Volar√©) Domenico Modugno                \n 3 Little Star                     The Elegants                    \n 4 It's All In The Game            Tommy Edwards                   \n 5 It's Only Make Believe          Conway Twitty                   \n 6 Tom Dooley                      The Kingston Trio               \n 7 To Know Him, Is To Love Him     The Teddy Bears                 \n 8 The Chipmunk Song               The Chipmunks With David Seville\n 9 Smoke Gets In Your Eyes         The Platters                    \n10 Stagger Lee                     Lloyd Price                     \n# ‚Ñπ 1,134 more rows\n\n\nNow we have a list of just No.¬†1 songs!\n\n\n4.8.2 Summarize the performers\nNow that we have our list of No.¬†1 songs, we can summarize the ‚Äúnumber‚Äù of times a performer is in the list to know how many No.¬†1 songs they have.\nWe‚Äôll again use the group_by/summarize/arrange combination for this, but we are only grouping by performer since that is the values we are counting.\n\nEdit your chunk to add a group_by on performer and then a summarize() to count the rows. Name the new column no_hits. Run it.\nAfter you are sure the group_by/summarize runs, add an arrange() to show the no1_hits in descending order.\n\n\n\nYou‚Äôve done this before! Give it a go before checking the code here.\n\n\nhot100 |&gt; \n  filter(current_rank == 1) |&gt;\n  distinct(title, performer) |&gt;\n  group_by(performer) |&gt;\n  summarize(no1_hits = n()) |&gt;\n  arrange(no1_hits |&gt; desc())\n\n# A tibble: 757 √ó 2\n   performer          no1_hits\n   &lt;chr&gt;                 &lt;int&gt;\n 1 The Beatles              19\n 2 Mariah Carey             16\n 3 Madonna                  12\n 4 Michael Jackson          11\n 5 Whitney Houston          11\n 6 The Supremes             10\n 7 Bee Gees                  9\n 8 Taylor Swift              8\n 9 The Rolling Stones        8\n10 Janet Jackson             7\n# ‚Ñπ 747 more rows\n\n\n\n\n\n4.8.3 Filter for a good cutoff\nLike we did earlier, use a filter() after your arrange to cut the list off at a logical place.\n\nEdit your chunk to filter the summary to show performers with 8 or more No.¬†1 hits.\n\n\n\nYou can do this. Really.\n\n\nhot100 |&gt; \n  filter(current_rank == 1) |&gt;\n  distinct(title, performer) |&gt;\n  group_by(performer) |&gt;\n  summarize(no1_hits = n()) |&gt;\n  arrange(no1_hits |&gt; desc()) |&gt; \n  filter(no1_hits &gt;= 8)\n\n# A tibble: 9 √ó 2\n  performer          no1_hits\n  &lt;chr&gt;                 &lt;int&gt;\n1 The Beatles              19\n2 Mariah Carey             16\n3 Madonna                  12\n4 Michael Jackson          11\n5 Whitney Houston          11\n6 The Supremes             10\n7 Bee Gees                  9\n8 Taylor Swift              8\n9 The Rolling Stones        8\n\n\n\nSo, The Beatles. Was that your guess? Look closely at that list ‚Ä¶ who has any chance of topping them?"
  },
  {
    "objectID": "counts-analysis.html#no.-1-hits-in-last-five-years",
    "href": "counts-analysis.html#no.-1-hits-in-last-five-years",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.9 No.¬†1 hits in last five years",
    "text": "4.9 No.¬†1 hits in last five years\nWhich performer had the most songs reach No.¬†1 in the most recent five years?\nLet‚Äôs talk through the logic. This is very similar to the No.¬†1 hits above but with two differences:\n\nIn addition to filtering for No.¬†1 songs, we also want to filter for songs in 2018-2022.\nWe might need to adjust our last filter for a better ‚Äúbreak point‚Äù.\n\nWe haven‚Äôt talked about filtering dates, so let me tell you this: You can use filter operations on dates just like you do any other text. This will give you rows after the last day of 2017.\nfilter(chart_date &gt; \"2017-12-31\")\nBut since we need this filter before our group, we can do this within the same filter function where we get the number one songs.\n\nCreate a new section (headline, text, chunk).\nBuild (from scratch, one line at a time) the same filter, group_by, summarize and arrange as above, but leave out the cut-off filter at the end. Make sure it runs.\nEdit your filter to put a comma after current_rank == 1 and then add this filter: chart_date &gt; \"2017-12-31\". Run the code.\nBuild a new cut-off filter at the end and keep only rows with more than 1 top_hits.\n\n\n\nNo, really. Try it on your own first.\n\n\nhot100 |&gt; \n  filter(\n    current_rank == 1,\n    chart_date &gt; \"2017-12-31\"\n  ) |&gt; \n  distinct(title, performer) |&gt; \n  group_by(performer) |&gt; \n  summarize(top_hits = n()) |&gt; \n  arrange(top_hits |&gt; desc()) |&gt; \n  filter(top_hits &gt; 1)\n\n# A tibble: 9 √ó 2\n  performer      top_hits\n  &lt;chr&gt;             &lt;int&gt;\n1 Drake                 5\n2 BTS                   4\n3 Taylor Swift          4\n4 Ariana Grande         3\n5 Harry Styles          2\n6 Lizzo                 2\n7 Olivia Rodrigo        2\n8 The Weeknd            2\n9 Travis Scott          2"
  },
  {
    "objectID": "counts-analysis.html#top-10-hits-overall-on-your-own",
    "href": "counts-analysis.html#top-10-hits-overall-on-your-own",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.10 Top 10 hits overall (on your own)",
    "text": "4.10 Top 10 hits overall (on your own)\nWhich performer had the most Top 10 hits overall?\nThis one I want you to do on your own.\nThe logic is very similar to the ‚ÄúMost No.¬†1 hits‚Äù quest you did before, but you need to adjust your filter to find songs within position 1 through 10. Don‚Äôt over think it, but do recognize that the ‚Äútop‚Äù of the charts are smaller numbers, not larger ones.\n\nMake a new section\nDescribe what you are doing\nDo it using the group_by/summarize method\nFilter to cut off at a logical number or rows. (i.e., don‚Äôt stop at a tie)"
  },
  {
    "objectID": "counts-analysis.html#complex-filters",
    "href": "counts-analysis.html#complex-filters",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.11 Complex filters",
    "text": "4.11 Complex filters\nYou can combine filters in different ways to really target which rows to keep. For these I want you to play around a bit.\n\nCopy each of the examples below into your notebook, but change the title and/or artists to some that you like.\n\n\n4.11.1 Multiple truths\nIf you want filter data for when two or more things are true, you can write two equations and combine with &. Only rows where both sides prove true are returned.\n\n# When Poor Little Fool was No. 1, but not any other position\nhot100 |&gt; \n  filter(title == \"Poor Little Fool\" & current_rank == 1) |&gt; \n  select(chart_date:performer)\n\n# A tibble: 2 √ó 4\n  chart_date current_rank title            performer   \n  &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;       \n1 1958-08-04            1 Poor Little Fool Ricky Nelson\n2 1958-08-11            1 Poor Little Fool Ricky Nelson\n\n\n\n\n4.11.2 Either is true\nIf you want an or filter, then you write two equations with a | between them.\n\n# songs by Adam Sandler OR Alabama Shakes\nhot100 |&gt; \n  filter(performer == \"Adam Sandler\" | performer == \"Alabama Shakes\") |&gt; \n  select(chart_date:performer)\n\n# A tibble: 4 √ó 4\n  chart_date current_rank title             performer     \n  &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;         \n1 1999-01-02           80 The Chanukah Song Adam Sandler  \n2 1999-01-09           98 The Chanukah Song Adam Sandler  \n3 2013-03-02          100 Hold On           Alabama Shakes\n4 2013-03-09           93 Hold On           Alabama Shakes\n\n\n\n| is the Shift of the \\ key above Return on your keyboard. That | character is also sometimes called a ‚Äúpipe‚Äù, which gets confusing in R with |&gt;.)\n\n\n\n4.11.3 Mixing criteria\nIf you have multiple criteria, you separate them with a comma ,. Note I‚Äôve also added returns to make the code more readable and added distinct to songs only once.\n\n# gives us rows with either Taylor Swift or Drake,\n# but only those that reached No. 1\nhot100 |&gt; \n  filter(\n    performer == \"Taylor Swift\" | performer == \"Drake\",\n    current_rank == 1\n) |&gt; \n  distinct(current_rank, title, performer)\n\n# A tibble: 13 √ó 3\n   current_rank title                                   performer   \n          &lt;dbl&gt; &lt;chr&gt;                                   &lt;chr&gt;       \n 1            1 We Are Never Ever Getting Back Together Taylor Swift\n 2            1 Shake It Off                            Taylor Swift\n 3            1 Blank Space                             Taylor Swift\n 4            1 Look What You Made Me Do                Taylor Swift\n 5            1 God's Plan                              Drake       \n 6            1 Nice For What                           Drake       \n 7            1 In My Feelings                          Drake       \n 8            1 Toosie Slide                            Drake       \n 9            1 Cardigan                                Taylor Swift\n10            1 Willow                                  Taylor Swift\n11            1 What's Next                             Drake       \n12            1 All Too Well (Taylor's Version)         Taylor Swift\n13            1 Anti-Hero                               Taylor Swift\n\n\n\n\n4.11.4 Search within a string\nAnd if you want to search for text within the data, you can use str_detect() to look for specific characters within a value to filter rows. str_detect() needs two arguments: 1) What column to search in, and 2) what to search for ‚Äúin quotes‚Äù. I also use distinct() here to show only unique title/performer combinations.\n\n# Songs where \"2 Chainz\" was among performers\nhot100 |&gt; \n  filter(str_detect(performer, \"2 Chainz\")) |&gt; \n  distinct(title, performer)\n\n# A tibble: 40 √ó 2\n   title                  performer                                            \n   &lt;chr&gt;                  &lt;chr&gt;                                                \n 1 Mercy                  Kanye West, Big Sean, Pusha T, 2 Chainz              \n 2 Beez In The Trap       Nicki Minaj Featuring 2 Chainz                       \n 3 No Lie                 2 Chainz Featuring Drake                             \n 4 Birthday Song          2 Chainz Featuring Kanye West                        \n 5 Yuck!                  2 Chainz Featuring Lil Wayne                         \n 6 Bandz A Make Her Dance Juicy J Featuring Lil Wayne & 2 Chainz               \n 7 My Moment              DJ Drama Featuring 2 Chainz, Meek Mill & Jeremih     \n 8 F**kin Problems        A$AP Rocky Featuring Drake, 2 Chainz & Kendrick Lamar\n 9 I'm Different          2 Chainz                                             \n10 R.I.P.                 Young Jeezy Featuring 2 Chainz                       \n# ‚Ñπ 30 more rows\n\n\nThere is a newer string search function called str_like() that has some interesting nuances derived from SQL. It must match the entire phrase so it will find only songs where ‚Äú2 Chainz‚Äù is the only artist, but it case INSENSITIVE by default, unlike str_detect().\n\nhot100 |&gt; \n  filter(str_like(performer, \"2 chainz\")) |&gt; # note chainz is lowercase in the search string\n  distinct(title, performer)\n\n# A tibble: 2 √ó 2\n  title         performer\n  &lt;chr&gt;         &lt;chr&gt;    \n1 I'm Different 2 Chainz \n2 Watch Out     2 Chainz \n\n\nOf course there is much, much more."
  },
  {
    "objectID": "counts-analysis.html#on-your-own",
    "href": "counts-analysis.html#on-your-own",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.12 On your own",
    "text": "4.12 On your own\nNow I want you to find something else on your own. It doesn‚Äôt matter what it is. Just find something about a performer or song you like.\n\nStart a new section with a Markdown headline\nUse Markdown text to declare what you are looking for\nFind it!\nAfter your code, explain what functions you used and why (like what did they do for you)"
  },
  {
    "objectID": "counts-analysis.html#a-shortcut-count",
    "href": "counts-analysis.html#a-shortcut-count",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.13 A shortcut: count()",
    "text": "4.13 A shortcut: count()\nIn the interest of full disclosure but at the risk of confusing you, I must reveal this fact:\nWe count stuff in data science (and journalism) all the time. So dplyr has a shortcut to group, count and arrange rows of data. We needed to use the long way above because a) we will use group_by() and summarize() with other math that isn‚Äôt just counting rows, and b) you need to understand what is happening inside count(), which uses group_by/summarize/arrange under the hood.\nThe count() function takes the columns you want to group and then does the summarize on n() for you:\n\nhot100 |&gt; \n  count(performer)\n\n# A tibble: 10,492 √ó 2\n   performer                      n\n   &lt;chr&gt;                      &lt;int&gt;\n 1 \"\\\"Groove\\\" Holmes\"           14\n 2 \"\\\"Little\\\" Jimmy Dickens\"    10\n 3 \"\\\"Pookie\\\" Hudson\"            1\n 4 \"\\\"Weird Al\\\" Yankovic\"       91\n 5 \"$NOT & A$AP Rocky\"            1\n 6 \"'N Sync\"                    172\n 7 \"'N Sync & Gloria Estefan\"    20\n 8 \"'N Sync Featuring Nelly\"     20\n 9 \"'Til Tuesday\"                53\n10 \"(+44)\"                        1\n# ‚Ñπ 10,482 more rows\n\n\nTo get the same pretty table you still have to rename the new column and reverse the sort, you just do it differently as arguments within the count() function. You can view the count() options here.\n\nAdd this chunk to your notebook (with a note you are trying count()) so you have it to refer to.\n\n\nhot100 |&gt; \n  count(performer, name = \"appearances\", sort = TRUE) |&gt; \n  filter(appearances &gt; 600)\n\n# A tibble: 13 √ó 2\n   performer       appearances\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Taylor Swift           1178\n 2 Elton John              889\n 3 Madonna                 857\n 4 Drake                   832\n 5 Kenny Chesney           777\n 6 Tim McGraw              739\n 7 Keith Urban             674\n 8 Stevie Wonder           659\n 9 Rod Stewart             657\n10 Mariah Carey            634\n11 Michael Jackson         613\n12 Chicago                 607\n13 Rascal Flatts           604\n\n\nSo the code above does same things here as we did in our first quest, but quicker.\n\nIMPORTANT: We will concentrate on using group_by/summarize/arrange because it can do SO MUCH MORE than count(). Count can ONLY count rows. It can‚Äôt do any other kind of math in summarize."
  },
  {
    "objectID": "counts-analysis.html#review-of-what-weve-learned",
    "href": "counts-analysis.html#review-of-what-weve-learned",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.14 Review of what we‚Äôve learned",
    "text": "4.14 Review of what we‚Äôve learned\nWe introduced a number of new functions in this lesson, most of them from the dplyr package. Mostly we filtered and summarized our data. Here are the functions we introduced in this chapter, many with links to documentation:\n\nfilter() returns only rows that meet logical criteria you specify.\nsummarize() builds a summary table about your data. You can count rows n() or do math on numerical values, like mean().\ngroup_by() is often used with summarize() to put data into groups before building a summary table based on the groups.\ndistinct() returns rows based on unique values in columns you specify. i.e., it deduplicates data.\nstr_detect() and str_like() to search within strings.\ncount() is a shorthand for the group_by/summarize operation to count rows based on groups. You can name your summary columns and sort the data within the same function."
  },
  {
    "objectID": "counts-analysis.html#turn-in-your-project",
    "href": "counts-analysis.html#turn-in-your-project",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.15 Turn in your project",
    "text": "4.15 Turn in your project\n\nMake sure everything runs properly (Restart R and Run All Chunks) and then Knit to HTML.\nZip the folder.\nUpload to the Canvas assignment."
  },
  {
    "objectID": "counts-analysis.html#soundtrack-for-this-assignment",
    "href": "counts-analysis.html#soundtrack-for-this-assignment",
    "title": "4¬† Summarize: count - analysis",
    "section": "4.16 Soundtrack for this assignment",
    "text": "4.16 Soundtrack for this assignment\nThis lesson was constructed with the vibes of The Bright Light Social Hour. They‚Äôve never had a song on the Hot 100 (at least not through 2021).\n\nhot100 |&gt; \n  filter(str_detect(performer, \"Bright Light\"))\n\n# A tibble: 0 √ó 7\n# ‚Ñπ 7 variables: chart_date &lt;date&gt;, current_rank &lt;dbl&gt;, title &lt;chr&gt;,\n#   performer &lt;chr&gt;, previous_rank &lt;dbl&gt;, peak_rank &lt;dbl&gt;, wks_on_chart &lt;dbl&gt;"
  },
  {
    "objectID": "sums-import.html#about-the-story-military-surplus-transfers",
    "href": "sums-import.html#about-the-story-military-surplus-transfers",
    "title": "5¬† Summarize: math - import",
    "section": "5.1 About the story: Military surplus transfers",
    "text": "5.1 About the story: Military surplus transfers\nAfter the 2014 death of Michael Brown and the unrest that followed, there was widespread criticism of Ferguson, Mo. police officers for their use of military-grade weapons and vehicles. There was also heightened scrutiny to a federal program that transfers unused military equipment to local law enforcement. Many news outlets, including NPR and Buzzfeed News, did stories based on data from the ‚Äú1033 program‚Äù handled through the Law Enforcement Support Office. Buzzfeed News also did a followup report in 2020 where reporter John Templon published his data analysis, which he did using Python.\nYou will analyze the same dataset to see how Central Texas police agencies have used the program and write a short data drop about transfers to those agencies.\n\n5.1.1 The 1033 program\nTo work through this story we need to understand how this transfer program works. You can read all about it here, but here is the gist:\nIn 1997, Congress approved the ‚Äú1033 program‚Äù that allows the U.S. military to give ‚Äúsurplus‚Äù equipment that they no longer need to law enforcement agencies like city police forces. The program is run by the Law Enforcement Support Office, which is part of the Defense Logistics Agency (which handles the global defense supply chain for all the branches of the military) within the Department of Defense. (The program is run by the office inside the agency that is part of the department.)\nAll kinds of equipment moves between the military and these agencies, from boots and water bottles to assault rifles and cargo planes. The local agency only pays for shipping the equipment, but that isn‚Äôt listed in the data. What is in the data is the ‚Äúoriginal value‚Äù of the equipment in dollars, but we can‚Äôt say the agency paid for it, because they didn‚Äôt.\nProperty falls into two categories: controlled and non-controlled. Controlled property ‚Äúconsists of military items that are provided via a conditional transfer or ‚Äòloan‚Äô basis where title remains with DoD/DLA. This includes items such as small arms/personal weapons, demilitarized vehicles and aircraft and night vision equipment. This property always remains in the LESO property book because it still belongs to and is accountable to the Department of Defense. When a local law enforcement agency no longer wants the controlled property, it must be returned to Law Enforsement Support Office for proper disposition.‚Äù This is explained in the LESO FAQ.\nBut most of the transfers to local agencies are for non-controlled property that can be sold to the general public, like boots and blankets. Those items are removed from the data after one year, unless it is deemed a special circumstance.\nThe agency releases data quarterly, but it is really a ‚Äúsnapshot in time‚Äù and not a complete history. Those non-controlled items transferred more than a year prior are missing, as are any controlled items returned to the feds.\n\n\n5.1.2 About the data\n\n\n\nThe raw data\n\n\nThe data comes in a spreadsheet that has a different tab for each state and territory. The data we‚Äôll use here was from June 31, 2022 and I‚Äôve done some initial work on the original data that is beyond the scope of this class, so we‚Äôll use my copy of the data. I will supply a link to the combined data below.\nThere is no data dictionary or record layout included with the data but I have corresponded with the Defense Logistics Agency to get a decent understanding of what is included.\n\nsheet: Which sheet the data came from. This is an artifact from the data merging script.\nstate: A two-letter designation for the state of the agency.\nagency_name: This is the agency that got the equipment.\nnsn: The National Stock Number, a special ID that identifies the item in a government supplies database.\nitem_name: The item transferred. Googling the names can sometimes yield more info on specific items, or you can search by nsn for more info.\nquantity: The number of the ‚Äúunits‚Äù the agency received.\nui: Unit of measurement (item, kit, etc.)\nacquisition_value: a cost per unit for the item.\ndemil_code: Categories (as single letter key values) that indicate how the item should be disposed of. Full details here.\ndemil_ic: Also part of the disposal categorization.\nship_date: The date the item(s) were sent to the agency.\nstation_type: What kind of law enforcement agency made the request.\n\nHere is a glimpse of a sample of the data:\n\n\nRows: 10\nColumns: 12\n$ sheet             &lt;dbl&gt; 17, 42, 5, 45, 36, 33, 5, 22, 3, 22\n$ state             &lt;chr&gt; \"KY\", \"SC\", \"CA\", \"TX\", \"OH\", \"NC\", \"CA\", \"MI\", \"AZ\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"MEADE COUNTY SHERIFF DEPT\", \"PROSPERITY POLICE DEPT‚Ä¶\n$ nsn               &lt;chr&gt; \"6115-01-435-1567\", \"1005-00-589-1271\", \"7520-01-519‚Ä¶\n$ item_name         &lt;chr&gt; \"GENERATOR SET,DIESEL ENGINE\", \"RIFLE,7.62 MILLIMETE‚Ä¶\n$ quantity          &lt;dbl&gt; 5, 1, 32, 1, 1, 1, 1, 1, 1, 1\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Dozen\", \"Each\", \"Each\", \"Each\", \"Ea‚Ä¶\n$ acquisition_value &lt;dbl&gt; 4623.09, 138.00, 16.91, 749.00, 749.00, 138.00, 499.‚Ä¶\n$ demil_code        &lt;chr&gt; \"A\", \"D\", \"A\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\"\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ ship_date         &lt;dttm&gt; 2021-03-22, 2007-08-07, 2020-10-29, 2014-11-18, 2014‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶\n\n\nAnd a look at just some of the more important columns:\n\nleso_sample |&gt; \n  select(agency_name, item_name, quantity, acquisition_value)\n\n# A tibble: 10 √ó 4\n   agency_name                      item_name         quantity acquisition_value\n   &lt;chr&gt;                            &lt;chr&gt;                &lt;dbl&gt;             &lt;dbl&gt;\n 1 MEADE COUNTY SHERIFF DEPT        GENERATOR SET,DI‚Ä¶        5            4623. \n 2 PROSPERITY POLICE DEPT           RIFLE,7.62 MILLI‚Ä¶        1             138  \n 3 KERN COUNTY SHERIFF OFFICE       MARKER,TUBE TYPE        32              16.9\n 4 LEAGUE CITY POLICE DEPT          RIFLE,5.56 MILLI‚Ä¶        1             749  \n 5 TRUMBULL COUNTY SHERIFF'S OFFICE RIFLE,5.56 MILLI‚Ä¶        1             749  \n 6 VALDESE POLICE DEPT              RIFLE,7.62 MILLI‚Ä¶        1             138  \n 7 SACRAMENTO POLICE DEPT           RIFLE,5.56 MILLI‚Ä¶        1             499  \n 8 BERRIEN COUNTY SHERIFF'S OFFICE  RIFLE,5.56 MILLI‚Ä¶        1             499  \n 9 LA PAZ COUNTY SHERIFF OFFICE     RIFLE,5.56 MILLI‚Ä¶        1             499  \n10 MUSKEGON HEIGHTS POLICE DEPT     RIFLE,5.56 MILLI‚Ä¶        1             749  \n\n\nEach row of data is a transfer of a particular type of item from the U.S. Department of Defense to a local law enforcement agency. The row includes the name of the item, the quantity, and the value ($) of a single unit.\nWhat the data doesn‚Äôt have is the total value of the items in the shipment. If there are 5 generators as noted in the first row above and the cost of each one is $4623.09, we have to multiply the quantity times the acquisition_value to get the total value of that equipment.\nThe data also doesn‚Äôt have any variable indicating if an item is controlled or non-controlled, but I‚Äôve corresponded with the Defense Logistics Agency to gain a pretty good understanding of how to isolate them based on the demilitarization codes.\nThese are things I learned about by talking to the agency and reading through documentation. This kind of reporting and understanding ABOUT your data is vital to avoid mistakes."
  },
  {
    "objectID": "sums-import.html#the-questions-we-will-answer",
    "href": "sums-import.html#the-questions-we-will-answer",
    "title": "5¬† Summarize: math - import",
    "section": "5.2 The questions we will answer",
    "text": "5.2 The questions we will answer\nAll answers should be based on data about Texas agencies from Jan.¬†1, 2010 to present. All of these questions are for the ‚Äúcontrolled‚Äù items, only.\n\nHow many total ‚Äúcontrolled‚Äù items were transferred, and what are they all worth? We‚Äôll summarize all the controlled items only to get the total quantity and total value of everything.\nHow many total ‚Äúcontrolled‚Äù items did each agency get and how much was it all worth? Which agency got the most stuff?\n\nHow about local agencies? I‚Äôll give you a list of those agencies.\n\nWhat specific ‚Äúcontrolled‚Äù items did each agency get and how much were they worth? Now we‚Äôre looking at the kinds of items.\n\nWhat did local agencies get?\n\n\nYou‚Äôll research some of the more interesting items the agencies received so you can include them in your data drop."
  },
  {
    "objectID": "sums-import.html#getting-started-create-your-project",
    "href": "sums-import.html#getting-started-create-your-project",
    "title": "5¬† Summarize: math - import",
    "section": "5.3 Getting started: Create your project",
    "text": "5.3 Getting started: Create your project\nWe will build the same project structure that we did with the Billboard project. In fact, all our class projects will have this structure. Since we‚Äôve done this before, some of the directions are less detailed.\n\nWith RStudio open, make sure you don‚Äôt have a project open. Go to File &gt; Close project.\nUse the create project button (or File &gt; New project) to create a new project in a ‚ÄúNew Directory‚Äù. Name the directory ‚Äúyourname-military-surplus‚Äù.\nCreate two folders: data-raw and data-processed."
  },
  {
    "objectID": "sums-import.html#importcleaning-notebook",
    "href": "sums-import.html#importcleaning-notebook",
    "title": "5¬† Summarize: math - import",
    "section": "5.4 Import/cleaning notebook",
    "text": "5.4 Import/cleaning notebook\nAgain, like Billboard, we‚Äôll create a notebook specifically for downloading, cleaning and prepping our data.\n\nCreate your RNotebook.\nRename the title ‚ÄúMilitary Surplus import/clean‚Äù.\nRemove the rest of the boilerplate template.\nSave the file and name it 01-import.Rmd.\n\n\n5.4.1 The goals of the notebook\nAs noted before, I separate cleaning into a separate notebook so that each subsequent analysis notebook can take advantage of that work. It‚Äôs the DRY principal in programming: Don‚Äôt Repeat Yourself. Often I will be in an analysis notebook realizing that it would be helpful to add a cleaning step to my import notebook, and I will. Because I‚Äôve worked with and researched this data, I‚Äôm aware of cleaning steps that a newcomer to the data might not be aware of at this point. But here we will take advantage of my experience and do all this cleaning work up front even though you haven‚Äôt seen the ‚Äúneed‚Äù for it yet.\nThat‚Äôs a long-winded opening to say let‚Äôs add our notebook goals so you know what‚Äôs going on here.\n\nIn Markdown, add a headline noting these are notebook goals.\nAdd the goals below:\n\n- Download the data\n- Import the data\n- Check datatypes\n- Create a total_value variable\n- Create a control_type variable\n- Filter to Texas agencies\n- Filter the date range (since Jan. 1 2010)\n- Export the cleaned data\n\n\n5.4.2 Add a setup section\nThis is the section in the notebook where we add our libraries and such. Again, every notebook has this section, though the packages may vary on need.\n\nAdd a headline and text about what we are doing: Our project setup.\nAdd a code chunk to load the libraries. You should only need tidyverse for this notebook because the data already has clean names (no need for janitor) and the dates will import correctly (no need for lubridate for this notebook, but will will use it for our analysis).\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "sums-import.html#download-the-data",
    "href": "sums-import.html#download-the-data",
    "title": "5¬† Summarize: math - import",
    "section": "5.5 Download the data",
    "text": "5.5 Download the data\n\nA new section means a new headline and description. Add it. It is good practice to describe and link to the data you will be using. You can use the text below:\n\n\nHINT: You might try triple-clicking on the text in the box below to select it all since it scrolls off the screen.\n\nWhile the data we will use here if from Prof. McDonald, it is from the [Law Enforcement Support Office](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/). Find more information [about the program here](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs/).\n\nUse the download.file() function to download the date into your data-raw folder. Remember you need two arguments:\n\ndownload.file(\"url_to_data\", \"path_to_folder/filename.csv\")\nWindows users might need to add an additional argument: mode = \"wb\".\n\nThe data can be found at this url: https://raw.githubusercontent.com/utdata/rwdir/main/data-raw/leso.csv\nIt should be saved into your data-raw folder with a name for the file.\n\nOnce you‚Äôve built your code chunk and run it, you should make sure the file downloaded into the correct place: in your data-raw folder.\n\n\nTry this on your own first, but the code is here if you need it.\n\n\n# You can comment the line below once you have the data\ndownload.file(\"https://raw.githubusercontent.com/utdata/rwdir/main/data-raw/leso.csv\",\n              \"data-raw/leso.csv\", mode = \"wb\")\n\n\n\n\nHINT: If you get an error about the path, you might make sure you created the data-raw folder first."
  },
  {
    "objectID": "sums-import.html#import-the-data",
    "href": "sums-import.html#import-the-data",
    "title": "5¬† Summarize: math - import",
    "section": "5.6 Import the data",
    "text": "5.6 Import the data\nWe are again working with a CSV, or comma-separated-values text file.\n\nAdd a new section: Headline, text if needed, code chunk.\n\nI suggest you build the code chunk a bit at a time in this order:\n\nUse read_csv() to read the file from our data-raw folder.\nEdit that line to put the result into a tibble object using &lt;-. Name your new tibble leso.\nPrint the tibble as a table to the screen again by putting the tibble object on a new line and running it. This allows you to see it in columnar form.\n\n\n\nTry real hard first before clicking here for the answer. Note the book will also show the response.\n\n\n# assigning the tibble\nleso &lt;- read_csv(\"data-raw/leso.csv\")\n\nRows: 117518 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (7): state, agency_name, nsn, item_name, ui, demil_code, station_type\ndbl  (4): sheet, quantity, acquisition_value, demil_ic\ndttm (1): ship_date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# printing the tibble\nleso\n\n# A tibble: 117,518 √ó 12\n   sheet state agency_name      nsn   item_name quantity ui    acquisition_value\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1     1 AL    ABBEVILLE POLIC‚Ä¶ 2540‚Ä¶ BALLISTI‚Ä¶       10 Kit              15872.\n 2     1 AL    ABBEVILLE POLIC‚Ä¶ 6760‚Ä¶ CAMERA R‚Ä¶        1 Each              1500 \n 3     1 AL    ABBEVILLE POLIC‚Ä¶ 5855‚Ä¶ ILLUMINA‚Ä¶       10 Each              1128 \n 4     1 AL    ABBEVILLE POLIC‚Ä¶ 1385‚Ä¶ UNMANNED‚Ä¶        1 Each             10000 \n 5     1 AL    ABBEVILLE POLIC‚Ä¶ 1240‚Ä¶ OPTICAL ‚Ä¶        1 Each               246.\n 6     1 AL    ABBEVILLE POLIC‚Ä¶ 2355‚Ä¶ MINE RES‚Ä¶        1 Each            658000 \n 7     1 AL    ABBEVILLE POLIC‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        9 Each               396 \n 8     1 AL    ABBEVILLE POLIC‚Ä¶ 2320‚Ä¶ TRUCK,UT‚Ä¶        1 Each             62627 \n 9     1 AL    ABBEVILLE POLIC‚Ä¶ 1005‚Ä¶ MOUNT,RI‚Ä¶       10 Each              1626 \n10     1 AL    ABBEVILLE POLIC‚Ä¶ 2320‚Ä¶ TRUCK,UT‚Ä¶        1 Each             62627 \n# ‚Ñπ 117,508 more rows\n# ‚Ñπ 4 more variables: demil_code &lt;chr&gt;, demil_ic &lt;dbl&gt;, ship_date &lt;dttm&gt;,\n#   station_type &lt;chr&gt;\n\n\n\n\n5.6.1 Glimpse the data\n\nIn a new code block, print the tibble but pipe it into glimpse() so you can see all the column names.\n\n\nleso |&gt;  glimpse()\n\nRows: 117,518\nColumns: 12\n$ sheet             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A‚Ä¶\n$ nsn               &lt;chr&gt; \"2540-01-565-4700\", \"6760-01-628-6105\", \"5855-01-577‚Ä¶\n$ item_name         &lt;chr&gt; \"BALLISTIC BLANKET KIT\", \"CAMERA ROBOT\", \"ILLUMINATO‚Ä¶\n$ quantity          &lt;dbl&gt; 10, 1, 10, 1, 1, 1, 9, 1, 10, 1, 1, 12, 11, 1, 1, 10‚Ä¶\n$ ui                &lt;chr&gt; \"Kit\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each‚Ä¶\n$ acquisition_value &lt;dbl&gt; 15871.59, 1500.00, 1128.00, 10000.00, 245.88, 658000‚Ä¶\n$ demil_code        &lt;chr&gt; \"D\", \"D\", \"D\", \"Q\", \"D\", \"C\", \"D\", \"C\", \"D\", \"C\", \"D‚Ä¶\n$ demil_ic          &lt;dbl&gt; 1, 7, 1, 3, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ ship_date         &lt;dttm&gt; 2018-01-30, 2017-02-08, 2017-03-28, 2017-03-28, 201‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶"
  },
  {
    "objectID": "sums-import.html#checking-datatypes",
    "href": "sums-import.html#checking-datatypes",
    "title": "5¬† Summarize: math - import",
    "section": "5.7 Checking datatypes",
    "text": "5.7 Checking datatypes\nTake a look at your glimpse returns. These are the things to watch for:\n\nAre your variable names (column names) clean? All lowercase with _ separating words?\nAre dates saved in a date format? ship_date looks good at &lt;dttm&gt;, which means ‚Äúdatetime‚Äù.\nAre your numbers really numbers? acquisition_value is the column we are most concerned about here, and it looks good.\n\nThis data set looks good (because I pre-prepared it fo you), but you always want to check and make corrections, like we did to fix the date in the Billboard assignment."
  },
  {
    "objectID": "sums-import.html#remove-unnecessary-columns",
    "href": "sums-import.html#remove-unnecessary-columns",
    "title": "5¬† Summarize: math - import",
    "section": "5.8 Remove unnecessary columns",
    "text": "5.8 Remove unnecessary columns\nSometimes at this point in a project, you might not know what columns you need to keep and which you could do without. The nice thing about doing this with code in a notebook is we can always go back, make corrections and run our notebook again. In this case, we will remove the -sheet column since we don‚Äôt need it. (It‚Äôs an artifact of the processing I‚Äôve done on the file.)\n\nStart a new section with a headline, text to explain you are removing unneeded columns.\nAdd a code chunk and the following code. I‚Äôll explain it below.\n\n\nleso_tight &lt;- leso |&gt; \n  select(-sheet)\n\nleso_tight |&gt; glimpse()\n\nRows: 117,518\nColumns: 11\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A‚Ä¶\n$ nsn               &lt;chr&gt; \"2540-01-565-4700\", \"6760-01-628-6105\", \"5855-01-577‚Ä¶\n$ item_name         &lt;chr&gt; \"BALLISTIC BLANKET KIT\", \"CAMERA ROBOT\", \"ILLUMINATO‚Ä¶\n$ quantity          &lt;dbl&gt; 10, 1, 10, 1, 1, 1, 9, 1, 10, 1, 1, 12, 11, 1, 1, 10‚Ä¶\n$ ui                &lt;chr&gt; \"Kit\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each‚Ä¶\n$ acquisition_value &lt;dbl&gt; 15871.59, 1500.00, 1128.00, 10000.00, 245.88, 658000‚Ä¶\n$ demil_code        &lt;chr&gt; \"D\", \"D\", \"D\", \"Q\", \"D\", \"C\", \"D\", \"C\", \"D\", \"C\", \"D‚Ä¶\n$ demil_ic          &lt;dbl&gt; 1, 7, 1, 3, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ ship_date         &lt;dttm&gt; 2018-01-30, 2017-02-08, 2017-03-28, 2017-03-28, 201‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶\n\n\nIn English, we are creating a new tibble leso_tight and then filling it with this: leso and then use select to remove the colunn called sheet. We then glimpse the new data.\nSo now we have a tibble called leso_tight that we will work with in the next section."
  },
  {
    "objectID": "sums-import.html#create-a-total_value-column",
    "href": "sums-import.html#create-a-total_value-column",
    "title": "5¬† Summarize: math - import",
    "section": "5.9 Create a total_value column",
    "text": "5.9 Create a total_value column\nDuring my reporting about this data I learned that the acquisition_value noted in the data is for a single ‚Äúunit‚Äù of each item. If the shipment item was a rifle with a quantity of ‚Äú5‚Äù and acquisition_value of ‚Äú200‚Äù, then each rifle is worth $200, but the total shipment would be 5 times $200, or $1,000. That $1000 total value is not listed in the data, so we need to add it.\nLet‚Äôs walk through how to do that with a different example.\nWhen we used mutate() to convert the date in the Billboard assignment, we were reassigning values in each row of a column back into the same column.\nIn this assignment, we will use mutate() to create a new column with new values based on a calculation (quantity multiplied by the acquisition_value) for each row. Let‚Äôs review the concept first.\nIf you started with data like this:\n\n\n\nitem\nitem_count\nitem_value\n\n\n\n\nBread\n2\n1.5\n\n\nMilk\n1\n2.75\n\n\nBeer\n3\n9\n\n\n\nAnd then wanted to create a total value of each item in the table, you would use mutate():\ndata |&gt; \n  mutate(total_value = item_count * item_value)\nYou would get a return like this, with your new total_value column added at the end:\n\n\n\nitem\nitem_count\nitem_value\ntotal_value\n\n\n\n\nBread\n2\n1.5\n3\n\n\nMilk\n1\n2.75\n2.75\n\n\nBeer\n3\n9\n27\n\n\n\nOther math operators work as well: +, -, * and /.\nSo, now that we‚Äôve talked about how it is done, I want you to:\n\nCreate a new section with headline, text and code chunk.\nUse mutate() to create a new total_value column that multiplies quantity times acquisition_value.\nAssign those results into a new tibble called leso_total so we can all be on the same page.\nGlimpse the new tibble so you can check the results.\n\n\n\nTry it on your own. You can figure it out!\n\n\nleso_total &lt;- leso_tight |&gt; \n  mutate(\n    total_value = quantity * acquisition_value\n  )\n\nleso_total |&gt; glimpse()\n\nRows: 117,518\nColumns: 12\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A‚Ä¶\n$ nsn               &lt;chr&gt; \"2540-01-565-4700\", \"6760-01-628-6105\", \"5855-01-577‚Ä¶\n$ item_name         &lt;chr&gt; \"BALLISTIC BLANKET KIT\", \"CAMERA ROBOT\", \"ILLUMINATO‚Ä¶\n$ quantity          &lt;dbl&gt; 10, 1, 10, 1, 1, 1, 9, 1, 10, 1, 1, 12, 11, 1, 1, 10‚Ä¶\n$ ui                &lt;chr&gt; \"Kit\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each‚Ä¶\n$ acquisition_value &lt;dbl&gt; 15871.59, 1500.00, 1128.00, 10000.00, 245.88, 658000‚Ä¶\n$ demil_code        &lt;chr&gt; \"D\", \"D\", \"D\", \"Q\", \"D\", \"C\", \"D\", \"C\", \"D\", \"C\", \"D‚Ä¶\n$ demil_ic          &lt;dbl&gt; 1, 7, 1, 3, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ ship_date         &lt;dttm&gt; 2018-01-30, 2017-02-08, 2017-03-28, 2017-03-28, 201‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶\n$ total_value       &lt;dbl&gt; 158715.90, 1500.00, 11280.00, 10000.00, 245.88, 6580‚Ä¶\n\n\n\n\nCheck that it worked!!. Use the glimpsed data to check the first item: For me, 10 * 15871.59 = 158715.90, which is correct!\nNote that new columns are added at the end of the tibble. That is why I suggested you glimpse the data instead of printing the tibble so you can easily see results on one screen."
  },
  {
    "objectID": "sums-import.html#controlled-vs.-non-controlled",
    "href": "sums-import.html#controlled-vs.-non-controlled",
    "title": "5¬† Summarize: math - import",
    "section": "5.10 Controlled vs.¬†non-controlled",
    "text": "5.10 Controlled vs.¬†non-controlled\nAgain, by reading through the documentation about this data I learned about controlled vs non-controlled property. Basically non-controlled generic stuff like boots and blankets are removed from the data after one year, but controlled items like guns and airplanes remain on the list until they are returned to the military for disposal. It is the mainly the controlled items we are concerned with.\nThere isn‚Äôt anything within the data that says it is ‚Äúcontrolled‚Äù and really no clear indication in the documentation on how to tell what is what. So, I emailed the agency and asked them. Here was their answer, edited:\n\nProperty with the DEMIL codes A and Q6 are considered non-controlled general property and fall off the LESO property books after one year. All other Demil codes are considered controlled items and stay on the LESO property book until returned to DLA for disposition/disposal. However, there are some exceptions. For instance, aircraft are always controlled regardless of the demil code. Also, LESO has the discretion to keep items as controlled despite the demil code. This happens with some high value items. There isn‚Äôt a standard minimum value. It also may also depend on the type of property.\n\nThis actually took some back and forth to figure out, as I had noticed there were AIRPLANE, CARGO-TRANSPORT items in the data that were older than a year, along with some surveillance robots. That‚Äôs when they replied about the airplanes, but it turns out the robots were simply an error. DATA IS DIRTY when humans get involved; somebody coded it wrong.\nThese ‚ÄúDEMIL codes‚Äù they referenced are the demil_code and demil_ic columns in the data, so we can use those to mark which records are ‚Äúnon-controlled‚Äù (A and Q6) and then mark all the rest as ‚Äúcontrolled‚Äù. We know of one exception ‚Äì airplanes ‚Äì which we need to mark controlled. We can‚Äôt do much with the ‚Äúhigh value‚Äù items since there isn‚Äôt logic available to find them. We‚Äôll just have to note that in our story, something like ‚Äúthe agency sometimes designates high value items like airplanes as controlled, but they are not categorized as such and may have been dropped from our analysis.‚Äù\nBut, we can catch most of them ‚Ä¶ we just need to work through the logic. This is not an uncommon challenge in data science, so we have some tools to do this. Our workhorse is the case_when() function, where we can make decisions based on values in our data.\nI usually approach this by thinking of the logic first, then writing some code, then testing it. Sometimes my logic is faulty and I have to try again, which is why we test the results. Know this could go on for many cycles. In the interest of time and getting this done, I will just show the finished code and explain how it works.\nHere is the basic idea:\n\nWe want to create a new column to denote if the item is controlled.\nIn that column we want it to be TRUE when an item is controlled, and FALSE when it is not.\nWe know that items with ‚ÄúAIRPLANE‚Äù are always controlled, no matter their demil designations.\nOtherwise we know that items that have a demil_code of ‚ÄúA‚Äù, OR a demil_code of ‚ÄúQ‚Äù AND a demil_id of ‚Äú6‚Äù, are non-controlled.\nEverything else is controlled.\n\nI‚Äôve noted this logic in a specific order for a reason: It‚Äôs the order that we write the logic in the code based on how the function case_when() works.\n\n5.10.1 Categorization logic with case_when()\nWe will use the mutate() function to create a new column called control_type. We‚Äôve done that before, so no problem.\nBut this time we will fill in values in the new column based on other data inside each row. case_when() allows us to create a test (or number of tests) and then mark the new value based on the answer. Once new data has been written the function goes to the next row, so we write the most specific rules first.\nThis process is powerful and can get complicated depending on the logic needed. This example is perhaps more complicated than I like to explain this process, but this is real data and we need this, so here we go.\n\nStart a new section and explain that you are marking controlled items.\nCopy/paste the code chunk below and run it\nREAD the explanation that follows, so you understand what is going on!\n\n\nleso_control &lt;- leso_total |&gt; \n  mutate(\n    control_type = case_when(\n      str_detect(item_name, \"AIRPLANE\") ~ TRUE,\n      (demil_code == \"A\" | (demil_code == \"Q\" & demil_ic == 6)) ~ FALSE,\n      TRUE ~ TRUE\n    )\n  )\n\nleso_control |&gt; glimpse()\n\nRows: 117,518\nColumns: 13\n$ state             &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"ABBEVILLE POLICE DEPT\", \"ABBEVILLE POLICE DEPT\", \"A‚Ä¶\n$ nsn               &lt;chr&gt; \"2540-01-565-4700\", \"6760-01-628-6105\", \"5855-01-577‚Ä¶\n$ item_name         &lt;chr&gt; \"BALLISTIC BLANKET KIT\", \"CAMERA ROBOT\", \"ILLUMINATO‚Ä¶\n$ quantity          &lt;dbl&gt; 10, 1, 10, 1, 1, 1, 9, 1, 10, 1, 1, 12, 11, 1, 1, 10‚Ä¶\n$ ui                &lt;chr&gt; \"Kit\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each‚Ä¶\n$ acquisition_value &lt;dbl&gt; 15871.59, 1500.00, 1128.00, 10000.00, 245.88, 658000‚Ä¶\n$ demil_code        &lt;chr&gt; \"D\", \"D\", \"D\", \"Q\", \"D\", \"C\", \"D\", \"C\", \"D\", \"C\", \"D‚Ä¶\n$ demil_ic          &lt;dbl&gt; 1, 7, 1, 3, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ ship_date         &lt;dttm&gt; 2018-01-30, 2017-02-08, 2017-03-28, 2017-03-28, 201‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶\n$ total_value       &lt;dbl&gt; 158715.90, 1500.00, 11280.00, 10000.00, 245.88, 6580‚Ä¶\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE‚Ä¶\n\n\nOK, let‚Äôs go through the code line-by-line.\n\nOur first line creates a new tibble and fills it with the result of the rest of our expression. We start with the leso_total tibble.\nWe mutate the data and start with the name of new column: control_type. We are filling that column with the result of the case_when() function for each row.\nWithin the case_when() we are making the determination if the item is controlled or not. The left side of the ~ is the test, and the right side of ~ is what we enter into the column. But we have more than one test:\n\nThe first test is we use the str_detect() function to look inside the item_name field looking for the term ‚ÄúAIRPLANE‚Äù. If the test finds the term, then the control_type field gets a value of TRUE and we move to the next row. If not, it moves to the next rule to see if that is a match. (We could fill this column with any text or number we want, but we are using TRUE and FALSE because that is the most basic kind of data to keep. If the item is controlled, set the value is TRUE. If not, it should be set to FALSE.)\nOur second rule has two complex tests and we want to mark the row FALSE if either are true. (Remember, this is based on what the DLA told me: items with A or Q6 are non-controlled.) Our case_when() logic first looks for the value ‚ÄúA‚Äù in the demil_code field. If it is yes, then it marks the row FALSE. If no it goes to the next part: Is there a ‚ÄúQ‚Äù in the demil_code field AND a ‚Äú6‚Äù in the demil_ic field? Both ‚ÄúQ‚Äù and ‚Äú6‚Äù have to be there to get marked as FALSE. If both fail, then we move to the next test.\nThe last test is our catch-all. If none of the other rules apply, then mark this row as TRUE, which means it is controlled. So our default in the end is to mark everything TRUE if any of the other rules don‚Äôt mark it first.\n\nLastly we glimpse at the data just so we can see the column was created.\n\nAs I said, we skipped the process of figuring all that out line-by-line, but I‚Äôll show some tests here to show that we did what we were intending.\nThis shows airplanes are marked as controlled with TRUE.\n\n# showing the results and some columns that determined them\nleso_control |&gt; \n  select(item_name, demil_code, demil_ic, control_type) |&gt; \n  filter(str_detect(item_name, \"AIRPLANE\"))\n\n# A tibble: 15 √ó 4\n   item_name                demil_code demil_ic control_type\n   &lt;chr&gt;                    &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;       \n 1 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n 2 AIRPLANE,CARGO-TRANSPORT A                 1 TRUE        \n 3 AIRPLANE,CARGO-TRANSPORT A                 1 TRUE        \n 4 FIRST AID KIT,AIRPLANE   A                 1 TRUE        \n 5 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n 6 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n 7 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n 8 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n 9 AIRPLANE,CARGO-TRANSPORT A                 1 TRUE        \n10 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n11 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n12 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n13 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n14 AIRPLANE,CARGO-TRANSPORT Q                 6 TRUE        \n15 AIRPLANE,FLIGHT T42A     Q                 3 TRUE        \n\n\nThis shows how many items are marked TRUE vs FALSE for each demil_code and demil_ic combination. Don‚Äôt sweat over this code as we cover it in later chapters, but know I used it to check that most A records were FALSE, along with Q6.\n\nleso_control |&gt; \n  count(demil_code, demil_ic, control_type, name = \"cnt\") |&gt; \n  pivot_wider(names_from = control_type, values_from = cnt) |&gt; \n  DT::datatable()\n\n\n\n\n\n\nOK, onto the next task to get Texas data for specific dates."
  },
  {
    "objectID": "sums-import.html#filtering-our-data",
    "href": "sums-import.html#filtering-our-data",
    "title": "5¬† Summarize: math - import",
    "section": "5.11 Filtering our data",
    "text": "5.11 Filtering our data\nYou used filter() in the Billboard lesson to get No.¬†1 songs and to get a date range of data. We need to do something similar here to get only Texas data of a certain date range, but we‚Äôll build the filters one at a time so we can check the results.\n\n5.11.1 Apply the TX filter\n\nCreate a new section with headlines and text that denote you are filtering the data to Texas and since Jan.¬†1, 2010.\nCreate the code chunk and start your filter process using the leso_control tibble.\nUse filter() on the state column to keep all rows with ‚ÄúTX‚Äù.\n\n\n\nTry this on your own, but the code is here.\n\n\nleso_control |&gt; \n  filter(\n    state == \"TX\"\n  )\n\n# A tibble: 7,817 √ó 13\n   state agency_name nsn   item_name quantity ui    acquisition_value demil_code\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     \n 1 TX    ABERNATHY ‚Ä¶ 2320‚Ä¶ TRUCK,UT‚Ä¶        1 Each              62627 C         \n 2 TX    ABERNATHY ‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        5 Each                333 D         \n 3 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 4 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 5 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 6 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 7 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 8 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n 9 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n10 TX    ABILENE PO‚Ä¶ 1005‚Ä¶ RIFLE,5.‚Ä¶        1 Each                499 D         \n# ‚Ñπ 7,807 more rows\n# ‚Ñπ 5 more variables: demil_ic &lt;dbl&gt;, ship_date &lt;dttm&gt;, station_type &lt;chr&gt;,\n#   total_value &lt;dbl&gt;, control_type &lt;lgl&gt;\n\n\n\n\nHow do you know if it worked? Well the first column in the data is the state column, so they should all start with ‚ÄúTX‚Äù. Also note you started with nearly 130k observations (rows), and there are only 8,600+ in Texas.\n\n\n5.11.2 Add the date filter\n\nNow, EDIT THAT SAME CHUNK to add a new part to your filter to also get rows with a ship_date of 2010-01-01 or later.\nSave the result into a new tibble called leso_filtered.\nPrint out the new tibble leso_filtered.\n\n\n\nIf you do this on your own, treat yourself to a cookie!\n\n\nleso_filtered &lt;- leso_control |&gt; \n  filter(\n    state == \"TX\",\n    ship_date &gt;= \"2010-01-01\"\n  )\n\nleso_filtered\n\n# A tibble: 6,698 √ó 13\n   state agency_name nsn   item_name quantity ui    acquisition_value demil_code\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     \n 1 TX    ABERNATHY ‚Ä¶ 2320‚Ä¶ TRUCK,UT‚Ä¶        1 Each              62627 C         \n 2 TX    ABERNATHY ‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        5 Each                333 D         \n 3 TX    ALLEN POLI‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        1 Each                396 D         \n 4 TX    ALLEN POLI‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        1 Each                396 D         \n 5 TX    ALLEN POLI‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        1 Each                396 D         \n 6 TX    ALLEN POLI‚Ä¶ 1385‚Ä¶ MK3MOD0          1 Each             371680 Q         \n 7 TX    ALLEN POLI‚Ä¶ 1385‚Ä¶ MK3MOD0          1 Each             371680 Q         \n 8 TX    ALLEN POLI‚Ä¶ 2355‚Ä¶ MINE RES‚Ä¶        1 Each             658000 C         \n 9 TX    ALLEN POLI‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        1 Each                396 D         \n10 TX    ALLEN POLI‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        1 Each                396 D         \n# ‚Ñπ 6,688 more rows\n# ‚Ñπ 5 more variables: demil_ic &lt;dbl&gt;, ship_date &lt;dttm&gt;, station_type &lt;chr&gt;,\n#   total_value &lt;dbl&gt;, control_type &lt;lgl&gt;\n\n\n\n\n\n5.11.3 Checking the results with summary()\nHow do you know this date filter worked? Well, we went from 8600+ rows to 7400+ rows, so we did something. You might look at the results table and click over to the ship_date columns so you can see some of the results, but you can‚Äôt be sure the top row is the oldest. We could use an arrange() to test that, but I have another suggestion: summary().\nNow, summary() is different than summarize(), which we‚Äôll do plenty of in a minute. The summary() function will show you some results about each column in your data, and when it is a number or date, it will give you some basic stats like min, max and median values.\n\nEdit your chunk to not just print out the tibble, but pipe that into summary(), like this:\n\n\nleso_filtered |&gt; summary()\n\n    state           agency_name            nsn             item_name        \n Length:6698        Length:6698        Length:6698        Length:6698       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    quantity             ui            acquisition_value  demil_code       \n Min.   :   1.000   Length:6698        Min.   :      0   Length:6698       \n 1st Qu.:   1.000   Class :character   1st Qu.:    132   Class :character  \n Median :   1.000   Mode  :character   Median :    499   Mode  :character  \n Mean   :   6.862                      Mean   :  19664                     \n 3rd Qu.:   1.000                      3rd Qu.:   2275                     \n Max.   :1223.000                      Max.   :5390000                     \n                                                                           \n    demil_ic       ship_date                   station_type      \n Min.   :0.000   Min.   :2010-01-05 00:00:00   Length:6698       \n 1st Qu.:1.000   1st Qu.:2012-03-20 00:00:00   Class :character  \n Median :1.000   Median :2014-10-27 00:00:00   Mode  :character  \n Mean   :1.399   Mean   :2015-10-24 17:11:10                     \n 3rd Qu.:1.000   3rd Qu.:2019-02-10 00:00:00                     \n Max.   :7.000   Max.   :2022-06-29 00:00:00                     \n NA's   :514                                                     \n  total_value      control_type   \n Min.   :      0   Mode :logical  \n 1st Qu.:    333   FALSE:1177     \n Median :    749   TRUE :5521     \n Mean   :  21910                  \n 3rd Qu.:   4506                  \n Max.   :5390000                  \n                                  \n\n\nNow look at the ‚ÄúMin.‚Äù value of ship_date and make sure it is not older than 2010."
  },
  {
    "objectID": "sums-import.html#export-cleaned-data",
    "href": "sums-import.html#export-cleaned-data",
    "title": "5¬† Summarize: math - import",
    "section": "5.12 Export cleaned data",
    "text": "5.12 Export cleaned data\nNow that we have our data selected, mutated and filtered how we want it, we can export your leso_filtered tibble into an .rds file to use in our analysis notebook. If you recall, we use the .rds format because it will remember data types and such.\n\nCreate a new section with headline and text explaining that you are exporting the data.\nDo it. The function you need is called write_rds and you need to give it a path/name that saves the file in the data-processed folder. Name it 01-leso-tx.rds so you know it a) came from the first notebook b) is the Texas only data. Well-formatted, descriptive file names are important to your future self and other colleagues.\n\n\nleso_filtered |&gt; write_rds(\"data-processed/01-leso-tx.rds\")"
  },
  {
    "objectID": "sums-import.html#things-we-learned-in-this-lesson",
    "href": "sums-import.html#things-we-learned-in-this-lesson",
    "title": "5¬† Summarize: math - import",
    "section": "5.13 Things we learned in this lesson",
    "text": "5.13 Things we learned in this lesson\nThis chapter was similar to when we imported data for Billboard, but we did introduce a couple of new concepts:\n\ncase_when() allows you to categorize a new column based on logic within your data.\nsummary() gives you descriptive statistics about your tibble. We used it to check the ‚Äúmin‚Äù date, but you can also see averages (mean), max and medians."
  },
  {
    "objectID": "sums-analysis.html#learning-goals-of-this-lesson",
    "href": "sums-analysis.html#learning-goals-of-this-lesson",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.1 Learning goals of this lesson",
    "text": "6.1 Learning goals of this lesson\nIn this chapter we will start querying the data using summarize with math, basically using summarize to add values in a column instead of counting rows, which we did with the Billboard assignment.\nOur learning goals are:\n\nTo use the combination of group_by(), summarize() and arrange() to add columns of data using sum().\nTo use different group_by() groupings in specific ways to get desired results.\nTo practice using filter() on those summaries to better see certain results, including filtering within a vector (or list of strings).\nWe‚Äôll research and write about some of the findings, practicing data-centric ledes and sentences describing data."
  },
  {
    "objectID": "sums-analysis.html#questions-to-answer",
    "href": "sums-analysis.html#questions-to-answer",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.2 Questions to answer",
    "text": "6.2 Questions to answer\nAll answers should be based on data about Texas agencies from Jan.¬†1, 2010 to present. All of these questions are for the ‚Äúcontrolled‚Äù items, only.\n\nHow many total ‚Äúcontrolled‚Äù items were transferred, and what are they all worth? We‚Äôll summarize all the controlled items only to get the total quantity and total value of everything.\nHow many total ‚Äúcontrolled‚Äù items did each agency get and how much was it all worth? Which agency got the most stuff?\n\nHow about local agencies? I‚Äôll give you a list.\n\nWhat specific ‚Äúcontrolled‚Äù items did each agency get and how much were they worth? Now we‚Äôre looking at the kinds of items.\n\nWhat did local agencies get?\n\n\nYou‚Äôll research some of the more interesting items the agencies received so you can include them in your data drop."
  },
  {
    "objectID": "sums-analysis.html#set-up-the-analysis-notebook",
    "href": "sums-analysis.html#set-up-the-analysis-notebook",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.3 Set up the analysis notebook",
    "text": "6.3 Set up the analysis notebook\nBefore we get into how to do this, let‚Äôs set up our analysis notebook.\n\nMake sure you have your military surplus project open in RStudio. If you have your import notebook open, close it and use Run &gt; Restart R and Clear Output.\nCreate a new RNotebook and edit the title as ‚ÄúMilitary surplus analysis‚Äù.\nRemove the boilerplate text.\nCreate a setup section (headline, text and code chunk) that loads the tidyverse library.\nSave the notebook at 02-analysis.Rmd.\n\nWe‚Äôve started each notebook like this, so you should be able to do this on your own now.\n\n\nLibrary setup\n\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.2     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.2     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n6.3.1 Load the data into a tibble\n\nNext create an import section (headline, text and chunk) that loads the data from the previous notebook and save it into a tibble called tx.\nAdd a glimpse() of the data for your reference.\n\nWe did this in Billboard and you should be able to do it. You‚Äôll use read_rds() and find your data in your data-processed folder.\n\n\nRemember your data is in data-processed\n\n\ntx &lt;- read_rds(\"data-processed/01-leso-tx.rds\")\n\ntx |&gt; glimpse()\n\nRows: 6,698\nColumns: 13\n$ state             &lt;chr&gt; \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\"‚Ä¶\n$ agency_name       &lt;chr&gt; \"ABERNATHY POLICE DEPT\", \"ABERNATHY POLICE DEPT\", \"A‚Ä¶\n$ nsn               &lt;chr&gt; \"2320-01-371-9584\", \"1240-01-540-3690\", \"1240-01-411‚Ä¶\n$ item_name         &lt;chr&gt; \"TRUCK,UTILITY\", \"SIGHT,REFLEX\", \"SIGHT,REFLEX\", \"SI‚Ä¶\n$ quantity          &lt;dbl&gt; 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ ui                &lt;chr&gt; \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Each\", \"Eac‚Ä¶\n$ acquisition_value &lt;dbl&gt; 62627, 333, 396, 396, 396, 371680, 371680, 658000, 3‚Ä¶\n$ demil_code        &lt;chr&gt; \"C\", \"D\", \"D\", \"D\", \"D\", \"Q\", \"Q\", \"C\", \"D\", \"D\", \"D‚Ä¶\n$ demil_ic          &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 6, 7, 7, 1, 1‚Ä¶\n$ ship_date         &lt;dttm&gt; 2016-03-07, 2016-02-02, 2013-09-13, 2013-09-13, 201‚Ä¶\n$ station_type      &lt;chr&gt; \"State\", \"State\", \"State\", \"State\", \"State\", \"State\"‚Ä¶\n$ total_value       &lt;dbl&gt; 62627, 1665, 396, 396, 396, 371680, 371680, 658000, ‚Ä¶\n$ control_type      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE‚Ä¶\n\n\n\n\nYou should see the tx object in your Environment tab. You also have the glimpse that gives you an idea of what each variable in the data is."
  },
  {
    "objectID": "sums-analysis.html#filter-for-controlled-items",
    "href": "sums-analysis.html#filter-for-controlled-items",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.4 Filter for controlled items",
    "text": "6.4 Filter for controlled items\nFor this assignment we want to focus on ‚Äúcontrolled‚Äù items vs the more generic non-controlled items we learned about in the documentation. Let‚Äôs filter to capture just the controlled data for our analysis.\nAs you might recall from the Billboard project, the filter() function is our workhorse for focusing our data. In our import notebook we created our control_type column so we could do exactly this: Find only the rows of ‚Äúcontrolled‚Äù items.\n\nStart a new Markdown section and note you are getting controlled items.\nStart with your tx data, but then filter it to control_type == TRUE.\nSave the result into a new tibble called tx_c.\n\n\ntx_c &lt;- tx |&gt; \n  filter(control_type == TRUE)\n\nAt this point you have a new tibble called tx_c that has only the weapons and other controlled property, so now we can take a closer look at that data."
  },
  {
    "objectID": "sums-analysis.html#building-summaries-with-math",
    "href": "sums-analysis.html#building-summaries-with-math",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.5 Building summaries with math",
    "text": "6.5 Building summaries with math\nAs we get into the first quest, let‚Äôs talk about ‚Äúhow‚Äù we go about these summaries.\nWhen I am querying my data, I start by envisioning what the result should look like. Let‚Äôs take the first question: How many total ‚Äúcontrolled‚Äù items were transferred, and what are they all worth?\nLet‚Äôs break this down:\n\n‚ÄúHow many total controlled items‚Äù is how many individual things were transferred. We have this quantity column that has the number of items in each row, so if we want the total for the data set, we can add together all the values in that column. We do this within a summarize() but instead of counting rows using n(), we‚Äôll use the function sum(quantity) which will add all the values in quantity column together.\n‚Äú‚Ä¶ what are they all worth‚Äù is very similar, but we want to add together all those values in our total_value column. (Remember, we don‚Äôt want to use acquisition_value because that is the value of only ONE item, not the total for the row.\n\n\n6.5.1 Summarize across the data\nSo, let‚Äôs put this together with code.\n\nStart a new Markdown section that you are getting total values.\nAdd a code chunk like below and run it.\n\n\ntx_c |&gt; \n  summarise(\n    sum(quantity),\n    sum(total_value)\n  )\n\n# A tibble: 1 √ó 2\n  `sum(quantity)` `sum(total_value)`\n            &lt;dbl&gt;              &lt;dbl&gt;\n1           22583         134445960.\n\n\nWalking through the code ‚Ä¶\n\nWe start with the tx_c tibble of the controlled items.\nThen we pipe into summarize(). Because we are going to add multiple things, I put them on separate lines just to make this more readable.\n\nYou‚Äôll notice that the names of the columns are the function names. We can ‚Äúname‚Äù our new columns just like we did in Billboard. We could call this whatever we want, but good practice is to name it what it is. We‚Äôll use good naming techniques and split the words using _. I also use all lowercase characters.\n\nEdit your chunk to add in the new column names and run it.\n\n\ntx_c |&gt; \n  summarise(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  )\n\n# A tibble: 1 √ó 2\n  summed_quantity summed_total_value\n            &lt;dbl&gt;              &lt;dbl&gt;\n1           22583         134445960.\n\n\nOK, from this we have learned something: Since Jan.¬†1, 2010, Texas law enforcement agencies have received more than 26,000 controlled items worth nearly $140 million.\nWe‚Äôre going to do this again below, so I‚Äôll dive deeper into explanations there.\n\n\n6.5.2 NA values in a sum, mean and median\nWhen we do math like this within summarize we need to take special note if our column has any blank values, called NA, as in not available. If there are, then you will get NA for the result. R will NOT do the math on the remaining values unless you tell it so. This is true not only for sum(), but also for mean() which gets an average, and for median() which finds the middle number in a column.\nThere is a way to get around this by including an argument within the mathy function: sum(col_name, na.rm = TRUE).\nI can show this with the demil_ic column which is a number datatype with some NA values. To be clear, the demil_ic variable isn‚Äôt really designed to do math on it as it is really a category, but it will show what I‚Äôm talking about here.\n\ntx_c |&gt; \n  summarise(\n    dumb_sum = sum(demil_ic),\n    less_dumb_sum = sum(demil_ic, na.rm = TRUE),\n    dumb_avg = mean(demil_ic),\n    less_dumb_avg = mean(demil_ic, na.rm = TRUE)\n  )\n\n# A tibble: 1 √ó 4\n  dumb_sum less_dumb_sum dumb_avg less_dumb_avg\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1       NA          7562       NA          1.41\n\n\nSo there you have examples of using sum() and mean() with and without na.rm = TRUE. OK, you‚Äôve been warned."
  },
  {
    "objectID": "sums-analysis.html#totals-by-agency",
    "href": "sums-analysis.html#totals-by-agency",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.6 Totals by agency",
    "text": "6.6 Totals by agency\nOK, your next question is this: For each agency, how many things did they get and how much was it all worth?\nThe key part of thinking about this logically is For each agency. That ‚Äúfor each‚Äù is a clue that we need group_by() for something. We basically need what we did above, but we first need to group_by the agency_name.\nLet‚Äôs break this question down:\n\n‚ÄúFor each agency‚Äù tells me I need to group_by the agency_name so I can summarize totals within each agency.\n‚Äúhow many total things‚Äù means how many items. Like before, we have the quantity variable, so we need to add all those together within summarize like we did above.\n‚Äúhow much was it worth‚Äù is another sum, but this time we want to sum the total_value column\n\nSo I envision my result looking like this:\n\n\n\nagency_name\nsummed_quantity\nsummed_total_value\n\n\n\n\nAFAKE POLICE DEPT\n6419\n10825707.5\n\n\nBFAKE SHERIFF‚ÄôS OFFICE\n381\n3776291.52\n\n\nCFAKE SHERIFF‚ÄôS OFFICE\n270\n3464741.36\n\n\nDFAKE POLICE DEPT\n1082\n3100420.57\n\n\n\nThe first columns in that summary will be our grouped values. This example is only grouping by one thing, agency_name. The other two columns are the summed values I‚Äôm looking to generate.\n\n6.6.1 Group_by, then summary with math\nWe‚Äôll start with the total_quantity.\n\nAdd a new section (headline, text and chunk) that describes the second quest: For each agency in Texas, find the summed quantity and summed total value of the equipment they received.\nAdd the code below into the chunk and run it.\n\n\ntx_c |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity)\n  )\n\n# A tibble: 335 √ó 2\n   agency_name                     summed_quantity\n   &lt;chr&gt;                                     &lt;dbl&gt;\n 1 ABERNATHY POLICE DEPT                         6\n 2 ALLEN POLICE DEPT                            11\n 3 ALVARADO ISD PD                               4\n 4 ALVIN POLICE DEPT                           478\n 5 ANDERSON COUNTY SHERIFFS OFFICE               7\n 6 ANDREWS COUNTY SHERIFF OFFICE                12\n 7 ANSON POLICE DEPT                             9\n 8 ANTHONY POLICE DEPT                          10\n 9 ARANSAS PASS POLICE DEPARTMENT               28\n10 ARCHER COUNTY SHERIFF OFFICE                  3\n# ‚Ñπ 325 more rows\n\n\nLet‚Äôs break this down a little.\n\nWe start with the tx_c, which is the ‚Äúcontrolled‚Äù data, and then ‚Ä¶\nWe group by agency_name. This organizes our data (behind the scenes) so our summarize actions will happen within each agency. Now I normally say run your code one line at a time, but you would not be able to see the groupings at this point, so I usually write group_by() and summarize() together.\nIn summarize() we first name our new column: summed_quantity, then we set that column to equal = the sum of all values in the quantity column. sum() is the function, and we feed it the column we want to add together: quantity.\nI put the inside of the summarize function in its own line because we will add to it. I enhances readability. RStudio will help you with the indenting, etc.\n\nIf you look at the first line of the return, it is taking all the rows for the ‚ÄúABERNATHY POLICE DEPT‚Äù and then adding together all the values in the quantity field.\nIf you wanted to test this (and that is a real good idea), you might look at the data from one of the values and check the math. Here are the Abernathy rows. I usually do these tests in a code chunk of their own, and sometimes I delete them after I‚Äôm sure my logic works.\n\ntx_c |&gt; \n  filter(agency_name == \"ABERNATHY POLICE DEPT\")\n\n# A tibble: 2 √ó 13\n  state agency_name  nsn   item_name quantity ui    acquisition_value demil_code\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     \n1 TX    ABERNATHY P‚Ä¶ 2320‚Ä¶ TRUCK,UT‚Ä¶        1 Each              62627 C         \n2 TX    ABERNATHY P‚Ä¶ 1240‚Ä¶ SIGHT,RE‚Ä¶        5 Each                333 D         \n# ‚Ñπ 5 more variables: demil_ic &lt;dbl&gt;, ship_date &lt;dttm&gt;, station_type &lt;chr&gt;,\n#   total_value &lt;dbl&gt;, control_type &lt;lgl&gt;\n\n\nIf we look at the quantity column there and eyeball all the rows, we see there 8 rows with a value of ‚Äú1‚Äù, and one row with a value of ‚Äú5‚Äù. 8 + 5 = 13, which matches our summed_quantity answer in our summary table. We‚Äôre good!\n\n\n6.6.2 Add the total_value\nWe don‚Äôt have to stop at one summary. We can perform multiple summarize actions on the same or different columns within the same expression.\nEdit your summary chunk to:\n\nAdd add a comma after the first summarize action.\nAdd the new expression to give us the summed_total_value and run it.\n\n\ntx_c |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  )\n\n# A tibble: 335 √ó 3\n   agency_name                     summed_quantity summed_total_value\n   &lt;chr&gt;                                     &lt;dbl&gt;              &lt;dbl&gt;\n 1 ABERNATHY POLICE DEPT                         6             64292 \n 2 ALLEN POLICE DEPT                            11           1404528 \n 3 ALVARADO ISD PD                               4               480 \n 4 ALVIN POLICE DEPT                           478           2436004.\n 5 ANDERSON COUNTY SHERIFFS OFFICE               7            733720 \n 6 ANDREWS COUNTY SHERIFF OFFICE                12              1476 \n 7 ANSON POLICE DEPT                             9              5329 \n 8 ANTHONY POLICE DEPT                          10              7490 \n 9 ARANSAS PASS POLICE DEPARTMENT               28            515396.\n10 ARCHER COUNTY SHERIFF OFFICE                  3           1101000 \n# ‚Ñπ 325 more rows\n\n\n\n\n6.6.3 Arrange the results\nOK, this gives us our answers, but in alphabetical order. We want to arrange the data so it gives us the most summed_total_value in descending order.\n\nEDIT your block to add an arrange() function below\n\n\ntx_c |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n# A tibble: 335 √ó 3\n   agency_name                       summed_quantity summed_total_value\n   &lt;chr&gt;                                       &lt;dbl&gt;              &lt;dbl&gt;\n 1 HOUSTON POLICE DEPT                          2384           7322064.\n 2 JEFFERSON COUNTY SHERIFFS OFFICE              206           3412547.\n 3 SAN MARCOS POLICE DEPT                        600           3200702.\n 4 DPS SWAT- TEXAS RANGERS                       137           3015221 \n 5 AUSTIN POLICE DEPT                           1392           2622087.\n 6 ALVIN POLICE DEPT                             478           2436004.\n 7 HARRIS COUNTY CONSTABLE PCT 3                 291           2321126.\n 8 MILAM COUNTY SHERIFF DEPT                      83           2196952.\n 9 HARRIS COUNTY SHERIFF'S OFFICE                 16           1834141 \n10 VAN ZANDT COUNTY SHERIFF'S OFFICE              45           1789636.\n# ‚Ñπ 325 more rows\n\n\nSo now we‚Äôve sorted the results to put the highest summed_total_value at the top.\nRemember, there are two ways we can set up that arrange() function in descending order:\n\narrange(summed_total_value |&gt; desc())\narrange(desc(summed_total_value))\n\nBoth work and are correct. It really is your preference.\n\n\n6.6.4 Consider the results\nIs there anything that sticks out in that list? It helps if you know a little bit about Texas cities and counties, but here are some thoughts to ponder:\n\nHouston is the largest city in the state (4th largest in the country, in fact). It makes sense that it tops the list. Same for Harris County or even the state police force. Austin being up there is also not crazy, as it‚Äôs almost a million people.\nBut what about San Marcos (pop. 63,220)? Or Milam County (pop. 24,770)? Those are way smaller cities and law enforcement agencies. They might be worth looking into.\n\nPerhaps we should look some at the police agencies closest to us."
  },
  {
    "objectID": "sums-analysis.html#looking-a-local-agencies",
    "href": "sums-analysis.html#looking-a-local-agencies",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.7 Looking a local agencies",
    "text": "6.7 Looking a local agencies\nOur second quest had a second part: How does this look for local police agencies?\nWe‚Äôll take the summary above, but then filter it to show only local agencies of interest.\n\n6.7.1 Save our ‚Äúby agency‚Äù list\nSince we want to take an existing summary and add more filtering to it, it makes sense to go back into that chunk and save it into a new object so we can reuse it.\n\nEDIT your existing summary chunk to save the result into a new tibble. Name it tx_agency_totals so we are all on the same page.\nAdd a new line that prints the result to the screen so you can still see it.\n\n\n# adding the new tibble object in next line\ntx_agency_totals &lt;- tx_c |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n# peek at the result\ntx_agency_totals\n\n# A tibble: 335 √ó 3\n   agency_name                       summed_quantity summed_total_value\n   &lt;chr&gt;                                       &lt;dbl&gt;              &lt;dbl&gt;\n 1 HOUSTON POLICE DEPT                          2384           7322064.\n 2 JEFFERSON COUNTY SHERIFFS OFFICE              206           3412547.\n 3 SAN MARCOS POLICE DEPT                        600           3200702.\n 4 DPS SWAT- TEXAS RANGERS                       137           3015221 \n 5 AUSTIN POLICE DEPT                           1392           2622087.\n 6 ALVIN POLICE DEPT                             478           2436004.\n 7 HARRIS COUNTY CONSTABLE PCT 3                 291           2321126.\n 8 MILAM COUNTY SHERIFF DEPT                      83           2196952.\n 9 HARRIS COUNTY SHERIFF'S OFFICE                 16           1834141 \n10 VAN ZANDT COUNTY SHERIFF'S OFFICE              45           1789636.\n# ‚Ñπ 325 more rows\n\n\nThe result is the same, but we can reuse the tx_agency_totals tibble.\n\n\n6.7.2 Filtering within a vector\nLet‚Äôs talk through the filter concepts before you try it with this data.\nWhen we talked about filtering with the Billboard project, we discussed using the | operator as an ‚ÄúOR‚Äù function. If we were to apply that logic here, it would look like this:\ndata |&gt; \n  filter(column_name == \"Text to find\" | column_name == \"More text to find\")\nThat can get pretty unwieldy if you have more than a couple of things to look for.\nThere is another operator %in% where we can search for multiple items from a list. (This list of terms is officially called a vector, but whatever.) Think of it like this in plain English: Filter the column for things in this list.\ndata |&gt; \n  filter(col_name %in% c(\"This string\", \"That string\"))\nWe can take this a step further by saving the items in our list into an R object so we can reuse that list and not have to type out all the terms each time we use them.\nlist_of_strings &lt;- c(\n  \"This string\",\n  \"That string\"\n)\n\ndata |&gt; \n  filter(col_name %in% list_of_strings)\n\n\n6.7.3 Create a vector to build this filter\nIn the interest of time, I‚Äôm going to give you the list of local police agencies to filter with. To be clear, I did considerable work to figure out the exact names of these agencies. I consulted a different data set that lists all law enforcement agencies in Texas and then I used some creative filtering to find their ‚Äúofficial‚Äù names in the leso data. It also helps that I‚Äôm familiar with local cities and counties so I can recognize the names. I don‚Äôt want to get sidetracked on that process here so I‚Äôll give you the list and show you how to use it.\n\nCreate a new section (headline, text and chunk) and describe you are filtering the summed quantity/values for local agencies.\nAdd the code below into that chunk.\n\n\nlocal_agencies &lt;- c(\n  \"AUSTIN PARKS POLICE DEPT\", #NI\n  \"AUSTIN POLICE DEPT\",\n  \"BASTROP COUNTY SHERIFF'S OFFICE\",\n  \"BASTROP POLICE DEPT\",\n  \"BEE CAVE POLICE DEPT\",\n  \"BUDA POLICE DEPT\",\n  \"CALDWELL COUNTY SHERIFFS OFFICE\",\n  \"CEDAR PARK POLICE DEPT\",\n  \"ELGIN POLICE DEPARTMENT\",\n  \"FLORENCE POLICE DEPT\", #NI\n  \"GEORGETOWN POLICE DEPT\",\n  \"GRANGER POLICE DEPT\", #NI\n  \"HAYS CO CONSTABLE PRECINCT 4\",\n  \"HAYS COUNTY SHERIFFS OFFICE\",\n  \"HUTTO POLICE DEPT\",\n  \"JARRELL POLICE DEPT\", #NI\n  \"JONESTOWN POLICE DEPT\", #NI\n  \"KYLE POLICE DEPT\",\n  \"LAGO VISTA POLICE DEPT\",\n  \"LAKEWAY POLICE DEPT\", #NI\n  \"LEANDER POLICE DEPT\",\n  \"LIBERTY HILL POLICE DEPT\", #NI\n  \"LOCKHART POLICE DEPT\",\n  \"LULING POLICE DEPT\",\n  \"MANOR POLICE DEPT\",\n  \"MARTINDALE POLICE DEPT\", #NI\n  \"PFLUGERVILLE POLICE DEPT\",\n  \"ROLLINGWOOD POLICE DEPT\", #NI\n  \"SAN MARCOS POLICE DEPT\",\n  \"SMITHVILLE POLICE DEPT\", #NI\n  \"SUNSET VALLEY POLICE DEPT\", #NI\n  \"TAYLOR POLICE DEPT\", #NI\n  \"THRALL POLICE DEPT\", #NI\n  # TEXAS STATE UNIVERSITY HI_ED\n  \"TRAVIS COUNTY SHERIFFS OFFICE\",\n  # TRAVIS CONSTABLE OFFICE,\n  # SOUTHWESTERN UNIVERSITY HI_ID\n  \"WESTLAKE HILLS POLICE DEPT\", #NI\n  \"UNIV OF TEXAS SYSTEM POLICE HI_ED\",\n  \"WILLIAMSON COUNTY SHERIFF'S OFFICE\"\n)\n\ntx_agency_totals |&gt; \n  filter(agency_name %in% local_agencies)\n\n# A tibble: 19 √ó 3\n   agency_name                        summed_quantity summed_total_value\n   &lt;chr&gt;                                        &lt;dbl&gt;              &lt;dbl&gt;\n 1 SAN MARCOS POLICE DEPT                         600           3200702.\n 2 AUSTIN POLICE DEPT                            1392           2622087.\n 3 UNIV OF TEXAS SYSTEM POLICE HI_ED                3           1305000 \n 4 LEANDER POLICE DEPT                            212           1182083 \n 5 GEORGETOWN POLICE DEPT                          41           1075807.\n 6 CALDWELL COUNTY SHERIFFS OFFICE                339            977096.\n 7 CEDAR PARK POLICE DEPT                         106            970222.\n 8 TRAVIS COUNTY SHERIFFS OFFICE                   78            896006.\n 9 BASTROP COUNTY SHERIFF'S OFFICE                263            712074.\n10 HAYS COUNTY SHERIFFS OFFICE                    384            442203.\n11 KYLE POLICE DEPT                               197            149673.\n12 WILLIAMSON COUNTY SHERIFF'S OFFICE             165             75460 \n13 LOCKHART POLICE DEPT                            16             54177.\n14 BEE CAVE POLICE DEPT                            38             51929.\n15 HUTTO POLICE DEPT                               90             13524.\n16 PFLUGERVILLE POLICE DEPT                         1             10747 \n17 BASTROP POLICE DEPT                             10              4990 \n18 LULING POLICE DEPT                              16              4700.\n19 BUDA POLICE DEPT                                16              1736.\n\n\nLet‚Äôs walk through that code and my notes there.\n\nWe start by giving our list of agencies a name: local_agencies. This creates an R object that we can reuse. We will need this list a number of times, and it makes sense to manage it in once place instead of each time we need it.\nNext we fill that object with a list of agency names. Again, I did some pre-work to figure out those names that we aren‚Äôt covering here, and in some cases there is a comment #NI next to them. That is a note to myself that the particular agency is NOT INCLUDED in our data, which means I haven‚Äôt confirmed the name spelling. It could be at a later date the Austin Parks department gets equipment, but they list their name as ‚ÄúCITY OF AUSTIN PARKS‚Äù instead of ‚ÄúAUSTIN PARKS POLICE DEPT‚Äù and it would not be filtered properly. This is another example that your most important audience for your code is your future self.\nNow that our list is created, we can use it to filter our tx_agency_totals data. So, we start with that data, and then ‚Ä¶\nWe pipe into filter(), but inside our filter we don‚Äôt set it == to a single thing, instead we say: agency_name %in% local_agencies, which says look inside that agency_name column and keep any row that has a value that is also in our local_agencies vector.\n\nThis filters our list of agencies to just those in Central Texas."
  },
  {
    "objectID": "sums-analysis.html#types-of-items-shipped-to-each-agency",
    "href": "sums-analysis.html#types-of-items-shipped-to-each-agency",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.8 Types of items shipped to each agency",
    "text": "6.8 Types of items shipped to each agency\nNow that we have an overall idea of what local agencies are doing, let‚Äôs dive a little deeper. It‚Äôs time to figure out the specific items that they received.\nOur question is this: What specific ‚Äúcontrolled‚Äù items did each agency get and how much were they worth?\nIn some cases an agency might get the same item shipped to them at different times. For instance, ‚ÄúAUSTIN POLICE DEPT‚Äù has multiple rows where they get a single ‚ÄúILLUMINATOR,INTEGRATED,SMALL ARM‚Äù shipped to them on the same date, but then on other dates they have the same item but the quantity is 30. We want all of these ‚ÄúILLUMINATOR,INTEGRATED,SMALL ARM‚Äù for the Austin police added together into a single record.\nThe logic works like this:\n\nStart with the controlled data, and then ‚Ä¶\nGroup by the agency_name and item_name, which will group all the rows where those values are the same. All ‚ÄúAUSTIN POLICE DEPT‚Äù rows with ‚ÄúILLUMINATOR,INTEGRATED,SMALL ARM‚Äù will be considered together, and then ‚Ä¶\nSummarize to sum the quantity, and then do the same for total_value.\n\nThe code for this is very similar to what we did above when we summaries agencies, except we are grouping by two things, the agency_name and the item_name. Let‚Äôs do it:\n\nCreate a new section (headline, text and code chunk) and describe that you are finding the sums for each item that each agency has received since 2010.\nConsult (or even copy) the code you wrote when you created the agency totals, but modify the group_by() to add the item_name, like this: group_by(agency_name, item_name).\nBe sure you rename your created R objects, too, perhaps to tx_agency_item_totals.\n\n\n\nGive it a go on your own first, then check here\n\n\n# adding the new tibble object in next line\ntx_agency_item_totals &lt;- tx_c |&gt; \n  group_by(agency_name, item_name) |&gt; \n  summarize(\n    summed_quantity = sum(quantity),\n    summed_total_value = sum(total_value)\n  ) |&gt; \n  arrange(summed_total_value |&gt; desc())\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n# peek at the result\ntx_agency_item_totals\n\n# A tibble: 1,548 √ó 4\n# Groups:   agency_name [335]\n   agency_name                      item_name summed_quantity summed_total_value\n   &lt;chr&gt;                            &lt;chr&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n 1 HOUSTON POLICE DEPT              AIRCRAFT‚Ä¶               1            5390000\n 2 DPS SWAT- TEXAS RANGERS          MINE RES‚Ä¶               4            2611000\n 3 DEPT OF CRIM JUSTICE OIG         TRUCK,CA‚Ä¶               4            1446516\n 4 UNIV OF TEXAS SYSTEM POLICE HI_‚Ä¶ MINE RES‚Ä¶               2            1228000\n 5 JEFFERSON COUNTY SHERIFFS OFFICE HELICOPT‚Ä¶               1             922704\n 6 ALVIN POLICE DEPT                HOIST,IN‚Ä¶               6             900420\n 7 VAN ZANDT COUNTY SHERIFF'S OFFI‚Ä¶ TRUCK,WR‚Ä¶               1             880674\n 8 BURKBURNETT POLICE DEPT          MINE RES‚Ä¶               1             865000\n 9 CLEBURNE POLICE DEPT             MINE RES‚Ä¶               1             865000\n10 CUERO POLICE DEPT                MINE RES‚Ä¶               1             865000\n# ‚Ñπ 1,538 more rows\n\n\n\n\nThis reuse of code like this ‚Äì copying the agency grouping code and editing it to add the item_name value ‚Äì is very common in coding, and there is nothing wrong with doing so as long as you are careful.\nWhen you reuse code, review it carefully so you don‚Äôt override things by accident. In this instance, our original code created an R object: tx_agency_totals &lt;- tx_c |&gt; ... that holds the result of our functions, and we call that later to view it. If we reuse this code and don‚Äôt update that object name, we‚Äôll reset the values inside that already-existing object, which was not our intent. We wabt to create a NEW thing tx_agency_item_totals so we can use that later, too. And if we don‚Äôt update the ‚Äúpeek‚Äù at the object, we‚Äôll be looking at the old one instead of the new one.\nOF NOTE: With that last code chunk you‚Äôll see a warning in your R Console: summarise() has grouped output by ‚Äòagency_name‚Äô. You can override using the .groups argument. This is not a problem, it‚Äôs just a reminder that when we group by more than one thing, the first grouping is retained when future functions are applied to this result. It‚Äôs more confusing than helpful, to be honest. Just know if we wanted to do further manipulation, we might need to use ungroup() first.\n\n6.8.1 Items for local agencies\nJust like we did for our agency totals, we want to filter this list of items to those sent to our local agencies. However, this time we‚Äôve already created the list of local agencies so we don‚Äôt have to redo that part ‚Ä¶ we just need to filter by it.\n\nStart a new section (headline, text) and explain that you are looking at items sent to local agencies.\nUse filter() to focus the data just on or local_agencies.\n\n\ntx_agency_item_totals |&gt; \n  filter(agency_name %in% local_agencies)\n\n# A tibble: 175 √ó 4\n# Groups:   agency_name [19]\n   agency_name                      item_name summed_quantity summed_total_value\n   &lt;chr&gt;                            &lt;chr&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n 1 UNIV OF TEXAS SYSTEM POLICE HI_‚Ä¶ MINE RES‚Ä¶               2           1228000 \n 2 AUSTIN POLICE DEPT               HELICOPT‚Ä¶               1            833400 \n 3 TRAVIS COUNTY SHERIFFS OFFICE    MINE RES‚Ä¶               1            767360 \n 4 CEDAR PARK POLICE DEPT           MINE RES‚Ä¶               1            733000 \n 5 GEORGETOWN POLICE DEPT           MINE RES‚Ä¶               1            733000 \n 6 LEANDER POLICE DEPT              MINE RES‚Ä¶               1            733000 \n 7 SAN MARCOS POLICE DEPT           MINE RES‚Ä¶               1            733000 \n 8 BASTROP COUNTY SHERIFF'S OFFICE  MINE RES‚Ä¶               1            658000 \n 9 SAN MARCOS POLICE DEPT           CAPABILI‚Ä¶               1            494724 \n10 AUSTIN POLICE DEPT               IMAGE IN‚Ä¶              85            458831.\n# ‚Ñπ 165 more rows\n\n\nBecause our original list arranged the data by the most expensive items, we can see that here. But it might be easier to rearrange the data by agency name first, then the most expensive items.\n\nEDIT your chunk to add an arrange function.\nWithin the arrange, set it to agency_name first, then summed_total_value in descending order.\n\n\ntx_agency_item_totals |&gt; \n  filter(agency_name %in% local_agencies) |&gt; \n  arrange(agency_name, desc(summed_total_value))\n\n# A tibble: 175 √ó 4\n# Groups:   agency_name [19]\n   agency_name        item_name               summed_quantity summed_total_value\n   &lt;chr&gt;              &lt;chr&gt;                             &lt;dbl&gt;              &lt;dbl&gt;\n 1 AUSTIN POLICE DEPT HELICOPTER,FLIGHT TRAI‚Ä¶               1            833400 \n 2 AUSTIN POLICE DEPT IMAGE INTENSIFIER,NIGH‚Ä¶              85            458831.\n 3 AUSTIN POLICE DEPT SIGHT,THERMAL                        29            442310 \n 4 AUSTIN POLICE DEPT PACKBOT 510 WITH FASTA‚Ä¶               4            308000 \n 5 AUSTIN POLICE DEPT SIGHT,REFLEX                        420            169256.\n 6 AUSTIN POLICE DEPT ILLUMINATOR,INTEGRATED‚Ä¶             135            122302 \n 7 AUSTIN POLICE DEPT RECON SCOUT XT                        8             92451.\n 8 AUSTIN POLICE DEPT TEST SET,NIGHT VISION ‚Ä¶               2             55610 \n 9 AUSTIN POLICE DEPT SCOPE,NIGHT-POCKET                    5             20535 \n10 AUSTIN POLICE DEPT POWER SUPPLY ASSEMBLY                63             20086.\n# ‚Ñπ 165 more rows\n\n\n\n\n6.8.2 Research some interesting items\nYou‚Äôll want a little more detail about some of these items for your data drop. I realize (and you should, too) that for a ‚Äúreal‚Äù story we would need to reach out to sources for more information, but you can get a general idea from what you find online to at least describe some of these items. There are a couple of ways to go about researching items.\n\nSimply search for the items on Google, like this.\nEach item has a ‚ÄúNational Stock Number,‚Äù which is an ID for a government database of supplies. You can search the data for the nsn value and then look up that value online.\n\nLet do an example, looking up ‚ÄúILLUMINATOR,INTEGRATED,SMALL ARMS‚Äù. First we filter the data to find the item. (I‚Äôm also using select so we can control the output and see it in this book, but you might not want that in your notebook so you can also check ship_dates, etc.)\n\ntx_c |&gt; \n  filter(\n    item_name == \"ILLUMINATOR,INTEGRATED,SMALL ARMS\",\n    agency_name == \"AUSTIN POLICE DEPT\"\n    ) |&gt; \n  select(item_name, nsn)\n\n# A tibble: 100 √ó 2\n   item_name                         nsn             \n   &lt;chr&gt;                             &lt;chr&gt;           \n 1 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 2 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 3 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-571-1258\n 4 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 5 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 6 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 7 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 8 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n 9 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n10 ILLUMINATOR,INTEGRATED,SMALL ARMS 5855-01-534-5931\n# ‚Ñπ 90 more rows\n\n\nIt looks like most of these illuminators use this nsn: 5855-01-534-5931.\nNow we go to the website https://nationalstocknumber.info/ and plug that number into the search bar. We get a couple of returns, and if we click on one we get this page. It gives us a description of the item:\n\n‚ÄúA device which is a combination of several lasers and white light illumination used to provide multiple capabilities for engaging targets and providing light. The device may contain a flashlight or other white light illumination source, an illuminator, infrared and stand alone aiming lasers/pointers. The device has the capability to mount on an individual weapon.‚Äù\n\nNot every item is in the database, but it is worth checking."
  },
  {
    "objectID": "sums-analysis.html#write-a-data-drop",
    "href": "sums-analysis.html#write-a-data-drop",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.9 Write a data drop",
    "text": "6.9 Write a data drop\nOnce you‚Äôve found answers to all the questions listed, you may be asked to use that information in a writing assignment. See Canvas for details."
  },
  {
    "objectID": "sums-analysis.html#what-we-learned-in-this-chapter",
    "href": "sums-analysis.html#what-we-learned-in-this-chapter",
    "title": "6¬† Summarize: math - analysis",
    "section": "6.10 What we learned in this chapter",
    "text": "6.10 What we learned in this chapter\n\nWe used sum() within a group_by()/summarize() function to add values within a column.\nWe used summary() to get descriptive statistics about our data, like the minimum and maximum values, or an average (mean).\nWe learned how to use c() to combine a list of like values into a vector, and then used that vector to filter a column for values %in% that vector."
  },
  {
    "objectID": "plots.html#goals-for-this-section",
    "href": "plots.html#goals-for-this-section",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.1 Goals for this section",
    "text": "7.1 Goals for this section\nIn this chapter, we‚Äôll learn the basics of data visualization using the Grammar of Graphics principles. We‚Äôll start with some smaller datasets to give you a sense of how the code works. And, in the next chapter, you‚Äôll apply this to a dataset we have already used in the class.\nOur learning goals are:\n\nTo learn about the Grammar of Graphics\nTo make scatterplots\nTo make bar charts"
  },
  {
    "objectID": "plots.html#introduction-to-ggplot",
    "href": "plots.html#introduction-to-ggplot",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.2 Introduction to ggplot",
    "text": "7.2 Introduction to ggplot\nggplot2 is the data visualization library within Hadley Wickham‚Äôs tidyverse. It is a beast of a package because it supports a whole variety of different types of data visualizations, from bar charts and line charts to fancy choropleth maps and animated figures.\nEven though the package is called ggplot2, the function to make graphs is just ggplot(). So, for simplicity, we‚Äôll just call everything ggplot\nThe ggplot package relies on a concept called the Grammar of Graphics, hence the gg in ggplot. The basic logic of the Grammar of Graphics is that any graph you could ever want to build will need similar things: a data set, some information about the scales of your variables, and the type of figure or graph that you want to create. These various things can be ‚Äúlayered‚Äù on top of each other to create a visually pleasing graph.\nFolks who have used Adobe creative programs (e.g., Photoshop, Illustrator, etc.) can think about it like laying an image: each layer in your image should do something to change the image. Likewise, each layer in a ggplot figure will add to the overall graph.\n\n7.2.1 What I like/dislike about ggplot\nLet me just start by saying that I‚Äôm a total ggplot geek. I‚Äôll talk about ggplot figures the way people talk about new TikTok trends. When producing figures and graphs in R, ggplot is the absolute best approach because you‚Äôll see the results right in your R notebook. And, basic data visualizations are an absolutely essential skill for any data journalist: it helps you find important things in your data that you may ultimately report on. So, ggplot is important for any R-based data journalism project.\nThat being said, there are less complicated ways of creating publishable graphics. Tools like Datawrapper and Flourish can produce equally beautiful graphics without the code. So why learn ggplot? Because, (1) ggplot is super useful when you‚Äôre just learning about the data and (2) to get good enough in ggplot to make publishable graphics, you have to practice, practice, practice. Yes, ggplot is a big package with lots of nuiance. But the more you take the time to learn it, the more you will master it.\n\n\n7.2.2 The Grammar of Graphics\nThis section was inspired by Matt Waite and the BBC Visual Cookbook.\nAs I said above, the gg in ggplot stands for ‚ÄúGrammar of Graphics,‚Äù which is a fancy way of saying we‚Äôll build our charts layer by layer. Once you know what data you are using, there are three main layers to any ggplot chart:\n\naesthetics (‚Äúaes‚Äù): this is where you put information about the dataset, including specifics about what fields/variables should be on the x-axis and y-axis.\ngeometries (‚Äúgeom‚Äù): this is where you tell R the shape of your visualization, whether it‚Äôs lines, bars, points, or something else.\nthemes (‚Äútheme‚Äù): this is where you tell R the font you‚Äôd like to use, the background color, and other things you want to ‚Äúpretty up‚Äù the data viz.\n\nIn addition to these three main layers, there are lots of helper layers we‚Äôll learn about along the way, including:\n\ncoord_flip: a special layer for flipping the chart\n\nscales: transforming the data to make the plot more read-able\n\nlabels (‚Äúlabs‚Äù): for making titles and labels\n\nfacets: For graphing many elements of the same data set in the same space (one dataset, multiple figures)\nThis all may seem complicated now, but it‚Äôll make sense once we start putting together these layers together, one at a time. After all, the best way to learn any R package is to do it"
  },
  {
    "objectID": "plots.html#start-a-new-project",
    "href": "plots.html#start-a-new-project",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.3 Start a new project",
    "text": "7.3 Start a new project\n\nGet into RStudio and make sure you don‚Äôt have any other files or projects open.\nCreate a new project, name it yourname-ggplot and save it in your rwd folder.\n(No need for a folder structure, we‚Äôll do this all in one file.)\nStart a new RMarkdown notebook and save it as 01-intro-ggplot.Rmd.\nRemove the boilerplate and create a setup section that loads library(tidyverse), like we do with every notebook. The ggplot package is a part of tidyverse, so when you load tidyverse, you‚Äôll load ggplot. We will also load janitor, which we will load to use the clean_names() function."
  },
  {
    "objectID": "plots.html#the-layers-of-ggplot",
    "href": "plots.html#the-layers-of-ggplot",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.4 The layers of ggplot",
    "text": "7.4 The layers of ggplot\n\nMuch of this first plot explanation comes from Hadley Wickham‚Äôs R for Data Science, with edits to fit the lesson here.\n\nTo explore how ggplot works, we‚Äôre going to work with some data that already comes with tidyverse. Let‚Äôs take a look at it now.\n\nStart a new section ‚ÄúFirst plot‚Äù and add a code chunk.\nAdd the code below and run it to see what the mpg dataset looks like.\n\n\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows\n\n\nThe mpg data contains observations collected by the US Environmental Protection Agency on 38 models of cars. It‚Äôs a data set embedded into the tidyverse for lessons like this one.\nFor this lesson, we‚Äôll use (at least two) two specific variables in this data:\n\ndispl, a car‚Äôs engine size, in liters.\nhwy, a car‚Äôs fuel efficiency on the highway, in miles per gallon (mpg).\n\nWith these two variables, we can test the theory that cars with smaller engines (displ) get better gas mileage (hwy). To do this, we‚Äôll make a plot.\n\n7.4.1 Build the base layer\nWhen working with the ggplot2 package, you‚Äôll start nearly every figure with the ggplot() function. In the ggplot() function, you‚Äôll tell R what data you‚Äôre using, and the coordinate system you want to build based on the data.\nThe first thing you‚Äôll want to do is tell ggplot the dataset you want to use (in this case, mpg). Let‚Äôs do that now.\nDo this:\n\nIn your first plot section, add some text that you are building the mpg chart.\nMake a new code chunk and add the code below.\n\n\nggplot(mpg)\n\n\n\n\nThis tells us‚Ä¶ absolutely nothing! But that‚Äôs not surprising: you haven‚Äôt even told ggplot what variables you want to focus on or the way you want to visualize the data. To do that, you‚Äôll need a second argument (and the first official layer in your plot): the aes() (short for ‚Äúaesthetic‚Äù). This is considered a mapping argument, because you use this argument to tell ggplot how you want to map your data (in our case, mpg).\nIn an aes() argument, you want to indicate the variables that you will be mapping to the x and y axes. This is usually done with the x and y arguments, so your aes() argument will look something like this: aes(x = some_variable, y = another_variable), where some_variable and another_variable are variables in your dataset.\nIn our case, we‚Äôll practice using displ (the car‚Äôs engine size) and hwy (the car‚Äôs highway efficiency). Let‚Äôs plug in our aes() layer now, directly into the ggplot() function.\nDo this:\n\nIn the code chunk you created above, add the following line of code.\n\n\nggplot(mpg, aes(x = displ, y = hwy))\n\n\n\n\nLet‚Äôs work through the code above:\n\nggplot() is the function we use to make a chart.\n\nThe first argument ggplot() needs is the data. It could be specified as data = mpg but we don‚Äôt need the data = part as it is always the first item specified inside of (or piped into) ggplot()\n\nNext is the aesthetics or aes(). This is where tell ggplot what data to plot on the x and y axis. You might see this as mapping = aes(&lt;VALUES&gt;) but we can often get by without the mapping = part.\n\nThis code tells us just a little bit more than nothing: instead of a blank box, we can now see the grid for the x and y axis. But we‚Äôll need another layer to add data to this grid!\nA quick FYI: The aes() that you put into ggplot() apply to the whole graph. Other (geom) layers that you write after this main layer can also take aes() arguments. We‚Äôll do that in future charts.\nNow that we have our ggplot() argument and first layer done, let‚Äôs talk about how to add layers to this.\n\n\n7.4.2 Layers can we add to our plots\nOur base layer is the starting point for every ggplot chart, but it‚Äôs certainly not the end. In the next section below, we‚Äôll work with three types of layers. Don‚Äôt worry if this seems like a lot of information: we‚Äôll go layer by layer so you can see the whole process.\nBelow are some of the layers we will work with:\n\ngeometries (or ‚Äúgeoms‚Äù as we call them) are the way we plot data on the base grid. There are many geoms, but here are a few common ones:\n\ngeom_point() adds dots onto the grid based on the data. Will will use these here to build a scatterplot graph.\ngeom_line() adds lines between data points on the grid. Basically a line chart.\ngeom_col() and geom_bars() adds bars to the grid based on values in the data. A bar chart. We‚Äôll use geom_col() later in this lesson but you can read about the difference between the two in a later chapter.\ngeom_text() adds labels based on values in the data.\n\nthemes change the visual styles of the grids and axis. There are several available within ggplot and many other from the R community.\nlabels (or labs, since we use the labs() function for them) are a series of text-based items we can layer onto our plots like titles, bylines and axis names.\n\nIn addition to these layers, we‚Äôll use the + at the end of each line. Think of the + as the %&gt;% of ggplot. So, your code will look something like this (don‚Äôt run this code chunk!):\n\nggplot(data, aes(x = some_variable, y = another_variable)) + #creates the base layer, with the + at the end\n  geom_layer #adds a geom\n\nOkay, now that we know what the code looks like, let‚Äôs proceed with the first geom.\n\n\n7.4.3 Add geom_point\nThe second layer we‚Äôll add to our figure is a geom layer. ‚ÄúGeom‚Äù is short for geometries: these layers provide a lot of different ways to shape and visualize the data. Simply put, geom layers tell R what kind of chart you‚Äôd like to make.\nLet‚Äôs start with a straightforward geom layer, geom_plot(), which adds a layer of points to your plot (this type of plot is called a scatterplot).\n\nEDIT your plot chunk to add the + and a new line for geom_point()\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + # don't forget the + at the end of this line\n  geom_point() # the geom_point layer\n\n\n\n#The geom_point() function will inherit the aes() values from the line above it.\n\nNow we‚Äôre starting to get somewhere! With the geom_point() layer added, our data are now finally displayed as points. And, as you can see, the pattern is pretty obvious: the lower the car‚Äôs displ (their engine size, in liters), the higher the hwy (their gas milage on a highway).\n\n\n7.4.4 Adding other mappings\nAs mentioned in the above section, there are aesthetics (aes) arguments that can apply to the plot as a whole (which we did with the x and y values above) and there are aesthetics we can write into specific geom layers (these aesthetics will not apply to other layers‚Äìjust that geom). This can be useful if you wanted to incorporate a third variable into your figure, as we will demonstrate below, using color.\n\nEdit your geom_point() function to add a color mapping to the points with aes(color = class). color is the type of aesthetic, and class is another variable (column) in the data.\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) # this is the line you are editing\n\n\n\n\nAs you can see, the dots were given colors based on the values in the class column, and ggplot also added a legend to the graphic. These colors are the default color settings in ggplot\nThere are other aesthetics you can use.\n\nChange the color aesthetic to one of these values and run it to see how it affects the chart: alpha, size and shape. (i.e., alpha = class.)\n\nOnce you‚Äôve tried them, change it back to color.\n\nOK, enough of the basics ‚Ä¶ let‚Äôs build a chart you might care about."
  },
  {
    "objectID": "plots.html#lets-build-a-bar-chart",
    "href": "plots.html#lets-build-a-bar-chart",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.5 Let‚Äôs build a bar chart",
    "text": "7.5 Let‚Äôs build a bar chart\nIn our first week of class, we sent out a survey where you told us your favorite Disney Princess and favorite flavor of ice cream. Let‚Äôs now play around with some of this data.\nFor this lesson, we‚Äôre not going to create a different notebook or download the data to our computer. Instead, we‚Äôre going to save the data directly into a tibble.\n\nStart a new section: Princess chart data upload.\nIn the text, note that we are importing the princess chart data.\nAdd the code below to get the data.\n\n\n# read the data and create an tibble object called \"class\"\nclass &lt;- read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQfwR6DBW5Qv6O5aEBFJl4V8itnlDxFEc1e_-fOAtBMDxXx1GeEGb8o5VSgi33oTYqeFhVCevGGbG5y/pub?gid=0&single=true&output=csv\")\n\nRows: 37 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): Name, Princess, Ice cream\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# peek at the data\nclass\n\n# A tibble: 37 √ó 3\n   Name      Princess   `Ice cream`        \n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;              \n 1 Abbie     Cinderella Chocolate          \n 2 Aislyn    Rapunzel   Coffee             \n 3 Ana       Ariel      Chocolate          \n 4 Ana Sofia Tiana      Cookie Dough       \n 5 Ardynne   Belle      Cookies and Cream  \n 6 Benton    Mulan      Coffee             \n 7 Cheney    Merida     Rocky Road         \n 8 Christy   Tiana      Rocky Road         \n 9 Corey     Mulan      Mint Chocolate Chip\n10 Darren    Mulan      Coffee             \n# ‚Ñπ 27 more rows\n\n\nSo, now, you should have the data in your environment.\n\n7.5.1 Prepare the data\nWhile there are ways for ggplot to calculate values from your data on the fly, I prefer to first build a table of the values I want, and then I will plot it on a chart. It‚Äôs helpful to think of these steps as separate so you have a good workflow (clean the data, prepare the data in a table form, and then plot the data).\nToday, our goal will be to make a bar chart, sometimes known as a column chart or histogram. This bar chart will show the number of votes for each princess from the data. So, we need to count the number of rows for each value ‚Ä¶ our typical group_by/summarize/arrange (GSA) process.\nFor this lesson, I‚Äôm going to use the count() shortcut, since we haven‚Äôt used it much lately. Next, I‚Äôll save the summarized data into a new dataframe called princess_data. Follow along in your notebook:\n\nAdd a section: Princess chart.\nAdd text that you are creating a data frame to plot.\nAdd the code below to create that data.\n\n\n#library(janitor) #run this if you have not loaded the janitor package!\n\nprincess_data &lt;- class %&gt;% \n  clean_names() %&gt;% #remember, this is from the janitor packae!\n  count(princess, name = \"votes\", sort = TRUE)\n  # this above line counts the princess rows, sets the name and sorts\n\n# peek at the data\nprincess_data\n\n# A tibble: 10 √ó 2\n   princess   votes\n   &lt;chr&gt;      &lt;int&gt;\n 1 Mulan         12\n 2 Belle          7\n 3 Pocahontas     4\n 4 Ariel          3\n 5 Rapunzel       3\n 6 Tiana          3\n 7 Jasmine        2\n 8 Aurora         1\n 9 Cinderella     1\n10 Merida         1\n\n\nAt this point, y‚Äôall should be plenty familiar with these summary functions, and the output should be easy to interpret: we‚Äôre just countin the number of rows for each princess.\nNow that we have our table data, let‚Äôs actually plot it.\n\n\n7.5.2 Build our plot with geom_col\nLike in the previous lesson, we‚Äôll start our plot by creating the first layer: the ggplot() function, which takes the data as its first argument and the aes() mapping layer as its second argument.\n\nAdd some text noting that you‚Äôll now plot.\nAdd the following code chunk, which is the first layer\n\n\nggplot(princess_data, aes(x = princess, y = votes)) # sets our x and y axes\n\n\n\n\nYou‚Äôll see the grid and x/y axis of the data, but no geometries are applied yet, so you won‚Äôt see any data. But remember, we‚Äôre adding these all in gradual layers.\n\n\n7.5.3 Add the geom_col layer\nNow it is time to add our columns. To do this, we‚Äôll use geom_col(). Similar to geom_point(), geom_col() adds a geometric layer that tells R how to display the data (in this case, with columns as opposed to points). Let‚Äôs write this code now.\n\nEdit the plot code to add the ggplot pipe + and on the next line add geom_col().\n\n\nggplot(princess_data, aes(x = princess, y = votes)) + # don't forget the + on this line\n  geom_col() # adds the bars\n\n\n\n\nOur two-layer chart is getting somewhere now. We‚Äôre able to see the data in the plot, but there are a couple issues:\n\nWe can‚Äôt read the value names. We can fix this.\nThe order of the bars is alphabetical instead of in vote order. Again, we can fix it.\n\n\n\n7.5.4 Flip the axes\nOne way to fix the labels is to ‚Äúflip‚Äù the axes, so the x axis becomes the y axis and vice versa. This is the equivalent of rotating the whole figure. When we do this, the axis will turn sideways, making it easier to read the labels. Worth noting: this can be a bit confusing later because the ‚Äúx‚Äù axis is now going up/down (as opposed to left and right).\nLet‚Äôs learn how to flip the axes now. We‚Äôll do this by adding a new layer, coord_flip(), which is a special layer that flips the axes. Just like we added the previous geom_col() layer using +, we‚Äôll do the same thing here. Let‚Äôs do that now.\n\nEdit your plot chunk to add the ggplot pipe +and coord_flip() on the next line.\n\n\nggplot(princess_data, aes(x = princess, y = votes)) +\n  geom_col() + # don't forget the +\n  coord_flip() # flips the axis\n\n\n\n\nAs you can see, rather than having vertical bars, we now have horizontal bars, and the names of each princess are fully displayed and read-able. Much better!\nBut the bars are still in an alphabetical order, as opposed to a vote order, so let‚Äôs fix that now.\n\n\n7.5.5 Reorder the bars\nThe bars on our chart are in alphabetical order of the x axis (and reversed thanks to our flip.) We want to order the values based on the votes in the data.\n\nComplication alert: Categorical data can have factors, which are like an internal ordering system. Some categories, like months in a year, have an ‚Äúorder‚Äù that is not alphabetical.\n\nWe can reorder our categorical values in a plot by editing the x values in our aes() using reorder(). (There is a tidyverse function called fct_reorder() that works the same way.\nreorder() takes two arguments: The column to reorder, and the column to base that reorder on. It can happen in two different ways, and I‚Äôll be honest and say I don‚Äôt know which is easier to comprehend.\n\nx = reorder(princess, votes) says ‚Äúset the x axis as princess, but order as votes. OR ‚Ä¶\nx = princess %&gt;% reorder(votes) says ‚Äúset the x axis as princess and then reorder by votes.\n\nThey both work. Even though I‚Äôm a fan of the tidyverse %&gt;% construct, I‚Äôm going with the first version.\n\nEdit the first line of your chunk to reorder the bars.\n\n\nggplot(princess_data, aes(x = reorder(princess, votes), y = votes)) + # this is the line you edit\n  geom_col() +\n  coord_flip()\n\n\n\n\nSo now, our princess names are read-able, and the bars are organized in vote size. But what if we wanted to be clearer in our figure, so that we knew the exact number of votes for each princess? Let‚Äôs learn how to add this information.\n\n\n7.5.6 Adding a geom_text layer\nNow, we‚Äôre really starting to take advance of the grammar of graphics by including more than one geometric layer. Specifically, we‚Äôll be using geom_text() to add some information to our bar charts.\nAs we mentioned previously, geom layers can take individual aesthetics (that build on top of the global aesthetics you put in the first layer). When using geom_text(), we‚Äôll include some local aesthetics using the aes() argument, to tell ggplot the label we‚Äôd like to add to the plot.\n\nEdit your plot chunk to add the ggplot pipe +and geom_text() on the next line.\nAdd the following line to the chunk geom_text(aes(label = votes).\n\n\nggplot(princess_data, aes(x = reorder(princess, votes), y = votes)) + \n  geom_col() +\n  coord_flip() + # don't forget +\n  geom_text(aes(label = votes)) # plots votes text values on chart\n\n\n\n\nWell that did‚Ä¶ something. We‚Äôve successfully added the numbers to this plot, but it‚Äôs not very pretty. First, the number sits at the end of the bar, making it harder to read. So we‚Äôll want to horizontally adjust this by shifting the numbers a bit to the left. Second, black text is really hard to read against a dark grey background. So we‚Äôll change the text of the number to white.\nWe can make both of these edits directly in the geom_text layer.\n\nEdit the last line of your plot chunk to add two new arguments.\nThe first argument you will add is hjust, which moves the text left. (hjust stands for horizontal justification. vjust, or vertical justification, would move it up and down).\nThe second argument you will add is color, which tells ggplot what the color of your text should be.\n\nAs a reminder, you should always separate your arguments within a function using commas (,).\n\nggplot(princess_data, aes(x = reorder(princess, votes), y = votes)) + \n  geom_col() +\n  coord_flip() + # don't forget +\n  geom_text(aes(label = votes), hjust = 2, color = \"white\") # plots read-able votes text values on chart\n\n\n\n\nGreat! But we‚Äôre still not done. Even though we‚Äôve added labels to each bar chart, we still haven‚Äôt added a title, and the titles of our x and y axes are not great. So let‚Äôs work on those now.\n\n\n7.5.7 Add some titles and more labels\nNow that we have a chart, with some information displayed in bars, flipped and arranged so we can see information, let‚Äôs add to this by giving the chart some labels. We‚Äôll do this by adding a layer of labels to our chart using the the labs() function. We can add and change a number of things with labs(), including creating a title, and changing the x and y axis titles.\n\nEdit the last line of your plot chunk to add the ggplot pipe + and labs() in the next line.\nAdd a title using the title = argument\nAdd a subtitle using the subtitle = argument. This is a great place to put information about your data (like when it was collected).\nAdd a caption using the caption = argument. Put your byline here!\nChange the x and y axes titles using x = and y =.\n\n\nggplot(princess_data, aes(x = reorder(princess, votes), y = votes)) + \n  geom_col() +\n  coord_flip() + # don't forget +\n  geom_text(aes(label = votes), hjust = 2, color = \"white\") + # plots votes text values on chart\n  # labs below has several settings\n  labs(\n    title = \"Favored princess\", # adds a title\n    subtitle = \"Disney Princess votes from Reporting with Data, Spring 2022.\", # adds a subtitle\n    caption = \"By Jo Lukito\", # adds the byline, replace this name with your own\n    x = \"Princess choices\", # renames the x axis label (which is really y since it is flipped)\n    y = \"Number of votes\" # renames the y axis label (which is really x since it is flipped)\n  )\n\n\n\n\nThere you go! You‚Äôve made a chart showing how our classes rated Disney Princesses."
  },
  {
    "objectID": "plots.html#on-your-own-ice-cream",
    "href": "plots.html#on-your-own-ice-cream",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.6 On your own: Ice cream!",
    "text": "7.6 On your own: Ice cream!\nNow it is time for you to put these skills to work:\n\nBuild a chart about the favorite ice creams from RWD classes.\n\nSome things to consider:\n\nYou need a new section, etc.\nYou‚Äôre starting with the same class data\nYou need to prepare the data based on ice_cream (which is the name of a variable in your class data frame)\nYou need to build the chart\n\nIt‚Äôs essentially the same process we used for the princess chart, but using ice_cream variable."
  },
  {
    "objectID": "plots.html#what-weve-learned",
    "href": "plots.html#what-weve-learned",
    "title": "7¬† Data Viz - Intro to ggplot",
    "section": "7.7 What we‚Äôve learned",
    "text": "7.7 What we‚Äôve learned\nThere is a ton, really.\n\nggplot2 (which is really the ggplot() function) is the charting library for the tidyverse. This whole lesson was about it.\n\nHere are some more references for ggplot:\n\nThe ggplot2 documentation and ggplot2 cheatsheets.\nR for Data Science, Chap 3. Hadley Wickam dives right into plots in his book.\nR Graphics Cookbook has lots of example plots. Good to harvest code and see how to do things.\nThe R Graph Gallery another place to see examples."
  },
  {
    "objectID": "tidy-data.html#goals-for-this-section",
    "href": "tidy-data.html#goals-for-this-section",
    "title": "8¬† Tidy data",
    "section": "8.1 Goals for this section",
    "text": "8.1 Goals for this section\n\nExplore what it means to have ‚Äútidy‚Äù data.\nLearn about and use pivot_longer(), pivot_wider() to shape our data for different purposes.\nUse candy data to practice shaping data."
  },
  {
    "objectID": "tidy-data.html#the-questions-well-answer",
    "href": "tidy-data.html#the-questions-well-answer",
    "title": "8¬† Tidy data",
    "section": "8.2 The questions we‚Äôll answer",
    "text": "8.2 The questions we‚Äôll answer\n\nAre candy colors evenly distributed within a standard package of M&M‚Äôs? (We‚Äôll get the mean of candies by color over a collection of packages.)\n\nWe‚Äôll plot the result as a column chart to show the average number of colored candies. We‚Äôll do it first in ggplot, then Datawrapper.\n\nBonus 1: Who got the most candies in their bag?\nBonus 2: What is the average number of candy in a bag?"
  },
  {
    "objectID": "tidy-data.html#what-is-tidy-data",
    "href": "tidy-data.html#what-is-tidy-data",
    "title": "8¬† Tidy data",
    "section": "8.3 What is tidy data",
    "text": "8.3 What is tidy data\n‚ÄúTidy‚Äù data is well formatted so each variable is in a column, each observation is in a row and each value is a cell. Our first step in working with any data is to make sure we are ‚Äútidy‚Äù.\n\nIt‚Äôs easiest to see the difference through examples. The data frame below is of tuberculosis reports from the World Health Organization.\n\nEach row is a set of observations (or case) from a single country for a single year.\nEach column describes a unique variable. The year, the number of cases and the population of the country at that time.\n\n\nTable2 below isn‚Äôt tidy. The count column contains two different type of values.\n\nWhen our data is tidy, it is easy to manipulate. We can use functions like mutate() to calculate new values for each case.\n\nWhen our data is tidy, we can use the tidyr package to reshape the layout of our data to suit our needs. It gets loaded with library(tidyverse) so we won‚Äôt need to load it separately.\n\n8.3.1 Wide vs long data\nIn the figure below, the table on the left is ‚Äúwide‚Äù. There are are multiple year columns describing the same variable. It might be useful if we want to calculate the difference of the values for two different years. It‚Äôs less useful if we want plot on a graphic because we don‚Äôt have a single ‚Äúyear‚Äù column to map as an x or y axes.\nThe table on the right is ‚Äúlong‚Äù, in that each column describes a single variable. It is this shape we need when we want to plot values on a chart in ggplot. We can then set our ‚ÄúYear‚Äù column as an x-axis, our ‚Äún‚Äù column on our y-axis, and group by the ‚ÄúCountry‚Äù.\n\n\n\nWide vs long\n\n\nNeither shape is wrong, they are just useful for different purposes. In fact, you‚Äôll find yourself pivoting the same data in different ways depending on your needs.\n\n\n8.3.2 Why we might want different shaped data\nThere are a myriad of reasons why you might need to reshape your data. Performing calculations on row-level data might be easier if it is wide. Grouping and summarizing calculations might be easier when it is long. ggplot graphics like long data, while Datawrapper sometimes wants wide data to make the same chart.\nI find myself shaping data back and forth depending on my needs."
  },
  {
    "objectID": "tidy-data.html#prepare-our-candy-project",
    "href": "tidy-data.html#prepare-our-candy-project",
    "title": "8¬† Tidy data",
    "section": "8.4 Prepare our candy project",
    "text": "8.4 Prepare our candy project\nWe will use candy count data we‚Äôve been collected in Reporting wth Data classes to explore this subject.\nStart a new project.\n\nCreate a new project and call it: yourname-candy\nNo need to create data folders as we‚Äôll just load data directly into the notebook.\nStart a new R Notebook and edit the headline. Name it 01-candy.\nCreate your setup block and load the libraries below.\n\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n8.4.1 Get the data\nWe‚Äôll just load this data directly from Google Sheets into this notebook. We‚Äôre doing something a little different here in we save the URL to our data into an object, then use that object in our read_csv() function. This is just a convenience for me, really, so I can swap that url out as needed.\n\nAdd a Markdown section noting that you are importing data.\nAdd this import chunk and run it.\n\n\n# save the url\nclass_data &lt;- \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRCGayKLOy-52gKmEoPOj3ZKnOQVtCiooSloiCr-i_ci27e4n1CMPL0Z9s6MeFX9oQuN9E-HCFJnWjD/pub?gid=1456715839&single=true&output=csv\"\n\n# read the data, clean names and save into the object \"raw_data\"\nraw_data &lt;- read_csv(class_data) |&gt; clean_names()\n\n# peek at the data\nraw_data\n\n# A tibble: 106 √ó 11\n   timestamp  first_name last_name candy_type box_code   red green orange yellow\n   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 2/21/2022‚Ä¶ Christian  McDonald  Plain      140BSCL‚Ä¶     2    17     11      4\n 2 2/28/2022‚Ä¶ Andrew     Logan     Plain      140BSCL‚Ä¶     2     9     14      4\n 3 2/28/2022‚Ä¶ Gabby      Ybarra    Plain      140BSCL‚Ä¶     4    11     14      0\n 4 2/28/2022‚Ä¶ Veronica   Apodaca   Plain      140BSCL‚Ä¶     1    17     16      2\n 5 2/28/2022‚Ä¶ Cristela   Jones     Plain      140BSCL‚Ä¶     3    19     14      3\n 6 2/28/2022‚Ä¶ Marina     Garcia    Plain      140BSCL‚Ä¶     7    11     13      1\n 7 2/28/2022‚Ä¶ Samuel     Stark     Plain      140BSCL‚Ä¶     3     9     10      5\n 8 2/28/2022‚Ä¶ Kevin      Malcolm ‚Ä¶ Plain      140BSCL‚Ä¶     4    15     13      6\n 9 2/28/2022‚Ä¶ Alexa      Haverlah  Plain      140BSCL‚Ä¶     5    15      6      2\n10 2/28/2022‚Ä¶ Zacharia   Washingt‚Ä¶ Plain      140BSCL‚Ä¶     1    15     14      1\n# ‚Ñπ 96 more rows\n# ‚Ñπ 2 more variables: blue &lt;dbl&gt;, brown &lt;dbl&gt;\n\n\nThis data comes from a Google Sheets document fed by a form that students have filled out, counting the colors of candies in a standard size bag of plain M&Ms.\n\n\n8.4.2 Drop unneeded columns\nFor this exercise we don‚Äôt need the timestamp, candy_type or box_code columns. We‚Äôll drop them so we can keep things simple.\n\nCreate a Markdown section noting you‚Äôll drop unneeded columns.\nCreate an R chunk and use select() to remove the columns noted above and save the result into a new data frame called candy.\n\nYou‚Äôve done this in the past, so you should be able to do it on your own.\n\n\nYou got this! (But, just in case ‚Ä¶)\n\n\ncandy &lt;- raw_data |&gt; \n  select(\n    -timestamp,\n    -candy_type,\n    -box_code\n  )\n\n\n\n\n8.4.3 Peek at the wide table\nLet‚Äôs look closer at this data:\n\ncandy |&gt; head()\n\n# A tibble: 6 √ó 8\n  first_name last_name   red green orange yellow  blue brown\n  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Christian  McDonald      2    17     11      4    16     4\n2 Andrew     Logan         2     9     14      4    17     9\n3 Gabby      Ybarra        4    11     14      0    19     7\n4 Veronica   Apodaca       1    17     16      2    13     8\n5 Cristela   Jones         3    19     14      3    13     3\n6 Marina     Garcia        7    11     13      1    14     6\n\n\nThis is pretty well-formed data. This format would be useful to create a ‚Äútotal‚Äù column for each bag, but there are better ways to do this with long data. Same with getting our averages for each color.\n\n\n8.4.4 Where are we going with this data\nWe have two goals here:\n\nFind the average distribution of candy by color, i.e., the average of each color column.\nWe want to chart the results as a bar chart simliar to the one below (which uses Skittles data).\n\n\nTo plot a chart like that in Datawrapper or ggplot, the data needs to be the same shape, like this:\n\n\n\nColor\nAverage\n\n\n\n\nGreen\n10.9\n\n\nOrange\n12.0\n\n\nPurple\n12.4\n\n\nRed\n11.4\n\n\nYellow\n12\n\n\n\nIt will be easier to accomplish both of these tasks if our data were in the long format.\nSo, instead of this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nred\ngreen\norange\nyellow\nblue\nbrown\n\n\n\n\nChristian\nMcDonald\n2\n17\n11\n4\n16\n4\n\n\n\nWe want this:\n\n\n\nfirst_name\nlast_name\ncolor\ncandies\n\n\n\n\nChristian\nMcDonald\nred\n2\n\n\nChristian\nMcDonald\ngreen\n17\n\n\nChristian\nMcDonald\norange\n11\n\n\nChristian\nMcDonald\nyellow\n4\n\n\nChristian\nMcDonald\nblue\n16\n\n\nChristian\nMcDonald\nbrown\n4"
  },
  {
    "objectID": "tidy-data.html#the-tidyr-verbs",
    "href": "tidy-data.html#the-tidyr-verbs",
    "title": "8¬† Tidy data",
    "section": "8.5 The tidyr verbs",
    "text": "8.5 The tidyr verbs\nThe two functions we‚Äôll use to reshape are data are:\n\npivot_longer() which ‚Äúlengthens‚Äù data, increasing the number of rows and decreasing the number of columns.\npivot_wider() which ‚Äúwidens‚Äù data, increasing the number of columns and decreasing the number of rows.\n\nAgain, the best way to learn this is to present a problem and then solve it with explanation."
  },
  {
    "objectID": "tidy-data.html#pivot-longer",
    "href": "tidy-data.html#pivot-longer",
    "title": "8¬† Tidy data",
    "section": "8.6 Pivot longer",
    "text": "8.6 Pivot longer\nThis visualization gives you an idea how pivot_longer() works.\n\n\n\nPivot longer\n\n\nEach column of data chosen (the colored ones) is turned into it‚Äôs own row of data. Supporting data (the grey columns) are duplicated.\nThe pivot_longer() function needs several arguments: cols=, names_to= and values_to. Below are two examples to pivot the example data shown above.\n\n\n\npivot_longer code\n\n\n\ncols= is where you define a range of columns you want to pivot. For our candy data we want the range red:brown.\nnames_to= allows you to name the new column filled by the column names. For our candy data we want to name this ‚Äúcolor‚Äù since that‚Äôs what those columns described.\nvalues_to= allows you to name the new column filled with the cell data. For us we want to call this ‚Äúcandies‚Äù since these are the number of candies in each bag.\n\nThere are a number of ways we can describe the cols= argument ‚Ä¶ anything in tidy-select works. You can see a bunch of examples here.\n\n8.6.1 Pivot our candy data longer\nWhat we want here is six rows for each person‚Äôs entry, with a column for ‚Äúcolor‚Äù and a column for ‚Äúcandies‚Äù.\nWe are using a range, naming the first ‚Äúred‚Äù and the last column ‚Äúbrown‚Äù with : in between. This only works because those columns are all together. We could also use cols = !c(first_name, last_name) to say everything but those two columns.\n\nAdd a note that you are pivoting the data\nAdd the chunk below and run it\n\n\ncandy_long &lt;- candy |&gt; \n  pivot_longer(\n    cols = red:brown, # sets which columns to pivot based on their names\n    names_to = \"color\", # sets column name for color\n    values_to = \"candies\" # sets column name for candies\n  )\n\ncandy_long |&gt; head()\n\n# A tibble: 6 √ó 4\n  first_name last_name color  candies\n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n1 Christian  McDonald  red          2\n2 Christian  McDonald  green       17\n3 Christian  McDonald  orange      11\n4 Christian  McDonald  yellow       4\n5 Christian  McDonald  blue        16\n6 Christian  McDonald  brown        4\n\n\n\n\n8.6.2 Get average candies per color\nTo get the average number of candies per each color, we can use our candy_long data and group_by color (which will consider all the red rows together, etc.) and use summarize() to get the mean.\nThis is very similar to the sum()s we did with military surplus, but you use mean() instead. There is one trick with mean(), though, in that sometimes you have to add an argument na.rm = TRUE if there are missing or zero values, since you can‚Äôt divide by zero. We have that case here in some bags don‚Äôt have ANY candies of a color at all!\nSave the resulting summary table into a new tibble called candy_avg.\n\ncandy_avg &lt;- candy_long |&gt; \n  group_by(color) |&gt; \n  summarize(avg_candies = mean(candies, na.rm = TRUE))\n  # the na.rm bit takes into account some records where there are zero or blank values.\ncandy_avg\n\n# A tibble: 6 √ó 2\n  color  avg_candies\n  &lt;chr&gt;        &lt;dbl&gt;\n1 blue         12.6 \n2 brown         6.34\n3 green        12.6 \n4 orange       10.1 \n5 red           5.62\n6 yellow        8.31\n\n\n\n\n8.6.3 Round the averages\nLet‚Äôs modify this summary to round the averages to tenths so they will plot nicely on our chart.‚Äô\nThe round() function needs the column to change, and then the number of digits past the decimal to include.\n\nEdit your summarize function to add the mutate() functin below.\n\n\ncandy_avg &lt;- candy_long |&gt; \n  group_by(color) |&gt; \n  summarize(avg_candies = mean(candies, na.rm = TRUE)) |&gt; \n  mutate(\n    avg_candies = round(avg_candies, 1)\n  )\n\ncandy_avg\n\n# A tibble: 6 √ó 2\n  color  avg_candies\n  &lt;chr&gt;        &lt;dbl&gt;\n1 blue          12.6\n2 brown          6.3\n3 green         12.6\n4 orange        10.1\n5 red            5.6\n6 yellow         8.3\n\n\nBONUS POINT OPPORTUNITY: Using a similar method to rounding above, you can also capitalize the names of the colors. You don‚Äôt have to do this, but I‚Äôll give you bonus points if you do:\n\nIn your mutate, add a rule that updates color column using str_to_title(color).\n\nYou can read more about converting the case of a string here. It‚Äôs part of the stringr package, which is loaded with tidyverse.\n\n\n8.6.4 On your own: Plot the averages\nNow I want you to use ggplot to create a bar chart that shows the average number of candies by color in a bag. This is very similar to the Disney Princesses bar chart in Chapter 7.\n\nBuild a bar chart of average color using ggplot.\n\nSome things to consider:\n\nI want the bars to be ordered by the highest average on top.\nI want you to have a good title, subtitle and byline, along with good axes names. Make sure a reader has all the information they need to understand what you are communicating with the chart.\nInclude the values on the bars.\nChange the theme to something other than the default.\n\nHere is what it should more or less look like, but with good text, etc:\n\n\n\n\n\nThe numbers in the example above may not be up to date, so don‚Äôt let that throw you."
  },
  {
    "objectID": "tidy-data.html#introducing-datawrapper",
    "href": "tidy-data.html#introducing-datawrapper",
    "title": "8¬† Tidy data",
    "section": "8.7 Introducing Datawrapper",
    "text": "8.7 Introducing Datawrapper\nThere are some other great charting tools that journalists use. My favorite is Datawrapper and it is free for the level you need it.\nDatawrapper is so easy I don‚Äôt even have to teach you how to use it. They have excellent tutorials.\nWhat you do need is the data to plot, but you‚Äôve already ‚Äúshaped‚Äù it the way you need it. Your candy_avg tibble is what you need.\nHere are the steps I want you to follow:\n\n8.7.1 Review how to make a bar chart\n\nIn a web browser, go to the Datawrapper Academy\nClick on Bar charts\nChoose How to create a bar chart\n\nThe first thing to note there is they show you what they expect the data to look like. Your candy_avg tibble is just like this, but with Color and Candies.\nYou‚Äôll use these directions to create your charts so you might keep this open in its own tab.\n\n\n8.7.2 Start a chart\n\nIn a new browser tab, go to datawrapper.de and click the big Start creating button.\nUse the Login/Sign Up button along the top to create an account or log in if you have one.\nThe first screen you have is where you can Upload data or paste it into the window. We are going to upload the data, but we have to write out the data to your computer first.\n\n\n\n8.7.3 Export your data for Datawrapper\n\nGo back to RStudio.\nCreate a new block and name it something about exporting\nTake your data and pipe it into write_csv(). As an argument to write_csv(), give it a path into your data-processed folder, and name the file candy_avg.csv.\n\n\ncandy_avg |&gt; write_csv(\"data-processed/candy_avg.csv\")\n\nThis will save your data file onto your computer in your project folder.\n\nReturn to Datawrapper in your browser and click on the XLS/CSV upload button and go find your file to import it.\n\n\n8.7.3.1 posit.cloud export\nIf you are using posit.cloud, you will also need to Export your data from the cloud to get it onto your computer so you can import it into Datawrapper. Do this only if you are using the cloud version.\n\nIn the Files window (bottom right) go inside your data-processed folder.\nClick on the checkbox next to the candy_avg.csv file.\nClick on the More blue gear thing and choose Export.\nYou‚Äôll get a prompt to name the file (which you can keep the same) and then a download button. Click the button.\nThe file should go to your Downloads folder like anything else you download from the Internet.\nWhen you go back to Datawrapper click on the XLS/CSV upload button and go find your file to import.\n\n\n\n8.7.3.2 An alternative export: ‚Äúcopy‚Äù your data\n\nThis is an alternative to exporting your data, but it only works in the desktop version of RStudio. It does NOT work for the online posit.cloud version of Rstudio. You either EXPORT or WRITE_CLIP, but you don‚Äôt need to do both.\n\nTo do this, you‚Äôll need to install a package called clipr.\n\nIn your R project in the R Console install the package clipr: install.packages(\"clipr\"). Do NOT save this code inside your notebook.\nStart a section that says you are going to get data for Datawrapper.\nCreate a chunk with the following and run it.\n\n\nlibrary(clipr)\n\ncandy_avg |&gt; write_clip(allow_non_interactive = TRUE)\n\nYou won‚Äôt see anything happen, but all the data in candy_avg has been added to your clipboard as if you highlighted it and did Copy. You must have the allow_non_interactive = TRUE argument to allow your RMarkdown document to knit.\n\nReturn to your browser where you are making the chart, put your cursor into the ‚ÄúPaste your copied data here ‚Ä¶‚Äù window and paste. (Like Cmd-V or use the menu Edit &gt; Paste.)\n\n\n\n\n8.7.4 Build the datawrapper graphic\n\nOnce your data is either uploaded or copied into Datawrapper, you can click Proceed to go to the next step.\n\nYou can now follow the Datawrapper Academy directions to finish your chart.\nWhen you get to the Publish & Embed window, I want you to click the Publish Now button and then add the resulting Link to your visualization: URL to your R Notebook so I can find it for grading."
  },
  {
    "objectID": "tidy-data.html#pivot-wider",
    "href": "tidy-data.html#pivot-wider",
    "title": "8¬† Tidy data",
    "section": "8.8 Pivot wider",
    "text": "8.8 Pivot wider\nIn this case we don‚Äôt have a real need to pivot our data wider, but I‚Äôd like to show you how it is done.\nAs you can imagine, pivot_wider() does the opposite of pivot_longer(). When we pivot wider we move our data from a ‚Äúlong‚Äù format to a ‚Äúwide‚Äù format. We create a new column based categories and values in the data.\n\n\n\nLong to wide\n\n\nWe‚Äôll practice this by taking our long candy data and pivot it so there is a column for each person in the data.\npivot_wider() needs two arguments:\n\nnames_from = lets us define from which column (or columns) we are pulling values from to create the new column names. In our case, we need two columns to combine the first and last names. We can do that with c(first_name, last_name).\nvalues_from = lets us say which column will be the values in the new arrangement. In our case, we want the candies column.\n\n\ncandy_long |&gt; \n  pivot_wider(names_from = c(first_name, last_name), values_from = candies)\n\n# A tibble: 6 √ó 107\n  color  Christian_McDonald Andrew_Logan Gabby_Ybarra Veronica_Apodaca\n  &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n1 red                     2            2            4                1\n2 green                  17            9           11               17\n3 orange                 11           14           14               16\n4 yellow                  4            4            0                2\n5 blue                   16           17           19               13\n6 brown                   4            9            7                8\n# ‚Ñπ 102 more variables: Cristela_Jones &lt;dbl&gt;, Marina_Garcia &lt;dbl&gt;,\n#   Samuel_Stark &lt;dbl&gt;, `Kevin_Malcolm Jr` &lt;dbl&gt;, Alexa_Haverlah &lt;dbl&gt;,\n#   Zacharia_Washington &lt;dbl&gt;, Jose_Martinez &lt;dbl&gt;, Ana_Garza &lt;dbl&gt;,\n#   Carolina_Cruz &lt;dbl&gt;, Anissa_Reyes &lt;dbl&gt;, Alaina_Bookman &lt;dbl&gt;,\n#   Bryan_Baker &lt;dbl&gt;, Mckenna_Lucas &lt;dbl&gt;, Marissa_DeLeon &lt;dbl&gt;,\n#   Claire_Stevens &lt;dbl&gt;, Katy_Vanatsky &lt;dbl&gt;, Vicente_Montalvo &lt;dbl&gt;,\n#   Eric_Seow &lt;dbl&gt;, Brandon_Jenkins &lt;dbl&gt;, Luke_Skywalker &lt;dbl&gt;, ‚Ä¶\n\n\n\n8.8.1 Pivot wider on your own\nNow I want you do apply the same pivot_wider() function to that same candy_long data, but to have the rows be people and the column be each color, basically like how our data started. (But, of course, I want to acdtually use pivot_wider() to do it!)\n\nStart a new section and note this is pivot_wider on your own.\nStart with the candy_long data, and then ‚Ä¶\nUse pivot_wider() to make the data shaped like this\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nred\ngreen\norange\nyellow\nblue\nbrown\n\n\n\n\nChristian\nMcDonald\n2\n17\n11\n4\n16\n4"
  },
  {
    "objectID": "tidy-data.html#bonus-questions",
    "href": "tidy-data.html#bonus-questions",
    "title": "8¬† Tidy data",
    "section": "8.9 Bonus questions",
    "text": "8.9 Bonus questions\nMore opportunities for bonus points on this assignment. These aren‚Äôt plots, just data wrangling to find answers.\n\n8.9.1 Most/least candies\nAnswer me this: Who got the most candies in their bag? Who got the least?\nI want a well-structured section (headline, text) with two chunks, one for the most and one for the least.\n\n\n8.9.2 Average total candies in a bag\nAnswer me this: What is the average number of candy in a bag?\nAgain, well-structured section and include the code.\nHint: You need a total number of candies per person before you can get an average."
  },
  {
    "objectID": "tidy-data.html#turn-in-your-work",
    "href": "tidy-data.html#turn-in-your-work",
    "title": "8¬† Tidy data",
    "section": "8.10 Turn in your work",
    "text": "8.10 Turn in your work\n\nMake sure your notebook runs start-to-finish.\nKnit the notebook\nStuff your project and turn it into the Candy assignment in Canvas."
  },
  {
    "objectID": "tidy-data.html#what-we-learned",
    "href": "tidy-data.html#what-we-learned",
    "title": "8¬† Tidy data",
    "section": "8.11 What we learned",
    "text": "8.11 What we learned\n\nWe learned what ‚Äútidy data‚Äù means and why it is important. It is the best shape for data wrangling and plotting.\nWe learned about pivot_longer() and pivot_wider() and we used pivot_longer() on our candy data.\nWe also used round() to round off some numbers, and you might have used str_to_title() to change the case of the color values."
  },
  {
    "objectID": "bind-join.html#goals-of-the-chapter",
    "href": "bind-join.html#goals-of-the-chapter",
    "title": "9¬† Bind and join",
    "section": "9.1 Goals of the chapter",
    "text": "9.1 Goals of the chapter\n\nMerge multiple data files with bind_rows()\nJoin data frames with inner_join()\nUse str_remove() to clean data\nIntroduce if_else() for categorization\n\nWe‚Äôll use the results of this chapter in our next one."
  },
  {
    "objectID": "bind-join.html#the-story-an-update-to-denied",
    "href": "bind-join.html#the-story-an-update-to-denied",
    "title": "9¬† Bind and join",
    "section": "9.2 The story: An update to Denied",
    "text": "9.2 The story: An update to Denied\nIn 2016, the Houston Chronicle published a multi-part series called Denied that outlined how a Texas Education Agency policy started in 2004 could force an audit of schools that have more than 8.5% of their students in special education programs. The story was a Pulitzer Prize finalist. Here‚Äôs an excerpt:\n\nOver a decade ago, the officials arbitrarily decided what percentage of students should get special education services ‚Äî 8.5% ‚Äî and since then they have forced school districts to comply by strictly auditing those serving too many kids.\n\n\nTheir efforts, which started in 2004 but have never been publicly announced or explained, have saved the Texas Education Agency billions of dollars but denied vital supports to children with autism, attention deficit hyperactivity disorder, dyslexia, epilepsy, mental illnesses, speech impediments, traumatic brain injuries, even blindness and deafness, a Houston Chronicle investigation has found.\n\n\nMore than a dozen teachers and administrators from across the state told the Chronicle they have delayed or denied special education to disabled students in order to stay below the 8.5 percent benchmark. They revealed a variety of methods, from putting kids into a cheaper alternative program known as ‚ÄúSection 504‚Äù to persuading parents to pull their children out of public school altogether.\n\nFollowing the Chronicle‚Äôs reporting (along with other news orgs), the Texas Legislature in 2017 unanimously banned using a target or benchmark on how many students a district or charter school enrolls in special education.\nWe want to look into the result of this reporting based on three things:\n\nHas the percentage of special education students in Texas changed since the benchmarking policy was dropped?\nHow many districts were above that arbitrary 8.5% benchmark before and after the changes?\nHow have local districts changed?\n\nTo prepare for this:\n\nRead Part 1 of the original Denied series.\nRead About this series\nRead this followup about the legislative changes."
  },
  {
    "objectID": "bind-join.html#about-the-data",
    "href": "bind-join.html#about-the-data",
    "title": "9¬† Bind and join",
    "section": "9.3 About the data",
    "text": "9.3 About the data\nEach year, the Texas Education Agency publishes the percentage of students in special education as part of their Texas Academic Performance Reports. We can download a file that has the percentages for each district in the state.\nThere are some challenges, though:\n\nWe have to download each year individually. There are nine years of data.\nThe are no district names in the files, only a district ID. We can get a reference file, though.\nThere are some differences in formatting for some files.\n\nI will save you the hassle of going through the TAPR database to find and download the individual files, and I will also supply code to clean the files to make them consistent. I‚Äôll try not to get lost in the weeds along the way.\n\n9.3.1 Set up your project\n\n9.3.1.1 If you are a posit.cloud user\n\nFrom your posit.cloud account, go to this shared project\nClick Save a permanent copy so you have your own version.\nRename the project yourname-sped but use your name.\nYou can skip to the Open, read and run section below.\n\n\n\n9.3.1.2 If you are using RStudio Desktop\n\nGo to this page. Yes, it looks like you can‚Äôt see anything, but ‚Ä¶\nLook for the Download button and download the zip file.\nFind that on your computer and uncompress it.\nRename the folder to yourname-sped but use your name.\nMove the folder to your rwd folder or wherever you‚Äôve been saving your class projects.\nIn RStudio, choose File &gt; New Project. Choose EXISTING Directory at the next step and then find the folder you just moved. Use that to create your project.\n\n\n\n\n9.3.2 Open, read and run\nOnce you have your project set up ‚Ä¶\n\nOpen up the 01-import notebook.\nRead every line\nRun each chunk as you do.\n\nGo ahead. I‚Äôll wait.\n\n\n\n\n\nvia GIPHY\n\nThere is a lot to take in there about where the data came from and how we dealt with it. Here is where you end up:\n\n\nYou have 10 data files for each year and one reference file imported."
  },
  {
    "objectID": "bind-join.html#merging-data-together",
    "href": "bind-join.html#merging-data-together",
    "title": "9¬† Bind and join",
    "section": "9.4 Merging data together",
    "text": "9.4 Merging data together\nOK, so we have all these different yearly files. Wouldn‚Äôt it be a lot easier if these were ONE thing? Indeed, we can merge these files together by stacking them on top of each other. Let‚Äôs review the concept using Starburst data:\n\n\nHere‚Äôs another representation of the concept. You have two data sets and you stack them on top of each other where the column names match. (Note here that identical rows in both data sets remain).\n\nSince all of our data files have the same column names, we can easily merge them with function bind_rows().\nLet‚Äôs demonstrate through building it.\n\nStart a new section on your R Markdown document and note you are merging data\nAdd a chunk with just the dstud13 data and run it.\n\n\ndstud13\n\n# A tibble: 1,228 √ó 5\n   district year  dpetallc dpetspec dpetspep\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 001902   2013       595       73     12.3\n 2 001903   2013      1236      113      9.1\n 3 001904   2013       751       81     10.8\n 4 001906   2013       406       45     11.1\n 5 001907   2013      3288      252      7.7\n 6 001908   2013      1619      151      9.3\n 7 001909   2013       439       48     10.9\n 8 002901   2013      3617      250      6.9\n 9 003801   2013       635       45      7.1\n10 003902   2013      2726      196      7.2\n# ‚Ñπ 1,218 more rows\n\n\nThe result shows there are 1228 rows and 5 variables in the data, which should match what shows for dstud13 in your Environment tab.\n\nNow edit that chunk to use bind_rows() with dstud14 and run it.\n\n\ndstud13 |&gt; \n  bind_rows(dstud14)\n\n# A tibble: 2,455 √ó 5\n   district year  dpetallc dpetspec dpetspep\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 001902   2013       595       73     12.3\n 2 001903   2013      1236      113      9.1\n 3 001904   2013       751       81     10.8\n 4 001906   2013       406       45     11.1\n 5 001907   2013      3288      252      7.7\n 6 001908   2013      1619      151      9.3\n 7 001909   2013       439       48     10.9\n 8 002901   2013      3617      250      6.9\n 9 003801   2013       635       45      7.1\n10 003902   2013      2726      196      7.2\n# ‚Ñπ 2,445 more rows\n\n\nThis shows we now have 2455 rows and 5 variables. This is good ‚Ä¶ we‚Äôve addded the rows of dstud14 but we don‚Äôt have any new columns because the column names were identical.\nNow edit the chunk to do all these things:\n\nWithin the bind_rows() function, also add the dstud15 dataframe so you can see you are adding more on.\nSave the result of the merge into a new data frame called sped_merged.\nAt the bottom of the chunk print out the sped_merged tibble and pipe it into count(year) so you can make sure you continue to add rows correctly.\n\nIt should look like this:\n\nsped_merged &lt;- dstud13 |&gt; \n  bind_rows(\n    dstud14,\n    dstud15\n  )\n\n# we use this to ensure we bind correctly when we add new years\nsped_merged |&gt; count(year)\n\n# A tibble: 3 √ó 2\n  year      n\n  &lt;chr&gt; &lt;int&gt;\n1 2013   1228\n2 2014   1227\n3 2015   1219\n\n\n(In the screencast video you might also see each new datatable added in their own bind_rows() function instead of all in one. Either works.)\nWe are NOT saving the count() result into a new object; We are just printing it to our screen to make sure we get all the years.\nNow that we know this is working, you‚Äôll finish this out on your own.\n\nEdit your chunk to add bind_rows() for the rest of the files dstud16 through dstud22. You just keep tacking them on like we did with dstud15.\nAfter you are done, make sure you look at the sped_merged listing in your environment to make sure you end up with a count for each year of data.\n\nOK, we have all our data in one file, but we still don‚Äôt know the district names. It‚Äôs time to Join our data with our reference file."
  },
  {
    "objectID": "bind-join.html#about-joins",
    "href": "bind-join.html#about-joins",
    "title": "9¬† Bind and join",
    "section": "9.5 About joins",
    "text": "9.5 About joins\nOK, before we go further we need to talk about joining data. It‚Äôs one of those Basic Data Journalism Functions ‚Ä¶\n\n\nWhat joins do is match rows from two data sets that have a column of common values, like an ID or county name. (The district ID column in our case). Columns from the second data set will be added based on where the ID‚Äôs match.\nThere are several types of joins. We describe these as left vs right based on which table we reference first (which is the left one). How much data you end up with depends on the ‚Äúdirection‚Äù of the join.\n\nAn inner_join puts together columns from both tables where there are matching rows. If there are records in either table where the IDs don‚Äôt match, they are dropped.\nA left_join will keep ALL the rows of your first table, then bring over columns from the second table where the IDs match. If there isn‚Äôt a match in the second table, then new values will be blank in the new columns.\nA right_join is the opposite of that: You keep ALL the rows of the second table, but bring over only matching rows from the first.\nA full_join keeps all rows and columns from both tables, joining rows when they match.\n\nHere are two common ways to think of this visually.\nIn the image below, The orange represents the data that remains after the join.\n\nThis next visual shows this as tables where only two rows ‚Äúmatch‚Äù so you can see how non-matches are handled (the lighter color represents blank values). The functions listed there are the tidyverse versions of each join type.\n\n\n9.5.1 Joining our reference table\nIn our case we start with the dref data and then use an inner_join to add all the yearly data values. We‚Äôre doing it in this order so the dref values are listed first in our resulting table.\n\nStart a new Markdown section and note we are joining the reference data\nAdd the chunk below and run it\n\n\nsped_joined &lt;- dref |&gt; \n  inner_join(sped_merged, by = \"district\")\n\nsped_joined |&gt; head()\n\n# A tibble: 6 √ó 9\n  district distname  cntyname dflalted dflchart year  dpetallc dpetspec dpetspep\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2013       595       73     12.3\n2 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2014       553       76     13.7\n3 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2015       577       76     13.2\n4 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2016       568       78     13.7\n5 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2017       576       82     14.2\n6 001902   CAYUGA I‚Ä¶ ANDERSON N        N        2018       575       83     14.4\n\n\nYou might also glimpse() it so you can see all the columns have been added.\n\nsped_joined |&gt; glimpse()\n\nRows: 11,882\nColumns: 9\n$ district &lt;chr&gt; \"001902\", \"001902\", \"001902\", \"001902\", \"001902\", \"001902\", \"‚Ä¶\n$ distname &lt;chr&gt; \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYUGA ISD\", \"CAYU‚Ä¶\n$ cntyname &lt;chr&gt; \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"ANDERSON\", \"‚Ä¶\n$ dflalted &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"‚Ä¶\n$ dflchart &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"‚Ä¶\n$ year     &lt;chr&gt; \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020‚Ä¶\n$ dpetallc &lt;dbl&gt; 595, 553, 577, 568, 576, 575, 564, 557, 535, 574, 1236, 1207,‚Ä¶\n$ dpetspec &lt;dbl&gt; 73, 76, 76, 78, 82, 83, 84, 82, 78, 84, 113, 107, 126, 144, 1‚Ä¶\n$ dpetspep &lt;dbl&gt; 12.3, 13.7, 13.2, 13.7, 14.2, 14.4, 14.9, 14.7, 14.6, 14.6, 9‚Ä¶\n\n\nLet‚Äôs explain what is going on here:\n\nWe are creating a new bucket sped_joined to save our data.\nWe start with dref so those fields will be listed first in our result.\nWe then pipe into inner_join() to sped_merged, which will attach our dref data to our merged data when the ID matches in the district variable.\nThe by = \"district\" argument ensures that we are matching based on the district column in both data sets.\n\nWe could‚Äôve left out the by = argument and R would match columns of the same name, but it is best practice to specify your joining columns so it is clear what is happening. You wouldn‚Äôt want to be surprised by other columns of the same name that you didn‚Äôt want to join on. If you wanted to specify join columns of different names it would look like this: df1 |&gt; inner_join(df2, by = c(\"df1_id\" = \"df2_id\"))\nThere are now 11882 rows in our joined data, fewer than what was in the original merged file because some districts (mostly charters) have closed and were not in our reference file. We are comparing only districts that have been open during this time period. For that matter, we don‚Äôt want charter or alternative education districts at all, so we‚Äôll drop those next."
  },
  {
    "objectID": "bind-join.html#some-cleanup-filter-and-select",
    "href": "bind-join.html#some-cleanup-filter-and-select",
    "title": "9¬† Bind and join",
    "section": "9.6 Some cleanup: filter and select",
    "text": "9.6 Some cleanup: filter and select\nFiltering and selecting data is something we‚Äôve done time and again, so you should be able to do this on your own.\nYou will next remove the charter and alternative education districts. This is a judgement call on our part to focus on just traditional public schools. We can always come back later and change if needed.\nYou‚Äôll also remove and rename columns to make the more descriptive.\n\nStart a new markdown section and note you are cleaning up your data.\nCreate an R chunk and start with the sped_joined and then do all these things ‚Ä¶\nUse filter() to keep rows where:\n\nthe dflalted field is ‚ÄúN‚Äù\nAND the dflchart field is ‚ÄúN‚Äù\n\nUse select() to:\n\nremove (or not include) the dflalted and dflchart columns. (You can only do this AFTER you filter with them!)\n\nUse select() or rename() to rename the following columns:\n\nchange dpetallc to all_count\nchange dpetspec to sped_count\nchange dpetspep to sped_percent\n\nMake sure all your changes are saved into a new data frame called sped_cleaned.\n\nI really, really suggest you don‚Äôt try to write that all at once. Build it one line at a time so you can see the result as you build your code.\n\n\nI‚Äôm being too nice here\n\n\nsped_cleaned &lt;- sped_joined |&gt; \n  filter(dflalted == \"N\" & dflchart == \"N\") |&gt; \n  select(\n    district,\n    distname,\n    cntyname,\n    year,\n    all_count = dpetallc,\n    sped_count = dpetspec,\n    sped_percent = dpetspep\n  )\n\n\n\nYou should end up with 10204 rows and 7 variables."
  },
  {
    "objectID": "bind-join.html#create-an-audit-benchmark-column",
    "href": "bind-join.html#create-an-audit-benchmark-column",
    "title": "9¬† Bind and join",
    "section": "9.7 Create an audit benchmark column",
    "text": "9.7 Create an audit benchmark column\nPart of this story is to note when a district is above the ‚Äú8.5%‚Äù benchmark that the TEA was using for their audit calculations. It would be useful to have a column that noted if a district was above or below that threshold so we could plot districts based on that flag. We‚Äôll create this new column and introduce the logic of if_else().\nOK, our data looks like this:\n\nsped_cleaned |&gt; head()\n\n# A tibble: 6 √ó 7\n  district distname   cntyname year  all_count sped_count sped_percent\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 001902   CAYUGA ISD ANDERSON 2013        595         73         12.3\n2 001902   CAYUGA ISD ANDERSON 2014        553         76         13.7\n3 001902   CAYUGA ISD ANDERSON 2015        577         76         13.2\n4 001902   CAYUGA ISD ANDERSON 2016        568         78         13.7\n5 001902   CAYUGA ISD ANDERSON 2017        576         82         14.2\n6 001902   CAYUGA ISD ANDERSON 2018        575         83         14.4\n\n\nWe want to add a column called audit_flag that says ABOVE if the sped_percent is above ‚Äú8.5‚Äù, and says BELOW if it isn‚Äôt. This is a simple true/false condition that is perfect for the if_else() function.\n\nAdd a new Markdown section and note that you are adding an audit flag column\nCreate an r chunk that and run it and I‚Äôll explain after.\n\n\nsped_flag &lt;- sped_cleaned |&gt; \n  mutate(audit_flag = if_else(sped_percent &gt; 8.5, \"ABOVE\", \"BELOW\"))\n\n# this pulls 30 random rows so I can check results\nsped_flag |&gt; sample_n(10)\n\n# A tibble: 10 √ó 8\n   district distname cntyname year  all_count sped_count sped_percent audit_flag\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;     \n 1 109913   BLUM ISD HILL     2022        344         50         14.5 ABOVE     \n 2 153904   TAHOKA ‚Ä¶ LYNN     2022        597         83         13.9 ABOVE     \n 3 073901   CHILTON‚Ä¶ FALLS    2014        509         59         11.6 ABOVE     \n 4 015915   NORTHSI‚Ä¶ BEXAR    2016     104437      11538         11   ABOVE     \n 5 129910   SCURRY-‚Ä¶ KAUFMAN  2019       1052        110         10.5 ABOVE     \n 6 062901   CUERO I‚Ä¶ DEWITT   2022       1920        319         16.6 ABOVE     \n 7 076904   ROTAN I‚Ä¶ FISHER   2017        246         19          7.7 BELOW     \n 8 216901   STERLIN‚Ä¶ STERLING 2016        329         33         10   ABOVE     \n 9 018905   WALNUT ‚Ä¶ BOSQUE   2014        174         19         10.9 ABOVE     \n10 043901   ALLEN I‚Ä¶ COLLIN   2021      21564       2759         12.8 ABOVE     \n\n\nLet‚Äôs walk through the code above:\n\nWe‚Äôre making a new data frame called sped_flag and then starting with sped_cleaned.\nWe use mutate() to create a new column and we name it audit_flag.\nWe set the value of audit_flag to be the result of this if_else() function. That function takes three arguments: A condition test (sped_percent &gt; 8.5 in our case), what is the result if the condition is true (‚ÄúABOVE‚Äù in our case), and what is the result if the condition is NOT true (‚ÄúBELOW‚Äù) in our case.\nLastly we print out the new data sped_cleaned and pipe it into sample_n() which gives us a number of random rows from the data. I do this because the top of the data was all TRUE so I couldn‚Äôt see if the mutate worked properly or not. (Always check your calculations!!)"
  },
  {
    "objectID": "bind-join.html#export-the-data",
    "href": "bind-join.html#export-the-data",
    "title": "9¬† Bind and join",
    "section": "9.8 Export the data",
    "text": "9.8 Export the data\nThis is something you‚Äôve done a number of times as well, so I leave you to you:\n\nMake a new section and note you are exporting the data\nExport your sped_flag data using write_rds() and save it in your data-processed folder.\n\nIn the next chapter we‚Äôll build an analysis notebook to find our answers!"
  },
  {
    "objectID": "answers.html#goals-of-this-chapter",
    "href": "answers.html#goals-of-this-chapter",
    "title": "10¬† Plotting for answers",
    "section": "10.1 Goals of this chapter",
    "text": "10.1 Goals of this chapter\n\nIntroduce datatables() from the DT package\nPractice pivots to prepare data for plotting\nPractice plots to reveal insights in data\nThere are wrap-up assignments that include writing, charts and this analysis\n\nWhen a new concept is introduced, it‚Äôs shown and explained here. However, there are also on your own parts where you apply concepts you have learned in previous chapters or assignments."
  },
  {
    "objectID": "answers.html#project-setup",
    "href": "answers.html#project-setup",
    "title": "10¬† Plotting for answers",
    "section": "10.2 Project setup",
    "text": "10.2 Project setup\n\nWithin the same project you‚Äôve been working, create a new R Notebook. You might call it 02-analysis.\nWe will be using a new package so you‚Äôll need to install it. Use your Console to run install.packages(\"DT\").\nInclude the libraries below and run them.\n\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)"
  },
  {
    "objectID": "answers.html#import-cleaned-data",
    "href": "answers.html#import-cleaned-data",
    "title": "10¬† Plotting for answers",
    "section": "10.3 Import cleaned data",
    "text": "10.3 Import cleaned data\n\nCreate a section for your import\nImport your cleaned data and call it sped if you want to follow along here.\n\nYou should know how to do all that and I don‚Äôt know what you called your export file anyway.\nBut to recap, the data should look like this:\n\nsped |&gt; head()\n\n# A tibble: 6 √ó 8\n  district distname  cntyname year  all_count sped_count sped_percent audit_flag\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;     \n1 001902   CAYUGA I‚Ä¶ ANDERSON 2013        595         73         12.3 ABOVE     \n2 001902   CAYUGA I‚Ä¶ ANDERSON 2014        553         76         13.7 ABOVE     \n3 001902   CAYUGA I‚Ä¶ ANDERSON 2015        577         76         13.2 ABOVE     \n4 001902   CAYUGA I‚Ä¶ ANDERSON 2016        568         78         13.7 ABOVE     \n5 001902   CAYUGA I‚Ä¶ ANDERSON 2017        576         82         14.2 ABOVE     \n6 001902   CAYUGA I‚Ä¶ ANDERSON 2018        575         83         14.4 ABOVE"
  },
  {
    "objectID": "answers.html#make-a-searchable-table",
    "href": "answers.html#make-a-searchable-table",
    "title": "10¬† Plotting for answers",
    "section": "10.4 Make a searchable table",
    "text": "10.4 Make a searchable table\nWouldn‚Äôt it be nice to be able to see the percentage of special education students for each district for each year? The way our data is formatted now, that‚Äôs pretty hard to see with our ‚Äúlong‚Äù data here.\nWe could use something more like this? (But with all the years)\n\n\n\ndistname\ncntyname\n2013\n2014\n2015\netc\n\n\n\n\nCAYUGA ISD\nANDERSON\n12.3\n13.7\n13.2\n‚Ä¶\n\n\nELKHART ISD\nANDERSON\n9.1\n8.9\n10.4\n‚Ä¶\n\n\nFRANKSTON ISD\nANDERSON\n10.8\n9.7\n9.7\n‚Ä¶\n\n\nNECHES ISD\nANDERSON\n11.1\n9\n11.1\n‚Ä¶\n\n\nPALESTINE ISD\nANDERSON\n7.7\n7.8\n8.7\n‚Ä¶\n\n\nWESTWOOD ISD\nANDERSON\n9.3\n10\n9.3\n‚Ä¶\n\n\n\nAnd what if you could make that table searchable to find a district by name or county? That would be magic, right?\nWe can do this by first reshaping our data using pivot_wider() and then applying a function called datatable().\n\n10.4.1 Pivot wider\nYou used pivot_wider() with the candy data in Chapter 9, so you can look back at how that was done, but here are some hints:\n\nCreate a new section that notes you are creating a table of district percents.\nFirst use select() to get just the columns you need: distname, cntyname, year and sped_percent.\nThen use pivot_wider()to make a tibble like the one above. Remember that the names_from = argument wants to know which column you want to use to create the names of the new columns. The values_from = argument wants to know which column to pull the cell values from.\nSave the result into a new tibble and call it district_percents_data\n\n\n\nYou won‚Äôt need this because I believe in you\n\n\ndistrict_percents_data &lt;- sped |&gt; \n  select(distname, cntyname, year, sped_percent) |&gt; \n  pivot_wider(names_from = year, values_from = sped_percent)\n\n\n\n\n10.4.2 Make a datatable\nNow comes the magic.\n\nIn a new R chunk take your district_percents_data and then pipe it into a function called datatable()\n\n\ndistrict_percents_data |&gt; \n  datatable()\n\n\n\n\n\n\nThat‚Äôs kinda brilliant, isn‚Äôt it? You know can search by any value in the table.\nQuick, tell me what was the percentage for Austin ISD in 2020?\nThat will be useful tool for you when you are writing about specific districts."
  },
  {
    "objectID": "answers.html#choosing-a-chart-to-display-data",
    "href": "answers.html#choosing-a-chart-to-display-data",
    "title": "10¬† Plotting for answers",
    "section": "10.5 Choosing a chart to display data",
    "text": "10.5 Choosing a chart to display data\nOur first question about this data was this: Has the percentage of special education students in Texas changed since the benchmarking policy was dropped?\nGiven the data we have, can we answer this?\nLet‚Äôs think about the charts that might be able to show two related variables like that. Choosing the chart type to display that takes experimentation and exploration. Chapter 4 of Nathan Yau‚Äôs Data Points book is an excellent look at which chart types help show different data.\nThis decision tree option might also help you think through it.\n\nLastly, another resource to consider this is the ggplot cheatsheet, where it includes possible chart types based on the type of data we are comparing."
  },
  {
    "objectID": "answers.html#plot-yearly-student-percentage",
    "href": "answers.html#plot-yearly-student-percentage",
    "title": "10¬† Plotting for answers",
    "section": "10.6 Plot yearly student percentage",
    "text": "10.6 Plot yearly student percentage\nI‚Äôll often do a hand drawing of a chart that might help me understand or communicate data. This helps me think about how to summarize and shape my data to get to that point.\nFor this question Has the percentage of special education students in Texas changed since the benchmarking policy was dropped?, we could show the percentage of special education students for each year. If we are to chart this, the x axis would be the year and the y axis would be the percentage for that year. Perhaps like this:\n\nWe have the percent of students in special education for each district in each year. We could get an average of those percentages for each year, but that won‚Äôt take into account the size of each district. Some districts have a single-digit number of students while others have hundreds of thousands of students.\nBut we also have the number of all students in a district with all_count and the number of special education students sped_count. With these, we can build a more accurate percentage across all the districts. This allows us to calculate our student groups by year.\n\n10.6.1 Build the statewide percentage by year data\nSo, let‚Äôs summarize our data. This is a ‚Äúsimple‚Äù group_by and summarize to get yearly totals, then a mutate to build our percentage. I‚Äôll supply the logic so you can try writing it yourself.\n\nStart a new notebook section and note you are getting yearly percentages\nIn an R chunk, start with your sped data.\nGroup by the year variable.\nIn summarize, create a sum() of the all_count variable. Call this total_students.\nIn the same summarize, create a sum() of the sped_count variable. You might call this total_sped.\nCheck your results at this point ‚Ä¶ make sure it makes sense.\nNext use mutate() to create a percentage called sped_percent from your total_students and total_sped summaries. The math for percentage is: (part / whole) * 100. You might want to round that resulting value to tenths as well.\nI suggest you save all that into a new tibble called yearly_percent (because that‚Äôs what I‚Äôm gonna do).\n\n\n\nMy version\n\n\nyearly_percent &lt;- sped |&gt; \n  group_by(year) |&gt; \n  summarise(\n    total_students = sum(all_count),\n    total_sped = sum(sped_count)\n  ) |&gt; \n  mutate(sped_percent = ((total_sped / total_students) * 100) |&gt; round(1))\n\nyearly_percent\n\n# A tibble: 10 √ó 4\n   year  total_students total_sped sped_percent\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 2013         4870038     418360          8.6\n 2 2014         4929867     420847          8.5\n 3 2015         4984714     427377          8.6\n 4 2016         5034274     437487          8.7\n 5 2017         5070664     449303          8.9\n 6 2018         5088351     468097          9.2\n 7 2019         5099385     499240          9.8\n 8 2020         5142334     551718         10.7\n 9 2021         4993274     566484         11.3\n10 2022         5025780     592457         11.8\n\n\n\n\n\n10.6.2 Build the statewide percentage chart\nAnd with that table, you can plot your chart using the year and sped_percent.\n\nyearly_percent |&gt; \n  ggplot(aes(x = year, y = sped_percent)) + \n  geom_col() +\n  geom_text(aes(label = sped_percent, vjust = -.5))\n\n\n\n\nThis definitely shows us that the percentage of students in special education (in traditional public schools) has increased each year since 2017 when the benchmark was dropped and then outlawed by the legislature.\nI would be careful about saying the increase is because of the changes (though that is likely true), but you can certainly say with authority that it has gone up, and interview other people to pontificate on the reasons why.\nBTW, if you wanted to make that chart in datawrapper, you could use the same tibble, select just the year and sped_percent columns and then use the clipr::write_clip() function to copy your data."
  },
  {
    "objectID": "answers.html#districts-by-benchmark-and-year",
    "href": "answers.html#districts-by-benchmark-and-year",
    "title": "10¬† Plotting for answers",
    "section": "10.7 Districts by benchmark and year",
    "text": "10.7 Districts by benchmark and year\nOur second question is this: How many districts were above that arbitrary 8.5% benchmark before and after the changes?\nIt was in anticipation for this that we built the audit_flag field in our data. With that we can count how many rows have the ‚ÄúABOVE‚Äù or ‚ÄúBELOW‚Äù value. (In reality, it‚Äôs probably at this point we would discover it useful to create, instead of at the data cleaning phase, but I wanted to get that part out of the way so we can concentrate on the charts.)\nTo decide on which chart to build, we can go back to our chart decision workflow or consult the ggplot cheetsheet. Given we have the discrete values of audit_flag and years, and we want to plot how many districts are counted (which is a continuous value), we‚Äôre looking at a stacked or grouped column/bar chart or a line chart.\n\n10.7.1 Build the audit flag data\nBefore we can build the chart, we need to summarize our data. The logic is this: We need to group our data by both the year and the audit_flag, and then count the number of rows for those values.\n\nStart a new Markdown section and note you are counting districts by the audit flag.\nStart with your original sped data.\nGroup your data by both year and audit_flag.\nSummarize your data by counting n() the results. Name the variable count_districts.\nSave the resulting data into a tibble called flag_count_districts. You might print that result out so you can refer to it.\n\n\n\nUse GSA or the count() shortcut\n\n\n# I used the count() shortcut instead of GSA\nflag_count_districts &lt;- sped |&gt; \n  count(year, audit_flag, name = \"count_districts\")\n\n\n The data should look like this:\n\n\n# A tibble: 20 √ó 3\n   year  audit_flag count_districts\n   &lt;chr&gt; &lt;chr&gt;                &lt;int&gt;\n 1 2013  ABOVE                  591\n 2 2013  BELOW                  429\n 3 2014  ABOVE                  571\n 4 2014  BELOW                  449\n 5 2015  ABOVE                  579\n 6 2015  BELOW                  441\n 7 2016  ABOVE                  578\n 8 2016  BELOW                  442\n 9 2017  ABOVE                  629\n10 2017  BELOW                  391\n11 2018  ABOVE                  688\n12 2018  BELOW                  332\n13 2019  ABOVE                  765\n14 2019  BELOW                  255\n15 2020  ABOVE                  865\n16 2020  BELOW                  156\n17 2021  ABOVE                  909\n18 2021  BELOW                  112\n19 2022  ABOVE                  945\n20 2022  BELOW                   77\n\n\n\n\n10.7.2 Build the audit flag chart\nNow that we have the data, we can build the ggplot column chart. The key new thing here is we are using a new aesthetic fill to apply colors based on the audit_flag column.\n\nAdd some notes you are buliding the first exploratory chart\nAdd an R chunk with the following:\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, fill = audit_flag)) +\n  geom_col()\n\n\n\n\nThis is actually not a bad look because it does clearly show the number of districts below the 8.5% audit benchmark is dropping year by year, and the number of districts above is growing.\nLeave that chart there for reference, but let‚Äôs build a new one that is almost the same, but we‚Äôll adjust it to be a grouped column chart instead of stacked. The key difference is we are adding position = \"dodge\" to the column geom.\n\nNote in Markdown text you are building the grouped column version\nAdd a new chunk and start with exactly what you have above.\nUpdate the column geom to this: geom_col(position = \"dodge\")\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, fill = audit_flag)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nThat‚Äôs not bad at all ‚Ä¶ it might be the winner.\nLastly, let‚Äôs chart this data as a line chart to see if that looks any better or is easier to comprehend.\n\nNote you are visualizing as a line\nAdd the chunk below.\n\n\nflag_count_districts |&gt; \n  ggplot(aes(x = year, y = count_districts, group = audit_flag)) +\n  geom_line(aes(color = audit_flag)) +\n  ylim(0,1000)\n\n\n\n\nThe chart above has ylim() added because the default didn‚Äôt start the y axis at zero. ylim is a shortcut for scale_y_continuous(limits = c(0,1000)).\n\n\n10.7.3 Which chart is best?\nRemember, this is the question you are trying to answer: How many districts were above that arbitrary 8.5% benchmark before and after the changes?\nI would say the grouped bar chart (the bars next to each other) is probably best to explain this concept."
  },
  {
    "objectID": "answers.html#local-districts",
    "href": "answers.html#local-districts",
    "title": "10¬† Plotting for answers",
    "section": "10.8 Local districts",
    "text": "10.8 Local districts\nWe have one last question: How have local districts changed? i.e., what are the percentages for districts in Bastrop, Hays, Travis and Williamson counties? We want to make sure none of these buck the overall trend.\nYou can certainly use the searchable table we made to get an idea of the number of districts and how the numbers have changed, but you can‚Äôt see them there. Searching there does reveal there are too many districts to visualize them all at once. Maybe we can chart one county at a time.\nLooking at the chart suggestions workflow, we are doing a comparison over time of many categories ‚Ä¶ so our trusty line chart is the horse to hitch.\nTo make that line chart we think about our axes and groups: We need our x axis of year, y axis of the sped_percent and we need to group the lines by their district. Since we need a column that has each year, the long data we started with should suffice:\n\nsped |&gt;\n  head()\n\n# A tibble: 6 √ó 8\n  district distname  cntyname year  all_count sped_count sped_percent audit_flag\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;     \n1 001902   CAYUGA I‚Ä¶ ANDERSON 2013        595         73         12.3 ABOVE     \n2 001902   CAYUGA I‚Ä¶ ANDERSON 2014        553         76         13.7 ABOVE     \n3 001902   CAYUGA I‚Ä¶ ANDERSON 2015        577         76         13.2 ABOVE     \n4 001902   CAYUGA I‚Ä¶ ANDERSON 2016        568         78         13.7 ABOVE     \n5 001902   CAYUGA I‚Ä¶ ANDERSON 2017        576         82         14.2 ABOVE     \n6 001902   CAYUGA I‚Ä¶ ANDERSON 2018        575         83         14.4 ABOVE     \n\n\nWe just need to filter that down to a single county cntyname == \"BASTROP\" to show this. We can even do this all in one code block. Let‚Äôs see if you can follow this logic and build Bastrop county for yourself.\n\nStart a new section that you are looking at local districts\nStart a new chunk with your original sped data\nFilter it to have just rows for BASTROP county\nPipe that result into the ggplot() function\nFor the x axis, you are using year, for the y use sped_percent and for group use distname\nInside your geom_line() set the the color to the district: aes(color = distname).\n\nIt should look like this:\n\n\n\n\n\n\nNow do the same for the other three counties, each in their own code chunk: Hays, Travis and Williamson.\n\nThese charts give you some reference for the local districts. You‚Äôll see the more districts there are within a county, the less effective the line chart becomes. But at least it gives you an idea of which districts are following the trend."
  },
  {
    "objectID": "answers.html#the-assignments",
    "href": "answers.html#the-assignments",
    "title": "10¬† Plotting for answers",
    "section": "10.9 The assignments",
    "text": "10.9 The assignments\nNow that you have looked at this data, you should be able to describe what has happened to the share of special education students across the state. You‚Äôll do this in three parts: a written piece, a published Datawrapper chart, and you‚Äôll turn in this analysis.\nSee Canvas for details on each of these.\n‚Äì 30 ‚Äì"
  },
  {
    "objectID": "group-dates.html#setting-up",
    "href": "group-dates.html#setting-up",
    "title": "11¬† Grouping by dates",
    "section": "11.1 Setting up",
    "text": "11.1 Setting up\nWe need to set up our notebook with libraries and data before we can talk specifics. We need to load both the tidyverse and lubridate libraries. Lubridate is installed with the tidyverse package, but you have to load it separately.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nAnd we need our cleaned Billboard Hot 100 data.\n\nhot100 &lt;- read_rds(\"data-processed/01-hot100.rds\")\n\nhot100 |&gt; glimpse()\n\nRows: 336,100\nColumns: 7\n$ chart_date    &lt;date&gt; 1958-08-04, 1958-08-04, 1958-08-04, 1958-08-04, 1958-08‚Ä¶\n$ current_rank  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ title         &lt;chr&gt; \"Poor Little Fool\", \"Patricia\", \"Splish Splash\", \"Hard H‚Ä¶\n$ performer     &lt;chr&gt; \"Ricky Nelson\", \"Perez Prado And His Orchestra\", \"Bobby ‚Ä¶\n$ previous_rank &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ peak_rank     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ wks_on_chart  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "group-dates.html#plucking-date-parts",
    "href": "group-dates.html#plucking-date-parts",
    "title": "11¬† Grouping by dates",
    "section": "11.2 Plucking date parts",
    "text": "11.2 Plucking date parts\nIf you look at the lubridate cheetsheet under ‚ÄúGET AND SET DATE COMPONENTS‚Äù you‚Äôll see functions to pluck out parts of a date, like year().\nIf we have a date, like perhaps Taylor Swift‚Äôs birthday, we can pluck the year from it.\n\nyear(\"1989-12-13\")\n\n[1] 1989"
  },
  {
    "objectID": "group-dates.html#grouping-by-a-date-part-on-the-fly",
    "href": "group-dates.html#grouping-by-a-date-part-on-the-fly",
    "title": "11¬† Grouping by dates",
    "section": "11.3 Grouping by a date part on the fly",
    "text": "11.3 Grouping by a date part on the fly\nLet‚Äôs show how this might be useful through an example question:\nWhich performer has the most appearances on the chart in a given year?\nThe logic works like this:\n\nGroup all the records by performer AND the year of the chart_date\nSummarize and count the rows\n\n\nhot100 |&gt; \n  group_by(\n    year(chart_date),\n    performer\n  ) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n`summarise()` has grouped output by 'year(chart_date)'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 22,953 √ó 3\n# Groups:   year(chart_date) [65]\n   `year(chart_date)` performer      appearances\n                &lt;dbl&gt; &lt;chr&gt;                &lt;int&gt;\n 1               1964 The Beatles            214\n 2               2021 Olivia Rodrigo         172\n 3               2018 Drake                  168\n 4               2022 Bad Bunny              148\n 5               2019 Billie Eilish          145\n 6               2016 Drake                  134\n 7               2015 The Weeknd             126\n 8               2005 Kelly Clarkson         124\n 9               2015 Drake                  124\n10               2009 Taylor Swift           122\n# ‚Ñπ 22,943 more rows\n\n\nWe can see here that The Beatles had the most hits in 1964 with 214 (at least as of this writing).\nBut notice how the year column name is kinda shite? We would not be able to easily reference that variable later, so we should rename that AS we make the group:\n\nhot100 |&gt; \n  group_by(\n    yr = year(chart_date), # added \"yr = \" here\n    performer\n  ) |&gt; \n  summarize(appearances = n()) |&gt; \n  arrange(desc(appearances))\n\n`summarise()` has grouped output by 'yr'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 22,953 √ó 3\n# Groups:   yr [65]\n      yr performer      appearances\n   &lt;dbl&gt; &lt;chr&gt;                &lt;int&gt;\n 1  1964 The Beatles            214\n 2  2021 Olivia Rodrigo         172\n 3  2018 Drake                  168\n 4  2022 Bad Bunny              148\n 5  2019 Billie Eilish          145\n 6  2016 Drake                  134\n 7  2015 The Weeknd             126\n 8  2005 Kelly Clarkson         124\n 9  2015 Drake                  124\n10  2009 Taylor Swift           122\n# ‚Ñπ 22,943 more rows\n\n\nIt is a good practice to rename any grouping variable made from a function like that. FWIW, it would‚Äôve worked if I called the new column year, but I named it yr so I‚Äôm less likely to confuse it with the function year(). It‚Äôs a personal preference what to name the new column."
  },
  {
    "objectID": "group-dates.html#making-reusable-date-parts",
    "href": "group-dates.html#making-reusable-date-parts",
    "title": "11¬† Grouping by dates",
    "section": "11.4 Making reusable date parts",
    "text": "11.4 Making reusable date parts\nIf you think you‚Äôll use a date parts more than once, then it makes sense to create a new columns and save them. You might make several date parts, but we‚Äôll start with only one.\nI usually go back to my cleaning notebook to add these once I recognize I need them, and then rerun everything.\nTo make this easier to show, I‚Äôve created a random sample of data with only the chart_date and title columns. Here is our sample:\n\n# you wouldn't normally do this!!!!\nhot100_sample &lt;- hot100 |&gt; slice_sample(n = 6) |&gt; select(chart_date, title)\n\nhot100_sample\n\n# A tibble: 6 √ó 2\n  chart_date title            \n  &lt;date&gt;     &lt;chr&gt;            \n1 2007-07-21 Paralyzer        \n2 2011-05-07 I Won't Let Go   \n3 2021-06-05 Without You      \n4 1998-09-12 Ava Adore        \n5 1990-06-23 Counting The Days\n6 1993-06-19 What's Up        \n\n\n\n11.4.1 Let‚Äôs make a year\nHere‚Äôs how we do it:\n\nWe use mutate to create a new column.\nWe name the new column yr.\nWe set the value of yr to equal the year() of chart_date.\n\n\nhot100_sample |&gt; \n  mutate(\n    yr = year(chart_date)\n  )\n\n# A tibble: 6 √ó 3\n  chart_date title                yr\n  &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt;\n1 2007-07-21 Paralyzer          2007\n2 2011-05-07 I Won't Let Go     2011\n3 2021-06-05 Without You        2021\n4 1998-09-12 Ava Adore          1998\n5 1990-06-23 Counting The Days  1990\n6 1993-06-19 What's Up          1993\n\n\n\n\n11.4.2 The magical month\nWe can also pluck out the month of the date, which is pretty useful if you want to measure seasonality within a year, like hot days of summer, etc. The default month() function pulls the month as a number.\n\nhot100_sample |&gt; \n  mutate(\n    mo = month(chart_date)\n  )\n\n# A tibble: 6 √ó 3\n  chart_date title                mo\n  &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt;\n1 2007-07-21 Paralyzer             7\n2 2011-05-07 I Won't Let Go        5\n3 2021-06-05 Without You           6\n4 1998-09-12 Ava Adore             9\n5 1990-06-23 Counting The Days     6\n6 1993-06-19 What's Up             6\n\n\nBut there are some options within month() to give us month NAMES that are ordered as factors instead of alphabetical.\n\nhot100_sample |&gt; \n  mutate(\n    mo_label = month(chart_date, label = TRUE),\n    mo_long = month(chart_date, label = TRUE, abbr = FALSE)\n  ) |&gt; \n  arrange(mo_label)\n\n# A tibble: 6 √ó 4\n  chart_date title             mo_label mo_long  \n  &lt;date&gt;     &lt;chr&gt;             &lt;ord&gt;    &lt;ord&gt;    \n1 2011-05-07 I Won't Let Go    May      May      \n2 2021-06-05 Without You       Jun      June     \n3 1990-06-23 Counting The Days Jun      June     \n4 1993-06-19 What's Up         Jun      June     \n5 2007-07-21 Paralyzer         Jul      July     \n6 1998-09-12 Ava Adore         Sep      September\n\n\nNote the datatype &lt;ord&gt; under the column mo_label and mo_long. That means this is an ‚Äúordered factor‚Äù and that when arranged by those labels it will list in MONTH order instead of alphabetical order, which is quite useful.\n\n\n11.4.3 Floor dates\nSometimes you want to count the number of records within a month and year, like all the songs in January 2000, then February 2000, etc. One way to do that is to create a floor_date, which gives you the ‚Äúlowest‚Äù date within a certain unit like year or month. It‚Äôs easiest to show with our sample data:\n\nhot100_sample |&gt; \n  mutate(\n    fl_month = floor_date(chart_date, unit = \"month\"),\n    fl_year = floor_date(chart_date, unit = \"year\")\n  )\n\n# A tibble: 6 √ó 4\n  chart_date title             fl_month   fl_year   \n  &lt;date&gt;     &lt;chr&gt;             &lt;date&gt;     &lt;date&gt;    \n1 2007-07-21 Paralyzer         2007-07-01 2007-01-01\n2 2011-05-07 I Won't Let Go    2011-05-01 2011-01-01\n3 2021-06-05 Without You       2021-06-01 2021-01-01\n4 1998-09-12 Ava Adore         1998-09-01 1998-01-01\n5 1990-06-23 Counting The Days 1990-06-01 1990-01-01\n6 1993-06-19 What's Up         1993-06-01 1993-01-01\n\n\nYou can see the resulting new columns are real dates, but they are normalized:\n\nThe fl_month gives you the first day of the month for that chart_date.\nThe fl_year gives you the first day of the year for that chart_date.\n\nLet‚Äôs put this to use with an example. I‚Äôll create a fl_month on the fly to find Taylor Swift appearances by month for 2020-2021. I‚Äôll also do the year() on the fly in my filter.\n\nswift_month &lt;- hot100 |&gt; \n  filter(\n    performer == \"Taylor Swift\",\n    year(chart_date) %in% 2020:2021\n  ) |&gt; \n  group_by(\n    fl_month = floor_date(chart_date, unit = \"month\")\n  ) |&gt; \n  summarize(appearances = n())\n\nswift_month\n\n# A tibble: 17 √ó 2\n   fl_month   appearances\n   &lt;date&gt;           &lt;int&gt;\n 1 2020-01-01           4\n 2 2020-02-01           1\n 3 2020-03-01           2\n 4 2020-04-01           3\n 5 2020-08-01          28\n 6 2020-09-01           6\n 7 2020-10-01           5\n 8 2020-11-01           1\n 9 2020-12-01          12\n10 2021-01-01           7\n11 2021-02-01           5\n12 2021-03-01           4\n13 2021-04-01          12\n14 2021-05-01           3\n15 2021-10-01           1\n16 2021-11-01          21\n17 2021-12-01          13\n\n\nAnd chart it:\n\nswift_month |&gt; \n  ggplot(aes(x = fl_month, y = appearances)) +\n  geom_col() +\n  geom_text(aes(label = appearances), vjust = -.3) +\n  scale_x_date(date_labels=\"%b %Y\",date_breaks  =\"1 month\") +\n  guides(x =  guide_axis(angle = 45)) +\n  labs(\n    x = \"Month and Year\",\n    y = \"Hot 100 appearances\",\n    title = \"Taylor Swift's Billboard Hot 100 appearances by month, 2020-2021\"\n  )\n\n\n\n\nThat tall bar with 28 appearances was right after ‚ÄúFolklore‚Äù was released in July 2020. There was another spike after ‚ÄúEvermore‚Äù was released in December 2020. We‚Äôll see what happened with ‚ÄúMidnights‚Äù once I update the data."
  },
  {
    "objectID": "functions.html#importexport",
    "href": "functions.html#importexport",
    "title": "12¬† R Functions",
    "section": "12.1 Import/Export",
    "text": "12.1 Import/Export\n\nread_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(\"path/to/file.csv\").\nwrite_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(\"path/to/file.rds\").\nreadxl is a package we didn‚Äôt use, but it has read_excel() that allows you to import from an Excel file, including specified sheets and ranges.\nclean_names() from the library(janitor) package standardizes column names."
  },
  {
    "objectID": "functions.html#data-manipulation",
    "href": "functions.html#data-manipulation",
    "title": "12¬† R Functions",
    "section": "12.2 Data manipulation",
    "text": "12.2 Data manipulation\n\nselect() to select columns. Example: select(col01, col02) or select(-excluded_col).\nrename() to rename a column. Example: rename(new_name = old_name).\nfilter() to filter rows of data. Example: filter(column_name == \"value\").\n\nSee Relational Operators like ==, &gt;, &gt;= etc.\nSee Logical operators like &, | etc.\nSee is.na tests if a value is missing.\n\ndistinct() will filter rows down to the unique values of the columns given.\narrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc())\nmutate() changes and existing column or creates a new one. Example: mutate(new_col = (col01 / col02)).\nround() is a base R function that can round a number to a set decimal point. Often used within a mutate() function.\nrecode(), if_else() and case_when() are all functions that can be used with mutate() to create new categorizations with your data.\npivot_longer() ‚Äúlengthens‚Äù data, increasing the number of rows and decreasing the number of columns. Example: pivot_longer(cols = 3:5, names_to = \"new_key_col_name\", values_to = \"new_val_col_name\") will take the third through the fifth columns and turn each value into a new row of data. It will put them into two columns: The first column will have the name you give it in names_to and contain the old column name that corresponds to each value pivoted. The second column will have the name of whatever you set in values_to and will contain all the values from each of the columns.\npivot_wider() is the opposite of pivot_longer(). Example: pivot_wider(names_from = col_of_key_values, values_from = col_with_values). See the link."
  },
  {
    "objectID": "functions.html#aggregation",
    "href": "functions.html#aggregation",
    "title": "12¬† R Functions",
    "section": "12.3 Aggregation",
    "text": "12.3 Aggregation\n\ngroup_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. We often add arrange() to these, calling this our GSA functions. Example: group_by(song, artist) %&gt;% summarize(weeks = n(), top_chart_position = min(peak_position)). To break or remove groupings, use ungroup().\ncount() is a shortcut for GSA that count the number rows based on variable groups you feed it."
  },
  {
    "objectID": "functions.html#math",
    "href": "functions.html#math",
    "title": "12¬† R Functions",
    "section": "12.4 Math",
    "text": "12.4 Math\nThese are the function often used within summarize():\n\nn() to count the number of rows. n_distinct() counts the unique values.\nsum() to add things together.\nmean() to get an average.\nmedian() to get the median.\nmin() to get the smallest value. max() for the largest.\n+, -, *, / are math operators similar to a calculator."
  },
  {
    "objectID": "troubleshooting.html#when-things-break",
    "href": "troubleshooting.html#when-things-break",
    "title": "13¬† Troubleshooting",
    "section": "13.1 When things break",
    "text": "13.1 When things break\nThe first and best troubleshooting advice I can give it this:\nWRITE ONE LINE OF CODE. RUN IT. CHECK THE RESULTS. REPEAT.\nWhen you write and run code one line at a time, problems are easier to find.\nBeyond that there are generally two categories of problems: You wrote ‚Äúit‚Äù wrong, or you used ‚Äúit‚Äù wrong. The first can sometimes be hard to see just like any typo.\nSome tips:\n\nRead the error message and look for words you recognize. You may not understand the error exactly, but words can be clues to what part of your code is wrong.\nCheck the spelling of variables, values and functions, especially if the error message says something like object ‚Äòwahtever‚Äô not found. If you are writing code that depends on matching strings and you are getting unexpected results, check those strings. Word case (as in Title case) can matter.\nCheck the code for balanced parenthesis and other punctuation problems. RStudio will show you matching parenthesis with highlighting, and will indicate problems with red X icons and red underlines in your code. Writing beautiful, well-indenting can sometimes help avoid and spot these types of errors.\n\nBeyond that, here are some common things students come across:\n\nThere is no package called ‚Äòpackagename‚Äô: You are trying to use a library that is named wrong or you don‚Äôt have installed. You can install packages with install.packages('packagename') where ‚Äòpackagename‚Äô is replaced with the name of the package you actually want.\nForgetting to use quotation marks when they are needed: install.packages(\"gclus\") will work, while install.packages(gclus) will generate an error.\nCould not find function ‚Äòfunctionname‚Äô: You either misspelled the function or you are missing a library() in your setup. It‚Äôs best practice to have every library() loaded in a setup chunk near the top of the notebook.\nUsing the wrong case: help(), Help(), and HELP() are three different functions (and only the first one will work)\nForgetting to include the parentheses in a function call: help() rather than help. Even if there are no options, you still need the ().\nUsing the \\ in a path name on Windows‚Äù R sees the backlash character as an escape character. setwd(\"c:\\mydata\") will generate an error. Use setwd(\"c:/mydata\") or setwd(\"c:\\\\mydata\") instead."
  },
  {
    "objectID": "troubleshooting.html#learning-how-things-work",
    "href": "troubleshooting.html#learning-how-things-work",
    "title": "13¬† Troubleshooting",
    "section": "13.2 Learning how things work",
    "text": "13.2 Learning how things work\nThere are infinite ways to write code incorrectly or use a function improperly. Documentation and experience (sometimes of others) are key to these challenges.\n\n13.2.1 Help docs\nOne way to find documentation is through the built-in Help function within RStudio. If you look at the pane at the bottom-right of RStudio, you‚Äôll see tabs for ‚ÄúFiles‚Äù, ‚ÄúPlots‚Äù, ‚ÄúPackages‚Äù and ‚ÄúHelp‚Äù. Click on the Help tab.\nYou can type in a function or part of a function and get a list of items. If you search for ‚Äúcount‚Äù and hit return you‚Äôll get documentation on how to use it. It takes some getting used to in reading the docs, but the examples at the bottom are often useful.\nThere are also some Console commands to find things:\n\n\n\n\n\n\n\nFunction\nAction\n\n\n\n\nhelp.start()\nGeneral help\n\n\nhelp(\"foo\") or ?foo\nHelp on function foo (the quotation marks are optional)\n\n\nhelp.search(\"foo\") or ??foo\nSearch the help system for instances of the string foo\n\n\nexample(\"foo\")\nExamples of function foo (the quotation marks are optional)\n\n\n\n\n\n13.2.2 Good Googling\nAnother way to get help is to Google for it, but there is an art to it especially since there are other data science languages and programs with similar terms as R. Some tips:\n\nUse ‚Äúin R‚Äù in your search: How to merge data frames in R\nUse the name of the package if you now it: Add labels with ggplot\nUse ‚Äútidyverse‚Äù if appropriate: convert text to date with tidyverse\n\nThere are plenty of Stack Overflow answers along with different tutorials from blogs and such. It is a well-used language, so there are lots of answers to help. Too many, sometimes.\n\n\n13.2.3 Tidyverse docs and cheatsheets\nIt is worth becoming familiar with the tidyverse site. Click on each icon from the home page to learn what each package does. R is also big on cheatsheets, which are printable pages that go through all the verbs. They can be a bit much at first, but useful once you use R more.\nWe‚Äôll try to put together a list of other resources and tutorials. You can find some I‚Äôve collected already here."
  },
  {
    "objectID": "troubleshooting.html#r-frequently-asked-questions",
    "href": "troubleshooting.html#r-frequently-asked-questions",
    "title": "13¬† Troubleshooting",
    "section": "13.3 R Frequently asked Questions",
    "text": "13.3 R Frequently asked Questions\n\n13.3.1 Functions, objects and variables\nThe names of things and how they are used are important in R, and can cause confusion. The term date could represent any number of things in R code depending on how you are using it, and that can be confusing. Knowing the difference between functions, objects and variables and how they are referenced in code helps.\n\nVariables are like the column names from a spreadsheet table. If you think of a data frame (or tibble in R) as a spreadsheet table, then when you reference a ‚Äúvariable‚Äù name, it is that column. A data frame of police calls might have a date column that has the date/time of the call in each row, like ‚Äú2021-01-06 17:29:38‚Äù.\nFunctions are collections of code that solve specific problems. In R, they are always followed by parenthesis, like this: date(), which is a function to pull only the date from a date/time variable. There are often arguments inside a function, like date(date) could be a function pull the date ‚Äú2021-01-06‚Äù from a date/time variable called date. Knowing that functions always have parenthesis is a good clue to help figure out how something is being used.\nObjects are stored values in R. You can name objects anything you like, including unwise things like date, since that is already an object in R as well as the name of a function.\n\nThat is all to prove it can be confusing. When you have a chance to name things, make good choices.\n\n13.3.1.1 Naming things\nBe thoughtfully obvious about the names you choose for objects and variable.\n\nAvoid naming things with what could be a function name.\nAvoid spaces. Use an underscore _ or dash - instead. I use underscores for naming things in code, but dashes for naming files or folders that could become URLs at some point.\nBe descriptive. Name things what they are, like police_calls.\nBe consistent. If you have multiple date variables like open_date and close_date then it is easy to know and select() them.\n\n\n\n\n13.3.2 Some R code basics\n\n&lt;- is known as an ‚Äúassignment operator‚Äù ‚Äì it means ‚ÄúMake the object named to the left equal to the output of the code to the right‚Äù\n& means AND, in Boolean logic\n| means OR, in Boolean logic\n! means NOT, in Boolean logic\nWhen referring to values entered as text, or to dates, put them in quote marks like this: \"United States\", or \"2016-07-26\". Numbers are not quoted\nWhen entering two or more values as a list, combine them using the function c, for combine, with the values separated by commas, for example: c(\"2017-07-26\", \"2017-08-04\")\nAs in a spreadsheet, you can specify a range of values with a colon, for example: c(1:10) creates a list of integers (whole numbers) from one to ten.\nSome common operators:\n\n+ - add, subtract\n* / multiply, divide\n&gt; &lt; greater than less than\n&gt;= &lt;= greater than or equal to, less than or equal to\n!= not equal to\n\nEqual signs can be confusing\n\n== tests whether the objects on either end are equal. This is often used in filtering data\n= makes an object equal to a value, which is similar to &lt;- but used within a function.\n\nHandling null values:\n\nNulls are designated as NA\nis.na(x) looks for nulls within variable x.\n!is.na(x) looks for non-null values within variable x\n\n\nHere, is.na() is a function."
  },
  {
    "objectID": "explore.html#start-by-listing-questions",
    "href": "explore.html#start-by-listing-questions",
    "title": "14¬† Exploring a new dataset",
    "section": "14.1 Start by listing questions",
    "text": "14.1 Start by listing questions\nIt‚Äôs likely you‚Äôve acquired data because you needed it to add context to a story or situation. Spend a little time at the beginning brainstorming as list of questions you want to answer. (You might ask a colleague to participate: the act of describing the data set will reveal questions for both of you.) I like to start my RNotebook with this list."
  },
  {
    "objectID": "explore.html#understand-your-data",
    "href": "explore.html#understand-your-data",
    "title": "14¬† Exploring a new dataset",
    "section": "14.2 Understand your data",
    "text": "14.2 Understand your data\nBefore you start working on your data, make sure you understand what all the columns and values mean. Look at your data dictionary, or talk to the data owner to make sure you understand what you are working with.\nTo get a quick summary of all the values, you can use a function called summary() to give you some basic stats for all your data. Here is an example from the Billboard Hot 100 data we used in a class assignment.\n\n\n\nSummary of billboard data\n\n\nA summary() will show you the data type for each column, and then for number values it will show you the min, max, median, mean and other stats."
  },
  {
    "objectID": "explore.html#pay-attention-to-the-shape-of-your-data",
    "href": "explore.html#pay-attention-to-the-shape-of-your-data",
    "title": "14¬† Exploring a new dataset",
    "section": "14.3 Pay attention to the shape of your data",
    "text": "14.3 Pay attention to the shape of your data\nIs your data long or wide?\nWide data adds new observations as columns, with the headers describing the observation. Official reports and Excel files from agencies are often in this format:\n\n\n\nCountry\n2018\n2017\n\n\n\n\nUnited States\n20,494,050\n19,390,604\n\n\nChina\n13,407,398\n12,237,700\n\n\n\nLong data is where each row in the data is a single observation, and each column is an attribute that describes that observation. Data-centric languages and applications like R and Tableau typically prefer this format.\n\n\n\nCountry\nYear\nGDP\n\n\n\n\nUnited States\n2018\n20,494,050\n\n\nUnited States\n2017\n19,390,604\n\n\nChina\n2018\n13,407,398\n\n\nChina\n2017\n12,237,700\n\n\n\nThe shape of the data will determine how you go about analyzing it. They are both useful in different ways. Wide data allows you to calculate columns to show changes. Visualization programs will sometimes want a long format to more easily categorize values based on the attributes.\nYou can pivot your data with pivot_longer() and pivot_wider to change the shape of your data."
  },
  {
    "objectID": "explore.html#counting-and-aggregation",
    "href": "explore.html#counting-and-aggregation",
    "title": "14¬† Exploring a new dataset",
    "section": "14.4 Counting and aggregation",
    "text": "14.4 Counting and aggregation\nA large part of data analysis is counting and sorting, or filtering and then counting and sorting. Depending on the program you are using you may approach it differently but think of these concepts:\n\n14.4.1 Counting rows based on a column\nIf you are just counting the number of rows based on the values within a column (or columns), then count() is the key. When you use count() like this, a new column called n is created to hold the count of the rows. You can rename n with the name = \"new_name\" argument, and you can change the sorting to descending order using the sort = TRUE argument.\nIn this example, we are counting the number of rows for each princess in our survey data, the arranging them in descending order.\nsurvey %&gt;% \n  count(princess, name = \"votes\", sort = TRUE)\n\n\n\nprincess\nvotes\n\n\n\n\nMulan\n14\n\n\nRapunzel (Tangled)\n7\n\n\nJasmine (Aladdin)\n6\n\n\nAriel (Little Mermaid)\n5\n\n\nTiana (Princess and the Frog)\n2\n\n\nAurora (Sleeping Beauty)\n1\n\n\nBelle (Beauty and the Beast)\n1\n\n\nMerida (Brave)\n1\n\n\nSnow White\n1\n\n\n\n\n\n14.4.2 Sum, mean and other aggregations\nIf you want to aggregate values in a column, like adding together values, or to find a mean or median, then you will want to use the GSA combination: group_by() on your columns of interest, then use summarize() to aggregate the data in the manner you choose, like sum(), mean() or the number of rows n(). You can then use arrange() to order the result however you want.\nHere is an example where we use group_by and summarize() to add together values in our mixed beverage data. In this case, we had multiple rows for each name/address group, but we wanted to add together total_receipts() for each group.\nreceipts %&gt;% \n  group_by(location_name, location_address) %&gt;% \n  summarize(\n    total_sales = sum(total_receipts)\n  ) %&gt;% \n  arrange(desc(total_sales))\n\n\n\n\n\n\n\n\nlocation_name\nlocation_address\ntotal_sales\n\n\n\n\nWLS BEVERAGE CO\n110 E 2ND ST\n35878211\n\n\nRYAN SANDERS SPORTS\n9201 CIRCUIT OF THE AMERICAS BLVD\n20714630\n\n\nW HOTEL AUSTIN\n200 LAVACA ST\n15435458\n\n\nROSE ROOM/ 77 DEGREE\n11500 ROCK ROSE AVE\n14726420\n\n\nTHE DOGWOOD DOMAIN\n11420 ROCK ROSE AVE STE 700\n14231072\n\n\n\nThe result will have all the columns you included in the group, plus the columns you create in your summarize statement. You can summarize more than one thing at a time, like the number of rows numb_rows = n() and average of the values average = mean(column_name).\n\n\n14.4.3 Creating columns to show difference\nSometimes you need to perform math on two columns to show the difference between them. Use mutate() to create the column and do the math. Here‚Äôs a pseudo-code example:\nnew_or_reassigned_df &lt;- df %&gt;% \n  mutate(\n    new_col_name = (part_col / total_col) * 100\n  )"
  },
  {
    "objectID": "explore.html#cleaning-up-categorical-data",
    "href": "explore.html#cleaning-up-categorical-data",
    "title": "14¬† Exploring a new dataset",
    "section": "14.5 Cleaning up categorical data",
    "text": "14.5 Cleaning up categorical data\nIf you are going to count our summarize rows based on categorical data, you should make sure the values in that column are clean and free of typos and values that might better be combined.\nSome strategies you might use:\n\nCreate a count() of the column to show all the different values and how often they show up.\nYou might want to use mutate() to create a new column and then update the values there. Or you might use recode() the set specific values to new values.\n\nIf you find you have hundreds of values to clean, then come see me. There are some other tools like OpenRefine that you can learn fairly quickly to help."
  },
  {
    "objectID": "explore.html#time-as-a-variable",
    "href": "explore.html#time-as-a-variable",
    "title": "14¬† Exploring a new dataset",
    "section": "14.6 Time as a variable",
    "text": "14.6 Time as a variable\nIf you have dates in your data, then you almost always want to see change over time for different variables.\n\nSummarize records by year or month as appropriate and create a Bar or Column chart to show how the number of records for each time period.\nDo you need to see how different categories of data have changed over time? Consider a line chart that shows those categories in different colors.\nIf you have the same value for different time periods, do might want to see the change or percent change in those values. You can create a new column using mutate() to do the math and show the difference."
  },
  {
    "objectID": "explore.html#explore-the-distributions-in-your-data",
    "href": "explore.html#explore-the-distributions-in-your-data",
    "title": "14¬† Exploring a new dataset",
    "section": "14.7 Explore the distributions in your data",
    "text": "14.7 Explore the distributions in your data\nWe didn‚Äôt talk about histograms in class, but sometimes you might want see the ‚Äúdistribution‚Äù of values in your data, i.e.¬†how the values vary within the column. Are many of the values similar? A histogram can show this.\nHere is an example of a histogram use wells data exploring the borehole_depth (how deep the well is). Each bar represents the number of wells broken down in 100ft depth increments (set with binwidth=100). So the first bar shows that most of the wells (more than 7000) are less than 100 feet deep.\nwells %&gt;% \n  ggplot(aes(x = borehole_depth)) +\n  geom_histogram(binwidth = 100)\n\n\n\nBorehole depth histogram\n\n\nWhile there are wells deeper than 1000 feet, they are so few they don‚Äôt even show on the graphic.\nYou‚Äôll rarely use a histogram as a graphic with a story because they are more difficult to explain to readers. But they do help you to understand how much values differ within a column.\n\n14.7.1 More on histograms\nIf you google around, you might see other ways to create a histogram, including hist() and qplot(). You might stick with the ggplot‚Äôs geom_histogram() since you already are familiar with the syntax.\n\nTutorial on histograms using ggplot from DataCamp.\nR Cookbook on histograms."
  },
  {
    "objectID": "explore.html#same-ideas-using-spreadsheets",
    "href": "explore.html#same-ideas-using-spreadsheets",
    "title": "14¬† Exploring a new dataset",
    "section": "14.8 Same ideas using spreadsheets",
    "text": "14.8 Same ideas using spreadsheets\nCheck out this resource by David Eads on the same topic, with some more specifics about Google Sheets."
  },
  {
    "objectID": "charts-tips.html#titles-descriptions-and-annotations",
    "href": "charts-tips.html#titles-descriptions-and-annotations",
    "title": "15¬† Chart production tips",
    "section": "15.1 Titles, descriptions and annotations",
    "text": "15.1 Titles, descriptions and annotations\nChart titles and descriptions can be some of the most difficult writing you can do as a journalist. You don‚Äôt want to describe the steps of your analysis nor say the obvious, but you do need to give the reader all the relevant detail needed to understand the chart. Write titles, descriptions an annotations as if the chart stands alone, and a reader knows nothing before viewing it.\nSome tips paraphrased from Nathan Yau‚Äôs Data Points book ‚Ä¶\n\nThe title ‚Äì typically larger and bolder fonts ‚Äî sets the stage or describes what people should see or look for in the data. A descriptive title also helps. For example, ‚ÄúRising Gas Prices‚Äù says more about the chart than just ‚ÄúGas Prices.‚Äù By including ‚ÄúRising‚Äù, it presents the conclusion immediately, and readers will look to the chart to verify and see the details. Saying just ‚ÄúGas Prices‚Äù leaves the data interpretation to the readers and places them in the exploration phase.\nThe description or lead-in text is used to prepare readers for what a chart shows, but in further detail. The text expands on what the title declares, where the data is from, how it was derived, or what it means (best charts do this, says @crit). Basically, it‚Äôs information that might help others understand the data better but often doesn‚Äôt directly point to the specific elements.\nTo explain specifics points or areas, you can use lines and arrows as an annotation layer on top of a chart. This places descriptions directly in the context of the data so tha a readers doesn‚Äôt have to look outside a graph for additional information to fully understand what you show."
  },
  {
    "objectID": "charts-tips.html#other-considerations",
    "href": "charts-tips.html#other-considerations",
    "title": "15¬† Chart production tips",
    "section": "15.2 Other considerations",
    "text": "15.2 Other considerations\n\nProper data encodings and visual cues: Think about what you are trying to convey with the graphic and plot your data in a fashion that furthers that understanding. (See Nathan Yau‚Äôs Data Points, Chap. 3.)\nLegends for encodings: If your plots include labels for categories on the chart, you may not need a separate legend, but be sure readers can distinguish items.\nLabels for axis: They help describe the value being plotted. In some obvious cases where the meaning in clear, like years, they may be dropped.\nInclude unit values to further understanding: Sometimes the value can be added to the plot itself, other times grid lines may be enough.\nAnnotations: Add explanations to the plot if they help readers understand nuance of what you area trying to convey.\nSource of the data: This is the course of the data, not the delivery method. (i.e., The Comptroller of Public accounts, not data.texas.gov.)\nYour byline: Credit yourself and your publications."
  },
  {
    "objectID": "index.html#some-words-from-prof.-mcdonald",
    "href": "index.html#some-words-from-prof.-mcdonald",
    "title": "Reporting with Data in R",
    "section": "Some words from Prof.¬†McDonald",
    "text": "Some words from Prof.¬†McDonald\nI‚Äôm a strong proponent of what I call Scripted Journalism, a method of committing data-centric journalism in a programmatic, repeatable and transparent way. There are a myriad of programming languages that further this, including Python (pandas using Jupyter) and JavaScript (Observable), but we‚Äôll be using R, Quarto and RStudio.\nR is a super powerful, open-source programming language for data that is deep with features and an awesome community of users who build upon it. No matter the challenge before you in your data storytelling, there is probably a package available to help you solve that challenge. Probably more than one.\nThere is always more than one way to do things in R. This book is a Tidyverse-oriented, opinionated collection of lessons intended to teach students new to programming and R for the expressed act of committing journalism. As a beginner course, we strive to make it as simple as possible, which means we may not go into detail about alternative (and possibly better) ways to accomplish tasks in favor of staying in the Tidyverse and reducing options to simplify understanding. We rarely discuss differences from base R; Tidyverse is our default.\nProgramming languages evolve constantly, and R has seen some significant changes in the past few years, many of them driven by Posit, the company that makes Rstudio and maintains the Tidyverse packages.\n\nThe introduction of Quarto in mid-2022. This modern implementation of RMarkdown is a major shift for this edition of the book. Every lesson and video will need to be updated, and we may not get to all of them. The good news is RMarkdown still works inside Quarto documents.\nThe introduction of the base R pipe |&gt; in 2021. Posit developers began using the |&gt; in favor of the magrittr pipe %&gt;% in 2022, and this book follows their lead. The two implementations work interchangeably and you‚Äôll see plenty of %&gt;% in the wild.\nThe use of YAML code chunk options. I first noticed this style when I started using Quarto in 2023. It might not matter much for this book as we don‚Äôt use that many code chunk options in our assignments, but we‚Äôll see."
  },
  {
    "objectID": "index.html#conventions-and-styles-in-this-book",
    "href": "index.html#conventions-and-styles-in-this-book",
    "title": "Reporting with Data in R",
    "section": "Conventions and styles in this book",
    "text": "Conventions and styles in this book\nWe will try to be consistent in the way we write documentation and lessons. But we are human, so sometimes we break our own rules, but in general keep the following in mind.\n\nThings to do\nThings to DO are in ordered lists:\n\nDo this thing.\nThen do this thing.\n\nExplanations are usually in text, like this very paragraph.\nSometimes details will be explained in lists:\n\nThis is the first thing I want you to know.\nThis is the second. You don‚Äôt have to DO these things, just know about them.\n\n\n\nCode blocks\nThis book often runs the code that is shown, so you ‚Äôll see the code and the result of that code below it.\n\n1 + 1\n\n[1] 2\n\n\n\nCopying code blocks\nWhen you see R code in the instructions, you can roll your cursor over the right-corner and click on the copy icon to copy the code clock content to your clipboard.\n\nYou can then paste the code inside your R chunk.\nThat said, typing code yourself has many, many benefits. You learn better when you type yourself, make mistakes and have to fix them. We encourage you to always type short code snippets. Leave the copying to long ones.\n\n\nFenced code\nSometimes we need to show code chunk options that are added, like when explaining how to name chunks. In those cases, you may see the code chunk with all the tick marks, etc. like this:\n\n```{r block-named}\n1 + 1\n```\n\n[1] 2\n\n\nor\n\n```{r}\n#| label: block-named-yaml\n\n1 + 1\n```\n\n[1] 2\n\n\nYou can still copy/paste these blocks, but you‚Äôll get the entire code block, not just the contents.\n\n\nHidden code\nSometimes we want to include code in the book but not display it so you can try the to write the code yourself first. When we do this, it will look like this:\n\n\nClick here to show the code\n1 + 1\n\n\n[1] 2\n\n\nIf you click on the triangle or the words that follow, you‚Äôll reveal the code. Click again to hide it.\n\n\n\nNotes, some important\nWe will use callouts to set off a less important aside:\n\n\n\n\n\n\nMarkdown was developed by JOHN GRUBER, as outlined on his Daring Fireball blog.\n\n\n\nBut sometimes those asides are important. We usually indicate that:\n\n\n\n\n\n\nImportant\n\n\n\nYou really should learn how to use Markdown as you will use it the whole semester, and hopefully for the rest of your life."
  }
]