# Summarize with math - import {#sums-import}

> First draft of Sept 2021 rewrite

With our Billboard assignment, we went through some common data wrangling processes â€” importing data, cleaning it and querying it for answers. All of our answers involved counting numbers of rows and we did so with two methods: The summary trio: `group_by`, `summmarize` and `arrange` (which I dub GSA), and then the shortcut `count()` that allows us to do all of that in one line.

For this data story we need to leave `count` behind and stick with the summary trio GSA because now we must do different kinds of math in our summarize functions, mainly `sum()`.

## About the story: Military surplus transfers

In June 2020, Buzzfeed published the story [_Police Departments Have Received Hundreds Of Millions Of Dollars In Military Equipment Since Ferguson_](https://www.buzzfeednews.com/article/johntemplon/police-departments-military-gear-1033-program) about the amount of military equipment transferred to local law enforcement agencies since Michael Brown was killed in Ferguson, Missouri. After Brown's death there was a public outcry after "what appeared to be a massively disproportionate show of force during protests brought scrutiny to a federal program that transfers unused military equipment to local law enforcement." Reporter John Templon used data from the [Law Enforcement Support Office](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/) for the update on the program and published his [data analysis](https://github.com/BuzzFeedNews/2020-06-leso-1033-transfers-since-ferguson), which he did in Python.

You will analyze the same dataset focusing on some local police agencies and write a short data drop about transfers to those agencies.

### The LESO program

The Defense Logistics Agency transfers surplus military equipment to local law enforcement through its [Law Enforcement Support Office](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/). You can find more information [about the program here](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs/).

The agency updates the data quarterly and the data I've collected contains transfers through **June 30, 2021**. The original file is linked from the headline "ALASKA - WYOMING AND US TERRITORIES".

The data there comes in an Excel spreadsheet that has a new sheet for each state. I used R to pull the data from each sheet and combine it into a single data set and I'll cover the process I used in class, but you won't have to do that part.

**I will supply a link to the combined data below.**

### About the data

There is no data dictionary or record layout included with the data but I have corresponded with the Defense Logistics Agency to get a decent understanding of what is included. Columns in bold are those we care about the most.

- sheet: Which sheet the data came from. This is an artifact from the data merging script.
- **state**: A two-letter designation for the state of the agency.
- **agency_name**: This is the agency that got the equipment.
- nsn: A special number that identifies the item. It is not germane to this specific assignment.
- **item_name**: The item transferred. Googling the names can sometimes yield more info on specific items.
- **quantity**: The number of the "units" the agency received.
- ui: Unit of measurement (item, kit, etc.)
- **acquisition_value**: a cost *per unit* for the item.
- demil_code: Another special code not germane to this assignment.
- demil_ic: Another special code not germane to this assignment.
- **ship_date**: The date the item(s) were sent to the agency.
- station_type: What kind of law enforcement agency made the request.

Here is a sample of our main columns of interest, except for the date:

```{r leso-setup, echo=F, message=F, warning=F}
library(tidyverse)
library(DT)
leso_sample <- read_rds("data-raw/leso-sample.rds")
```

```{r leso-print, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
leso_sample %>% 
  select(
    state,
    agency_name,
    item_name,
    quantity,
    acquisition_value
  ) %>%
  head(5) %>% 
  datatable(rownames = FALSE, options = list(
  dom = "t"
  ))
```
<br>

Each row of data is a transfer of a particular type of item from the U.S. Department of Defense to a local law enforcement agency. The row includes the name of the item, the quantity, and the value ($) of a single unit.

What the data doesn't have is the **total value** of the items in the shipment. If there are 5 generators as noted in the first row above and the cost of each one is $4623.09, we have to multiply the `quantity` times the `acquisition_value` to get the total value of that equipment. We will do that as part of the assignment.

The local agencies really only pay the shipping costs for the item, _so you can't say they paid for the items_, so the **total value** you calculate is the "value" of the items, not their cost to the local agency.

## The questions we will answer

All answers will be based on data from **Jan. 1, 2010** to present. In addition, we'll only consider **Texas** agencies as you answer the following.

- For each agency in Texas, find the summed **quantity** and summed **total value** of the equipment they received. (When I say "summed" that means we'll add together all the values in the column.)
  - Once you have the list, we'll think about what stands out and why?
- We'll take the list above, but filter that summary to show only the following local agencies:
  - AUSTIN POLICE DEPT
  - SAN MARCOS POLICE DEPT
  - TRAVIS COUNTY SHERIFFS OFFICE
  - UNIV OF TEXAS SYSTEM POLICE HI_ED
  - WILLIAMSON COUNTY SHERIFF'S OFFICE
- For each of the agencies above we'll summarize the **total quantity** and **acquisition_value** of each **item** shipped to the agency. We'll arrange the list by agency so we can write about each one.
- Youl'll research some of the more interesting items the agencies received (i.e. Google the names) so you can include them in your data drop.

## Create your project

We will build the same project structure that we did with the Billboard project. In fact, all our class projects will have this structure. Since we've done this before, some of the directions are less detailed.

1. With RStudio open, make sure you don't have a project open. Go to File > Close project.
1. Use the create project button (or File > New project) to create a new project in a "New Directory". Name the directory "yourname-military-surplus".
1. Create two folders: `data-raw` and `data-processed`.

## Import/cleaning notebook

Again, like Billboard, we'll create a notebook specifically for downloading, cleaning and prepping our data.

1. Create your RNotebook.
1. Rename the title "Military Surplus import/clean".
1. Remove the rest of the boilerplate template.

### Add the goals of the notebook

1. In Markdown, add a headline noting these are notebook goals.
1. Add the goals below:

```text
- Download the data
- Import the data
- Clean datatypes
- Remove unnecessary columns
- Create a total_value column
- Filter to Texas agencies
- Filter the date range (since )
- Export the cleaned data
```

> NOTE: Most of these are pretty standard in a import/cleaning notebook. Filtering to Texas agencies is specific to this data set, but we would do all these other things in all projects.

### Add a setup section

This is the section where we add our libraries and such. Again, every notebook has this section, though the packages may vary on need.

1. Add a headline and text about what we are doing: Our project setup.
2. Add a code chunk to load the libraries. You should only need `tidyverse` for this notebook because the data already has clean names (no need for janitor) and the dates will import correctly (no need for lubridate).

```{r setup, message=FALSE}
library(tidyverse)
```


### Download the data

1. A new section means a new headline and description. Add it. It is good practice to describe and link to the data you will be using. You can use this:

```text
The Defense Logistics Agency transfers surplus military equipment to local law enforcement through its [Law Enforcement Support Office](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/). You can find more information [about the program here](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs/).
```

1. Use the `download.file()` function to download the date into your `data-raw` folder. Remember you need two arguments:

```r
download.file("url_to_data", "path_to_folder/filename.csv")
```

- The data can be found at this url: `https://github.com/utdata/rwd-r-leso/blob/main/data-processed/leso.csv?raw=true`
- It should be saved in our `data-raw` folder with a name for the file.

Once you've built your code chunk and run it, you should make sure the file downloaded into the correct place: in your `data-raw` folder.

<details>
  <summary>You should be able to do this on your own. Really.</summary>
  
```{r download}
# You can comment the line below once you have the data
download.file("https://github.com/utdata/rwd-r-leso/blob/main/data-processed/leso.csv?raw=true",
              "data-raw/leso.csv")
```

</details>

### Import the data

We are again working with a CSV, or comma-separated-values text file. I suggest you build the code chunk a bit at a time in this order:

1. Use `read_csv()` to read the file from our `data-raw` folder.
1. Edit that line to put the result into a tibble object using `<-`. Name your new tibble `leso`.
1. Print the tibble as a table to the screen again by putting the tibble object on a new line and running it. This allows you to see it in columnar form.

<details>
  <summary>Try real hard first before clicking here for the answer</summary>
  
```{r import}
leso <- read_csv("data-raw/leso.csv")

leso
```

</details>

### Glimpse the data

1. In a new block, print the tibble but pipe it into `glimpse()` so you can see all the column names.

```{r leso-glimpse}
leso %>% glimpse()
```

#### Checking datatypes

Take a look at your glimpse returns. These are the things to watch for:

- Are your variable names (column names) clean? All lowercase with `_` separating words?
- Are dates saved in a date format? `ship_date` looks good at `<dttm>`, which means "datetime".
- Are your numbers really numbers? `acquisition_value` is the column we are most concerned about here, and it looks good.

This data set looks good (because I pre-prepared it fo you), but you always want to check and make corrections, like we did to fix the date in the Billboard assignment.

### Remove unnecessary columns

Sometimes at this point in a project, you might not know what columns you need to keep and which you could do without. The nice thing about doing this with code in a notebook is we can always go back, make corrections and run our notebook again. In this case, I'm going to tell you which columns you can remove so we have a tighter data set to work with. We'll also learn a cool trick with `select()`.

1. Start a new section with a headline, text to explain you are removing unneeded columns.
2. Add the following code. I'll explain it below.

```{r remove-cols}
leso_tight <- leso %>% 
  select(
    -sheet,
    -nsn,
    -starts_with("demil")
  )

leso_tight %>% glimpse()
```

We did a select like this with billboard, but note the third item within the `select()`: `-starts_with("demil")`. This removes both the `demil_code` and `demil_ic` columns in one move.

There are other special operators like that: `ends_with()`, `contains()` and many more. [Check out the docs on the select function](https://dplyr.tidyverse.org/reference/select.html).

So now we have a tibble called `leso_tight` that we will work with in the next section.

### Create a total_value column

When we used `mutate()` to convert the date in the Billboard assignment, we were reassigning values in each row of a column back into the same column.

In this assignment, we will use `mutate()` to create a **new** column with new values based on a calculation -- `quantity` multiplied by the `acquisition_value` -- for each row. Let's review the concept first

If you started with data like this:

| item  | item_count | item_value |
|-------|-----------:|-----------:|
| Bread |          2 |        1.5 |
| Milk  |          1 |       2.75 |
| Beer  |          3 |          9 |

And wanted to create a total value of each item in the table, you would use `mutate()`:

```r
data %>% 
  mutate(total_value = item_count * item_value)
```

And you would get a return like this, with your new `total_value` column added at the end:

| item  | item_count | item_value | total_value |
|-------|-----------:|-----------:|------------:|
| Bread |          2 |        1.5 |           3 |
| Milk  |          1 |       2.75 |        2.75 |
| Beer  |          3 |          9 |          27 |

Other math operators work as well: `+`, `-`,  `*` and `/`.

So, now that we've talked about how it is done, I want you to:

1. Use `mutate()` to create= a new `total_value` column that multiplies `quantity` times `acquisition_value`.
2. Put the results into a new tibble called `leso_total` so we can all be on the same page.
3. Glimpse the new tibble so you can check the results.

<details>
  <summary>Try it on your own. You can figure it out!</summary>

```{r create-total-value}
leso_total <- leso_tight %>% 
  mutate(
    total_value = quantity * acquisition_value
  )

leso_total %>% glimpse()
```

</details>

**Check that it worked!!**. Use the glimpsed data to check the first item: For me, 10 * 1626.00 = 16260.00, which is correct!

Note that new columns are added at the end of the tibble. That is why I suggested you glimpse the data instead of printing the tibble so you can easily see results on one screen.

### Filtering our data

You used filter in the Billboard lesson to get No. 1 songs and to get a date range of data. We need to do something similar here to get only Texas data of a certain date range, but we'll build the filters one at a time so we can check the results.

#### Apply the TX filter

1. Create a new section with headlines and text that denote you are filtering the data to Texas and since Jan. 1, 2010
2. Create the code chunk and start your filter process using the `leso_total` tibble.
3. Use `filter()` on the `state` column to keep all rows with "TX".

<details>
  <summary>Really, you got this.</summary>

```{r filter-tx}
leso_total %>% 
  filter(
    state == "TX"
  )
```

</details>

How do you know if it worked? Well the first column in the data is the `state` column, so they should all start with "TX". Also note you started with nearly 130k observations (rows), and there are only 8,600+ in Texas.

#### Add the date filter

1. Now, **EDIT THAT SAME CHUNK** to add a new part to your filter to also get rows with a `ship_date` of 2010-01-01 or later.

<details>
  <summary>If you do this on your own, treat yourself to a cookie</summary>

```{r filer-date}
leso_total %>% 
  filter(
    state == "TX",
    ship_date >= "2010-01-01"
  )
```

</details>

#### Checking the results with summary()

How do you know this date filter worked? Well, we went from 8600+ rows to 7400+ rows, so we did something. You might look at the results table and click over to the `ship_date` columns so you can see some of the results, but you can't be sure the top row is the oldest. We could use an `arrange()` to test that, but I have another suggestion: `summary()`.

Now, `summary()` is different than `summarize()`, which we'll do plenty of in a mintue. The summary function will show you some results about each column in your data, and when it is a number or date, it will give you some basic stats like min, max and median values.

1. Use the image below to add a `summary()` function to your filtering data chunk.
2. Once you've confirmed that the "Min." of `ship_date` is not older than 2010, then **REMOVE THE SUMMARY STATEMENT**.

If you leave the summary statement there when we create our updated tibble, then you'll "save" the summary and not the data.


![Summary function](images/military-date-summary.png)

#### Add filtered data to new tibble

Once you've checked and removed the summary, you can save your filtered data into a new tibble.

1. Edit the filtering chunk to put the results into a new tibble called `leso_filtered`.

<details>
  <summary>Seriously? You were going to look?</summary>

```{r filter-saved}
leso_filtered <- leso_total %>% 
  filter(
    state == "TX",
    ship_date >= "2010-01-01"
  )

leso_filtered %>% glimpse()
```

</details>

### Export cleaned data

Now that we have our data selected, mutated and filtered how we want it, we can export your `leso_filtered` tibble into an `.rds` file to use in our analysis notebook. If you recall, we use the `.rds` format because it will remember data types and such.

1. Create a new section with headline and text explaining that you are exporting the data.
1. Do it. The function you need is called `write_rds` and you need to give it a path/name that saves the file in the `data-processed` folder. Name it `01-leso-tx.rds` so you know it a) came from the first notebook b) is the Texas only data. **Well-formmated, descriptive file names are important to your future self and other colleagues**.

